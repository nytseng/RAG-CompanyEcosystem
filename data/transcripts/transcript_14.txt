JENSEN HUANG:  We had a fantastic quarter, a terrific ramp, nothing was easy about it. And a couple of quarters ago, of course, people were worried about how successfully we’d be able to ramp something as complex as Blackwell. Blackwell, people don’t, you know, maybe they just forget that it’s not just a chip, but it’s a whole system. That system is a ton and a half, has a million and a half components. You know, it’s incredibly hard to build. It’s built in 350 plants around the world and 100,000 different operators, factory operators, you know, contribute to building this thing. And so, I think it’s, it was logical to be concerned about the ramping of Blackwell, but we have now successfully ramped Blackwell. The other concern that people had was the Hopper-Blackwell transition, it might create a pocket, an air pocket. And I think we’re now well successfully behind the air pocket. We’re going to have a good quarter this quarter, we had a great quarter. We’re going to have a good quarter next quarter. And we’ve got a fairly good pipeline of demand for Blackwell.

JON FORTT:  Yes. The guide was also above, as you mentioned. Earlier today and speaking of demand, I was talking to Amazon CEO, Andy Jassy. He told me that, as of now, if he had more AI resources to sell through AWS, he could sell more. That’s kind of the short-term signal of demand that you talked about on the call. Tell me more about the mid-term signals that investors should be aware of that give you confidence in the continued demand that the scale outs of data centers, AI factories relative to what you’ve historically seen.

HUANG:  The short-term signal are just our P.O.s and the forecasts. And on top of that, the things that are not forecasted are new startup companies that are spinning off. And some of these are quite famous. And, you know, at the risk of forgetting any of them, I won’t mention any of them, but there’s some really, really fantastic startups that have come out as a result of a new reasoning AI capabilities and, you know, artificial general intelligence capabilities that they have breakthroughs in. And several of them, there’s several of them that are related to agentic AIs, really exciting companies. And there’s several of them related to physical AIs. You know, there’s just handfuls of each one of them, and each one of them needs additional compute. And that’s, you know, the type of things that Andy talks about because they need to go to AWS and they have urgent need for more compute right away. And so, that’s on top of what we already knew to have P.O.s and forecasts and such. The mid-term comes from the fact that this year’s capital investment for data centers is so much greater than last year’s. And of course, we had a very large year last year. We had a great year last year. It stands to reason that with Blackwell and with all the new data centers going online, we’re going to have a fairly great year. Now, long-term, the thing that’s really exciting is we’re just at the beginning of the reasoning AI era. You know, this is the era, the time when AI is thinking to itself before it answers a question instead of just immediately generating an answer, they’ll reason about it, maybe break it down step by step, it’ll do maybe some searching in its own mind before it creates and composes a smart answer for you. The amount of computation necessary to do that reasoning process is a hundred times more than what we used to do. So, if you could imagine, we thought computation, the amount of compute necessary was a lot last year. And then, all of a sudden, reasoning AI, DeepSeek was an example of that, ChatGPT 4.0 is an example of that, Grok 3 reasoning is an example of that. So, all of these reasoning AI models now need a lot more compute than what we used to we’re expecting.

FORTT:  Well, let me stop you there, because –

HUANG:  It puts even more load.

FORTT:  Because some people took DeepSeek to mean actually that you need less compute, right? Because the initial report was that they were doing more with less. But you’re saying, in fact, some of what came out of DeepSeek was the opposite, that there’s going to be more compute demanded? Unpack that for me.

HUANG:  There are three phases in how AI works, how AI is developed largely. Number one is pre-training. It’s kind of like us going through high school. A lot of basic math, basic language, basic everything. That basic understanding of human knowledge is essential to do what is the next step, which is called post-training. In post-training, you might get human feedback. You know, it’s like a teacher showing it to you. We call it reinforcement learning human feedback. You might practice and do thought experiments. You’re just preparing for a test. You’re doing a whole lot of practices. We call it reinforcement learning AI feedback. You could either also do tests and practice, and we call it reinforcement learning verifiable reward feedback. So, now, basically, it’s AIs teaching AIs how to be better. AIs. That post-training process is where an enormous amount of innovation is happening right now. A lot of it happened with these reasoning models, and that computation load could be 100 times more than pre-training. And then, here comes inference. The reasoning process. Instead of just spewing out an answer, when prompted, it reasons about it. It thinks about how best to answer that question, breaks it down step by step, might even reflect upon it, come up with several versions, pick the best one, and then presents it to you. So, the amount of computation that we have to do, even at inference time now is a hundred times more than what we used to do when ChatGPT first came out. And so, all of a sudden, the combination of all these ideas largely related to reinforcement learning and synthetic data generation and reasoning, all of this is just causing compute demand to go sky high.

FORTT:  Now, tell me about price performance because this is one of those things that I think a lot of investors are trying to figure out. They see the hyperscalers coming out with their own chips. They say, hey, boy, there are a lot of customers who don’t have a lot of money to spend. They want to get the most bang for their buck. So, they’ll view Trainium, Inferentia, Microsoft chips, Google’s chips as potential competitors for NVIDIA. But at the same time, you talked on the earnings call about performance per watt, right. And so, it seems to me at GDC, and I know our Jim Cramer is going to be there with you next month, that’s where you introduce new products and new powerful use cases. If your performance of what you’re coming out with next is that much higher than what’s otherwise available on the market, does that mean, would you argue that your price performance actually ends up better than what’s cheaper?

HUANG:  The reason why we sell so much is because our price performance is the best, and it’s absolutely the case that performance per watt is incredibly important. And the reason for that is because a data center is only so large. That data center could be 250 megawatts, or it could be a gigawatt. But within that data center, however large it is, you want the amount of revenues you can generate to be as high as possible. So, you want to generate, you want to do two things. You want to generate very high-quality tokens. AI is ultimately expressed in tokens, and that’s what you monetize, million, dollars per million tokens. You want to generate very high-quality tokens because you could get better pricing on that, better ASP. On the other hand, you want to get as many tokens out of that data center as you can. And in order to do that, your performance has to be excellent, and your performance per watt has to be excellent. And so, the simple way to thinking about that is if your performance per unit energy is the highest, the revenues you can help a company generate is the absolute highest. And there’s, if you look at the way we are driving our roadmap between Hopper and Blackwell, our token generation rate for these reasoning AI models can be as high as 25 times. That’s the same thing as saying that factory can generate 25 times more revenues using Blackwell than you could using Hopper before that.

FORTT:  Right.

HUANG:  Which is the reason why demand for Blackwell is so great.

FORTT:  Let me—

HUANG:  And then of course, we on a roadmap that’s once a year and every single year we’re increasing our performance per dollar, performance per energy, performance per watt, so that everybody’s data centers become more energy efficient on the one hand, generate more revenues on the other hand.

FORTT:  So, let me ask you about your China business. You talked on the call about how the percentage of revenue is half what it was before export controls. Does the emergence of DeepSeek, which some have cast as a workaround, some of those restrictions, does it tell us anything about the effectiveness of export controls?

HUANG:  It’s hard to tell whether export control is effective. The thing that I can tell you is this, our percentage revenues in China before export controls is twice as high as it is now. There’s a fair amount of competition in China, export control and otherwise, Huawei, other companies are doing very, are quite rigorous and very, very competitive. And so, I think that ultimately software finds a way, maybe that’s the easiest way of thinking about it. You know, software is always, whether you’re developing software for a supercomputer or a software for a personal computer or software for a phone or software for a game console, you ultimately make that software work on whatever system that you’re targeting and you create great software. And so, that’s kind of the beauty of software engineers. They’re incredibly innovative and clever in this way. Our architecture is about the flexibility of software, and which is kind of nice. But ultimately, here in the United States, if you look at where we are now compared to what is controlled, GB200 is probably something along the lines of 60 times the token generation rate of what is being shipped in China that’s currently export controlled.

FORTT:  Okay.

HUANG:  And so, the separation of performance is quite high. And you know, ultimately, we, what we know we experience is a great deal of competition there.

FORTT:  Real quick. I think last quarter you said demand for Blackwell was insane. This quarter you said it’s extraordinary. Those about the same? Is one better than the other?

HUANG:  I would say that my feelings about Blackwell is better today than it was at last quarter. And the reason for that is because we, of course, ramped up into production, we exceeded our target, and the teams did an amazing job. As you recall, we had a hiccup in our, a design flaw in Blackwell that we found, early on last quarter or quarter before that. And we recovered tremendously well, and I’m very proud of the team for that. And so, for those reasons, I feel pretty great from an execution perspective. From a demand perspective, you know, DeepSeek was fantastic. It was fantastic because it open sourced the reasoning model that’s absolutely world class. Just about every AI developer in the world today has either incorporated R1 using — it’s called distillation, distilled from R1, or using techniques that have been open sourced out of R1 so that their models could be a lot more capable. Across the world, AI has become better as a result of the last several months. And so, I’m excited about that. And the demand for computation for inference time, for test time scaling, which is one of the reasons why Grace Blackwell, NVLink72 is so exciting, that feature is now more prominent than ever, more demanded than ever. And I think the demand side of it is more exciting too.

FORTT:  Jensen Huang, thank you, CEO of NVIDIA. Hope to see you soon. HUANG:  Thank you, Jon. Great to see you.

