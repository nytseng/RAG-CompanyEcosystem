YouReapWhatYouSow:OntheChallengesofBiasEvaluationUnderMultilingualSettingsZeerakTalat1,AurélieNévéol2,StellaBiderman3,4,MirunaClinciu5,6,7,MananDey8,ShayneLongpre9,AlexandraSashaLuccioni10,MaraimMasoud11,MargaretMitchell10,DragomirRadev12,ShanyaSharma13,ArjunSubramonian14,15,JaesungTae10,12,SamsonTan16,17,DeepakTunuguntla18,OskarvanderWal191DigitalDemocraciesInstitute,SimonFraserUniversity2UniversitéParis-Saclay,CNRS,LISN3BoozAllenHamilton4EleutherAI5EdinburghCentreforRobotics6Heriot-WattUniversity7UniversityofEdinburgh8SAP9MIT10HuggingFace11AdaptCentre,TrinityCollegeDublin12YaleUniversity13WalmartLabs,India14UniversityofCalifornia,LosAngeles15Queer-in-AI16AWSAIResearchandEducation17NationalUniversityofSingapore18IndependentResearcher19UniversityofAmsterdamAbstractEvaluatingbias,fairness,andsocialimpactinmonolinguallanguagemodelsisadifﬁculttask.Thischallengeisfurthercompoundedwhenlanguagemodelingoccursinamultilingualcontext.Consideringtheimplicationofeval-uationbiasesforlargemultilinguallanguagemodels,wesituatethediscussionofbiaseval-uationwithinawidercontextofsocialscien-tiﬁcresearchwithcomputationalwork.Wehighlightthreedimensionsofdevelopingmul-tilingualbiasevaluationframeworks:(1)in-creasingtransparencythroughdocumentation,(2)expandingtargetsofbiasbeyondgender,and(3)addressingculturaldifferencesthatex-istbetweenlanguages.Wefurtherdiscussthepowerdynamicsandconsequencesoftraininglargelanguagemodelsandrecommendthatre-searchersremaincognizantoftheramiﬁcationsofdevelopingsuchtechnologies.1IntroductionMachinelearning(ML)systems,especiallylargelan-guagemodels(LLMs),areproneto(re)produceharmfuloutcomesandsocialbiases(Benderetal.,2021;Rajietal.,2021;Blodgettetal.,2020;AguerayArcasetal.,2018).DespiterecentadvancesinLLMs(BenderandKoller,2020),theyhaveshowntodisproportionatelyproduceharmfulcontentwhenaddressingcertaintopics(Gehmanetal.,2020;Linetal.,2021)anddemograph-ics(Shengetal.,2019;Liangetal.,2021;Devetal.,2021a)—inpartduetothetrainingdataused(Dunn,2020;Gaoetal.,2020;Benderetal.,2021),andthedesignofmodelingprocesses(Talatetal.,2021;HovyandPrabhumoye,2021).Inresponse,previousworkhasexploredwaysinwhichsuchsocialbiasescanbemea-suredandcounteracted(Nangiaetal.,2020;Gehmanetal.,2020;Czarnowskaetal.,2021).Typically,theseissueshavebeenaddressedeitherbyconceptualizingtheunderlyingsystemicdiscriminationas“bias”orbydevelopingevaluationdatasetsthatshedlightonhowLLMsproduceharmfulsocialoutcomes.However,intheformercase,asBlodgettetal.(2020)pointsout,theseconceptualizationsoftenlackcleardescriptions,e.g.,typeofsystemicdiscriminationandaffecteddemo-graphics.Thisresultsinahighlyunder-speciﬁed“bias”,whichcouldleadtoadownstreamissueinthevalidityofthetechnicalapproachesthataredeveloped(Blodgettetal.,2021).Similarly,theill-deﬁned“bias”isfur-thercompoundedbythespeciﬁcsofmanybenchmarks.Often,benchmarksexhibitdiscrepanciesbetweenun-derstandingsoftheunobservabletheoreticalconstructsagainstwhich“bias”isbeingmeasuredandtheiropera-tionalization(JacobsandWallach,2021;Friedleretal.,2021).Furthermore,manypriorbenchmarkdatasetsweredevelopedwithspeciﬁcmodelingarchitecturesinmind(Nangiaetal.,2020).TheyarelimitedtoEnglishandareculturallyAnglo-centric.1Inthispositionpaper,wepresentanoverviewofthecurrentstate-of-the-artconcerningchallengesandmeasurestakentoaddressbiasinlanguagemodels.Speciﬁcally,wedocumentthechallengesofevaluat-inglanguagemodels,withafocusonthegenerationofharmfultext.Byengagingourchallengeswiththerele-vantsocialscientiﬁcliterature,wepropose(1)amoretransparentevaluationofbiasviascopinganddocumen-tation,(2)focusingonthediversityofstereotypesforincreasedinclusivity,(3)carefulcurationofculturallyawaredatasets,and(4)creationofgeneralbiasmeasuresthatareindependentofmodelarchitecturebutcapturethecontextofthetask.Werecognizethatmanyofthechallengesthatwehaveencounteredanddescribedherearelargeopenproblemsthatwillrequirejointworktoaddress.Ourgoalistoanalyzethesechallengesandprovidescaffold-ingforfuturework.2GroundingBias,FairnessandSocialImpactacrossDisciplinesConsideringbiasesinsocio-technicalsystemsasapurelytechnicalconstructisaninsufﬁcientconsider-ationoftheproblem(Blodgettetal.,2020).Inthissection,wesituateLLMs,andtheirapplications,withinthewiderinterdisciplinaryliteratureonsocialharmsanddiscrimination.

1Forexample,theBigSciencebiomedicalworkinggrouphasestimatedthat82%ofevaluationdatasetsinthebiomedicalandclinicalﬁeldareforcorporainEnglish(Dattaetal.,2021).

26 Proceedings of BigScience Episode #5 – Workshop on Challenges & Perspectives in Creating Large Language Models, pages 26 - 41 May 27, 2022 c(cid:13)2022 Association for Computational Linguistics

2.1SocialDiscriminationIssuesofsociallydiscriminatory(humanandtechno-logical)systemshavelongbeenthesubjectofstudyforscholarsacrossdisciplines,e.g.inScienceandTechnologyStudies(Haraway,1988),discardstudies(Lepawsky,2019),socialanthropology(Douglas,1978),philosophyofdemocracy(Fraser,1990),genderandLGBTQIA+studies(Spade,2015;RajunovandDu-ane,2019;Keyesetal.,2021;D’IgnazioandKlein,2020),mediastudies(Gitelman,2013),archivalstudies(Agostinhoetal.,2019),sociolinguistics(Labov,1986;Cheshire,2007),andcriticalracetheory(Noble,2018;Benjamin,2019).2Scholarsarguethattechnicalsystemsareembeddedinsocialcontexts(Lepawsky,2019;Haraway,1988)andarethereforenecessarilyevaluatedassocio-technicalsystemsinteractingwithcomplexsocialhierarchies(Winner,1980;Benjamin,2019;Costanza-Chock,2018;Friedleretal.,2021).Whentechnologicalsystemspri-oritizemajorities,thereisarisktheyoppressminori-tiesatthepersonal,communal,andinstitutionallevels(Costanza-Chock,2018).Haraway(1988)arguesthatresearchersdefaulttoa“viewfromnowhere”,withoutreﬂectingonthecontextoruseoftheirresearch.Thisdefaultviewoftenrepresentstheinterestsofdominantmajorities,disregardingknowledgesfrommarginalizedcommunities.Consideringmachinelearningsystems,Chun(2021)arguesthatthedevelopmentofsuchtech-nologicalsystemsreliesonfaultyassumptions(e.g.,thatpastdatacollectionscanadequatelyandfairlypredictfuturehumanbehavior)whichcanleadtoembeddedsocialbiases.Situatingourselvesinthewideracademicliteratureofsocialdiscriminationandmarginalization,compelsustorecognizethatourtechnicalsystemsmustbeconsideredinthesocialcontextinwhichtheyexist.2.2Machine-learnedSystemsinSocialContextOnthetopicofsociallydiscriminatorysystemswithinmachinelearning,BuolamwiniandGebru(2018)andRajiandBuolamwini(2019)showthattherearesig-niﬁcantdisparitiesalonggenderedandracializedlinesincommerciallyavailablefacialrecognitionandanal-ysissystems.Similarissuesofdiscriminatorysocialbiasesinnaturallanguageprocessing(NLP)systemshaveresultedinemergingresearchdedicatedtotheiden-tiﬁcation,quantiﬁcation(e.g.Rudingeretal.,2018;De-Arteagaetal.,2019;Czarnowskaetal.,2021),andmiti-gationofbias(Bolukbasietal.,2016;Sunetal.,2019;Garimellaetal.,2021)inNLPsystems.However,thesemethodstendtoobscureratherthanremovesocialbiases(GonenandGoldberg,2019),andareparticularlybrittlewhenappliedtocomplex,contex-tuallanguagerepresentations(Devetal.,2020).Further,operationalizationofunder-speciﬁed“bias”

2Manyrecentworksonsociallybiasedtechnologicalsys-temsareinterdisciplinary,e.g.,‘RaceAfterTechnology:TheNewJimCode’(Benjamin,2019)spanscriticalracetheory,scienceandtechnology,Blackfeminism,andmediastudies.hasvariedwidelyacrossstudies,andinsomecaseshasbeeninternallyinconsistentwiththeirstatedgoals(Blod-gettetal.,2020;JacobsandWallach,2021).TherecentsurgeofLLMsisnoexceptiontosuchconcerns.HovyandPrabhumoye(2021);Talatetal.(2021),andCaoandDauméIII(2020)arguethatsociallydiscriminatorybiasescanbeencodedinseveralstagesoftheLLMdevelopmentprocess(BidermanandScheirer,2020),includingdatasampling,annotation,selectionofinputrepresentationsormodel,researchdesign,andhowthemodelsaresituatedwithregardstothelanguagecom-munitiesthattheyareappliedto.Languagegenerationmodels,despitetheirinference-timeﬂexibility,arepar-ticularlysusceptibletoreproducinghegemonicsocialbiasesandgeneratingoffensivelanguage,evenwhennotexplicitlypromptedtodoso(Shengetal.,2021;Wallaceetal.,2019;Benderetal.,2021).Ineffortstoaddresstheexpressionofsuchsocialbiases,anumberofbiasevaluationbenchmarkshavebeenproposed(Devetal.,2021b;Zhaoetal.,2018;CaoandDauméIII,2020).However,commonevaluationbenchmarksarefraughtwithpitfallsintheirconcep-tualizationofbias,stereotypes,andharms,includingmeaninglessorpoorlyformedstereotypeconstructions,non-intersectionalexamples,contextsthatdon’treﬂectdownstreamuse,andrelianceonspeciﬁcmodelarchi-tectures(Blodgettetal.,2021;Jinetal.,2021).Further-more,biasevaluationbenchmarksoftenmakestrongassumptionsaboutthevalidity,reliability,andexistenceofobservableproperties,e.g.pronouns,assignalsforunobservabletheoreticalconstructssuchasgender(Ja-cobsandWallach,2021).Thisisparticularlyproblem-aticwhenbuildingbenchmarksforbiasesagainstcom-munitiesthatresistcategorizationbasedonobservablecharacteristics(e.g.LGBTQIA+andracializedpeople)andleadstorelianceonexistingstereotypes(Tomasevetal.,2021;Devetal.,2021a).ThisrapiddevelopmentofNLPresourcesandtoolshavefurtheryieldedanon-inclusiveenvironment,skewedheavilytowardsEnglishandAnglo-centricbi-ases(Joshietal.,2020).Sambasivanetal.(2021)andChanetal.(2021)contendthereremainsasigniﬁcantgapbetweenthecommunitiesgoverningandgovernedbyAI,andadvocateforaredistributionofpowersandresponsibilitiesindevelopingresponsibleAI.Consideringgenderbias,StanczakandAugenstein(2021)showthatexistingmethods(1)largelyavoideth-icalconsiderationsorevaluationsofgenderbias,(2)focusprimarilyonbinarygendertreatment,inmostlyAnglo-centricsettings,and(3)employlimitedorﬂawedevaluationmethodologies.Suchissuesareinpartex-acerbatedbythegeneralpovertyofdocumentationofdatasets(Gebruetal.,2018;BenderandFriedman,2018)andmachinelearningmodels(Mitchelletal.,2019).Onewaytomitigatethesebiasesincludescre-atingdiverseteamswithvariedbackgroundsandlifeexperiencestoassuretheexpressionofdiverseperspec-tives(MonteiroandCastillo,2019;Nekotoetal.,2020).

27

However,ascritiquedbyTalatetal.(2021);Westetal.(2019),incorporatingthediversityfactormaybein-adequate.Biasesinlanguagerepresentationsandtaskmodelscannotonlyreﬂect,butalsoamplifybiaspresentinthedatasets(BarocasandSelbst,2016;Wangetal.,2019).Thesebiaseshavebeeninvestigatedandattemptsmadeatcreatinginterpretablerepresentationsandpro-vidingpost-hocexplanationsofmodelpredictions.2.3Bias,Fairness,andExplainabilityGiventhegraveconsequencesthatinherentorconcep-tualizedbiasesinMLsystemscaninﬂict,responsibleAIhasreceivedagrowingamountofresearchatten-tion(Amershietal.,2020).ResponsibleAIreferstothecreationofethicalprinciplesforAIandthedevelopmentofAIsystemsbasedontheseprinciples(Dignum,2017;Schiff,2020).Colloquially,responsibleAIencompassesdistinctmachinelearningﬁeldssuchasfairness,explain-ability,privacy,andinterpretability.Concretely,howcanresponsibleAIprinciplesbestcontributetothede-velopmentofequitablesystems?Examiningthisquestion,Friedleretal.(2021)pro-posethatbuildingjustMLsystemsrequiresanapriorideﬁnitionoffairness.However,contemporarydecision-makingsystemsbuildonaso-calledwhat-you-see-is-what-you-get(WYSIWYG)approachthatimplicitlyimbibesmultiplefairnessdeﬁnitionsorworldviews,leadingtoasystembasedontheconﬂictbetweentheunderlyingvaluesystems.Totacklethisissue,MLen-gineersshouldexplicitlystatetheunderlyingsystemicvalues,assystemswillinevitablycomprisecertainas-sumptions(Birhaneetal.,2021).Thus,implyingthatbiasesasinherenttothesedecision-makingsystemsandshouldbeclearlyarticulated(Benderetal.,2021)byexplainingthewhysandwhats(explainability).However,amorepromisingcourseofactionforre-searcherswouldbetoprioritizefairnessintheentirelifecycleofalanguagemodel.Thetendencytoconsiderandmitigateundesirablebiasesinmodelsaftertrain-inghascompletedleavesharmfulresiduesthataffectthecommunitiesweseektoprotect(Devetal.,2021a).Hence,afruitfulapproachcouldbetoreducesystemicunfairnessbygroundingthediscussiononcleardeﬁni-tionsoffairnessbasedoninputfromthecommunitiesthatcouldbeharmedbythesystem(LiaoandMuller,2019),explainingtheinherentbiases,and,ifpossible,minimizingbiasissuesbyemployingthemeasuresdis-cussedin,both,thepreviousandthefollowingsections.3ChallengesofBiasEvaluatingthesocialimpactsandharmfulbiasesLLMsexhibitisanimportantdevelopmentstep.However,despitetheincreasedinterestindevelopingbiasbench-marks,theﬁeldstillfacesvariouschallengesinevaluat-ingLLMswithoff-the-shelfbenchmarks.Inthissection,weprovideexamplesofexistingbiasmeasurescurrentlyusedinNLP.Wethendiscussthechallengesthatorig-inatefromthese:(1)theyrelyonvaguedeﬁnitionsofbias,(2)arerestrictedtoparticularmodelarchitectures,(3)havelimitedrelevancefordifferentculturalcontexts,and(4)aredifﬁculttovalidateandinterpret.3.1ExamplesofBiasMeasureStudiesRecently,researchersandpractitionershavebeguntopaymoreattentiontobiasmeasuresinNLPsystems(Blodgettetal.,2020;Devetal.,2021b).Onelineofworkhasfocusedonidentifyingbiasinwordembed-dings:TheWordEmbeddingAssociationTest(WEAT,Caliskanetal.,2017)measuresbiasbycomparingtherelativedistancesoftwosetsoftargetwords(e.g.occu-pationwords:nurse,doctor)withrespecttotwosetsofattributewords(e.g.,genderattributes:male,female)—andhasinspiredothersimilarapproaches(Kuritaetal.,2019;Mayetal.,2019;Devetal.,2020).AlthoughwordembeddingsmayhelpidentifybiasesinthecontextofLLMs,itisoftendifﬁculttoaccessthelearnedcontextuallanguagerepresentationsofthemodel(Abidetal.,2021;Devetal.,2020).Further-more,suchmethodsaredevelopedtoaddressstaticwordembeddingsratherthanthedynamiccontextualwordembeddingsLLMsrelyon(Subramonian,2021).Anotherresearchdirectionistheuseofcausalin-ferenceformeasuringbiasesinLLMs,forexampletoanalyzeifthegeneratedtextbyanLLMisaffectedconsiderablybyonlychangingtheprotectedattributesorcategoriesintheinput(Huangetal.,2020;Madaanetal.,2021;Chengetal.,2021).Inlinewiththisidea,Huangetal.(2020)usedasentimentclassiﬁertoquan-tifyandreducethesentimentbiasexistentinLLMs.Similarly,theCrowS-Pairsbenchmark(Nangiaetal.,2020)leveragestheparadigmofminimalpairstocon-trastsentencesexpressingstereotypesagainstsocialcat-egorieswiththesamesentencesaddressingdifferentsocialcategories.Crows-Pairsisdesignedsuchforlanguagemodelstobeprobedfordisparatebehaviorbetweenthesentencespairs,withthehypothesisthatsystematicdifferenceinthetreatmentreﬂectingthepref-erenceforstereotypeindicatesthepresenceofbiasinthelanguagemodels.OtherexamplesofbiasmeasuresbenchmarksincludeStereoSet(Nadeemetal.,2020),WinoMT(Stanovskyetal.,2019),BBQ(Parrishetal.,2021),BOLD(Dhamalaetal.,2021),andToxicityCom-mentClassiﬁcationcompetition(Jigsaw,2017).3.2DeﬁningBiasTheterm“bias”isoverloadedintheMLandNLPcom-munities,asitisusedinthelay(aprejudicetowardsoragainstsomeentity)andthestatisticalsense(asys-tematicdeviationfromadistribution’smean)(Campoloetal.,2018).Moreover,researchersoftenrefertovaguedeﬁnitionsofbiasandglossoverthedetails,whichre-sultsinmethodsthatlackspeciﬁcity(Blodgettetal.,2020).Whendiscussingmethodstoaddressbias,itiscriticaltobepreciseaboutthebiasbeingaddressed.Biascan,forinstance,bemademorespeciﬁcbybe-ingdeﬁnedalongsociallyrelevantdimensions.Nangia

28

etal.(2020)considertheprotectedcategoriesfromtheUSEqualEmploymentOpportunitiesCommissionandQueerinAIusesasimilarlist(genderidentityandex-pression,sexualorientation,disability,neurodivergence,skillset,physicalappearance,bodysize,race,caste,age,nationality,citizenshipstatus,colonialexperience,religion),yetothercharacteristicsmayberelevantelse-whereintheworld(e.g.illness,migrant,andsocialstatus).3However,protectedclassesareonlyonedimen-sionalongwhichtodeﬁnebias;researchersshouldalsobemindfulofpoliticalbiasesandbiasesresultingfromthefocusonprestigious,highlyresourcedlanguageva-rieties,inadditionstotheintersectionsofmultipledi-mensions(Kearnsetal.,2018;BuolamwiniandGebru,2018;Crenshaw,1991).Withrespecttoanyoftheaforementioneddimen-sions,a“bias”isapreferentialdispositiontowardsoragainstanentity.Colloquially,itisperceivednegativelyandconsideredtobeunfairtreatment.AspointedoutbyBarocasetal.(2017),biasesinlanguagemodelscanmanifestintheformofquality-of-serviceandrepresen-tationdisparities.Asquality-of-servicebiasdescribessubparperformanceofalanguagemodelwhenusedbyaparticulargroup.Forexample,LLM-drivenmachinetranslationsystemsprovidesigniﬁcantlybettersupportfor“prestigious”,high-resourcelanguages,andconse-quentlydenyqualityperformancetoindividualswhodonotspeaktheselanguages(Nekotoetal.,2020).Further-more,infundamentalNLPtaskssuchascoreferenceresolution,LLMscanfailforpeoplewhouseneopro-nouns,andoftencapturemeaninglessrepresentationsforlanguageassociatedwithtransandnon-binaryindi-viduals.(CaoandDauméIII,2020;Devetal.,2021a).Additionally,Blodgettetal.(2018)showthatparsingsystemstrainedprimarilyonWhiteMainstreamAmeri-canEnglishexhibitdisparateperformanceonAfricanAmericanEnglishandTanetal.(2020)showthatEn-glishquestionansweringandmachinetranslationsys-temsoftenfailonthemorphologicalvariationthatisoftenpresentinnon-prestigeandLearnerEnglishes.Representationbiasesconsistofstereotypesandunder-representation(orover-representation)ofdataormodeloutputs.Stereotypingisacognitiveprocessthatmanifestsfromoftennegativeculturalnormsaboutacharacteristic;stereotypingpermeateswhatpeopledo,say,orwrite.Alonglineofworkhasshownthatlanguagemodelscapturesocialstereotypes,forexample,withrespecttobinarygenderandoccupa-tions(Zhaoetal.,2018;BordiaandBowman,2019;deVassimonManelaetal.,2021).Withregardto(under)representation,inMIMIC-III,aclinicalnotesdataset,only1.9%ofpatientsidentifyasAsian,incom-parisonto71.5%whoidentifyaswhite(Chenetal.,

3QueerinAI(http://queerinai.org/)isagrass-rootsD&IorganizationthatseekstoempowerqueerandtransresearchersinAIandadvanceresearchattheintersectionsofAIandqueerness.Theirlistofcategoriescanbefoundhere:http://queerinai.org/code-of-conduct.2020).Furthermore,blocklistsintheColossalCleanCrawledCorpus(C4)datasetdisproportionatelyﬁlterwordsrelatedtoqueernessandlanguagethatisnotWhite-alignedEnglish(Dodgeetal.,2021).Notably,quality-of-serviceandrepresentationbiasesarenotmu-tuallyexclusive;forinstance,thebrittlerepresentationslearnedbyaLLMforlanguageassociatedwithtransandnon-binaryindividualslargelystemsfromthese-vereunder-representationofthisintrainingdata(Devetal.,2021a;BarocasandSelbst,2016).Thebreakdownofbiasesintoquality-of-serviceandrepresentationdisparitiesisonlyoneofmanypossi-blelenses.Itisalsocriticaltoexplicitlyconsiderbi-asesstemmingfromdisparitiesinresources,broadlydeﬁnedintermsofdataavailability,timetoinvestintodatasetcuration,accesstocomputeresources,ﬁnancialresources,andmore(Benderetal.,2021).3.3OverrelianceonModelArchitecturesCurrentbenchmarksoftenmeasurebiasinspeciﬁcdownstreamtasks(e.g.MachineTranslation(Stanovskyetal.,2019),QuestionAnswering(Parrishetal.,2021),orTextGeneration(Dhamalaetal.,2021)),whileothersfocusonbiasinLLMsmoregenerally(e.gKuritaetal.,2019;Nadeemetal.,2020;Nangiaetal.,2020).Thishastheadvantageofbeingmorewidelyapplicable,asmanyNLPsystemsarebasedonLLMs,anditavoidstheneedforcreatingandvalidatinganewbenchmarkforeachpossibledownstreamtask.Yet,whenthebench-marksheavilyrelyonthemodelarchitectureratherthanthetaskspeciﬁcation,quantitativecomparisonbetweendifferentmodelsbasedonthesebenchmarksisnolongerpossible.Insuchcases,italsobecomesmoredifﬁculttoassessthevalidityofthebiasmeasureinhowitrelatestootherbenchmarks(criterionvalidity)andthemoreabstractnotionoffairness(constructvalidity).Someresearcherscircumventthisproblembyadapt-ingtheoriginalbiasmetric,butcareshouldbetakenwhendoingso.Forinstance,biasmetricsoriginallyde-velopedformaskedlanguagemodelshavebeenadaptedbyusingperplexity(e.g.Nadeemetal.,2020)orprompt-ing(e.g.Gaoetal.,2021;Sanhetal.,2021)instead.Whilethesecouldstillresultinimportantinsights,theyalsoopennewquestions.Aretheunderlyingassump-tionsofthebiasmeasurestillvalid?Canyoucomparethebiasmetricsacrossdifferent(future)typesofmod-els?Dotheresultsoftheinitialvalidationofthebench-markstillhold?Andhowdoesthekindoftrainingdataimpacttheevaluationthatassumesadifferenttrainingdomain(e.g.,legaltextsvs.socialmedia)?Whilebiasisideallydeﬁnedindependentlyoftheparticularmodelarchitecture—notleastbecauseimple-mentationschangeovertime—weshouldnotfallintoageneralizationtrapeither.Asarguedbefore,biasisin-herenttosystemsandcontext-sensitive,andweshouldnotstriveforapanaceabiasmeasure.Instead,thegoalshouldbetodevelopmethodsthataretask-speciﬁcyetindependentofagivenarchitecture,tothedegreethat

29

thisispossible.Researchersshouldkeepthistensionbe-tweentask-andarchitecture-speciﬁcmeasuresinmindwhendesigningmethodsformeasuringbiasesinLLMs.3.4BiasMeasuresareAnglo-centricDespitetheneedforevaluatingLLMsforawiderangeoflanguages,biasbenchmarksthatcovernon-Englishlanguagesarerare(Zhouetal.,2019;Joshietal.,2020).Asasolution,simplytranslatingexistingEn-glishbenchmarksisnotideal:manualtranslationisalabor-intensiveandhighlyskilledtask,whileautomatedtranslationsarepronetoerrorsandcouldpotentiallyintroducenewalgorithmicsourcesofbias.Moreover,translatedbenchmarksmayonlytestforAnglo-centricbiases,whichdonotnecessarilyholdinmanynon-Westernculturalcontexts.Forinstance,manygenderbiasevaluationsfocusonWesternprofessions,whicharegrammaticallygenderedinsomelanguages(Chenetal.,2021;Zhouetal.,2019)ormaynotcoverotherprevalentoccupationsoutsidetheU.S.(EscudéFontandCosta-jussà,2019).WinoMT(Stanovskyetal.,2019)isoneofthefewbenchmarksthatcoversmultiplelanguages,butitcomeswithitsowndownsides.Thesentencesaregeneratedfromtemplatesthatcapturealimitedrangeofactuallanguageuse;thesamplesaretranslatedfromEnglishexamples,whichmaynotre-ﬂecthowstereotypeswouldoccurinotherlanguages;andthescopeislimitedtomachinetranslationsystems,andthereforeWinoMTmaynotbesuitableformulti-lingualmodelsthatarenottrainedonthisspeciﬁctask.Thetightlycouplednatureofbiasandculturalcontextshouldbeemphasizedwhendesigningamultilingualbiasbenchmark.3.5ValidityofBiasMeasuresTowardsmakingNLPsystemsmorejust,wemustunder-standtheﬂawsofcommonbiasmeasuresanddevelopbetterguidelinestoaddressbiases.AccordingtoJacobsandWallach(2021)andBlodgettetal.(2021),biasmea-suresaremeasurementmodelswhichlinkobservableproperties,e.g.,quality-of-serviceandrepresentationalbiases,withunobservabletheoreticalconstructssuchassocialdiscrimination,powerdynamics,andsystemicoppression.Consequently,biasmeasuresaredeeplypolitical.Notably,avastmajorityofbiasmeasuresthemselvesrelyonothermeasurementmodels,suchasthepresenceofgenderedpronouns,toinfertheoreti-calprotectedcategories,e.g.,gender.Moreover,biasmeasuresmaycausefurtherepistemicviolenceontothemarginalizedbycreatingaveneeroffairness,inspiteofongoingmarginalization(GonenandGoldberg,2019;Talatetal.,2021;JacobsandWallach,2021).Inensur-ingthereliability,validity,andcorrectinterpretationofbiasmeasures,itiscriticaltoexamineallcomponentsinabiasmeasurementmethod.Upstreammeasurementmodelsthatinferprotectedcategoriescanbeunreliableorevennon-existent.Forinstance,pronounsandgenderednamesareusuallyem-ployedasproxiesforbinarygender,whichisproblem-atic(Devetal.,2021a).Furthermore,characteristicslikesexualityanddisabilityareusuallyunobservable,whichcanleadtoarelianceonhegemonicstereotypesandunnaturallanguageinbiasevaluationbenchmarks(Tomasevetal.,2021;Hutchinsonetal.,2020).Withregardtovalidity,Blodgettetal.(2021)reviewshowbiasmeasuresoftenrelyonoperationalizationofstereotypesthatareinvalidforreasonssuchasmisalign-mentandconﬂation.Additionally,themathematicalformalizationofmostbiasmeasuresisbasedonnotionsofparity-basedfairnessanddonotreﬂectothercon-ceptualizationsoffairnesssuchasdistributivejustice(JacobsandWallach,2021).Anothersourceofinvalid-ityofbiasmeasuresliesinthepurportedgeneralityofassociatedbenchmarks.Rajietal.(2021)arguethatthe“instantiation[ofbenchmarks]inparticulardata,metricsandpractice”underminesthevalidityoftheirconstruc-tiontohave“generalapplicability.”Moreover,mea-surementmodelsforprotectedcategoriesfallaciouslyassumethattheidentitiesbeingindirectlyobservedcanbediscretized.Hence,Devetal.(2021b)advocatefordocumentingthelimitationsofbiasmeasuresandre-lateddataintermsoftheirvalidity.Inthisprocess,itiscriticaltodescribetherelationshipbetweenthecontextofthedata,modelusage,andbiasmeasureatstake.4TheElephantintheRoom:Power,Privilege,andPointofViewThroughoutthepaper,wehaveprimarilydiscussedbiasinlanguagemodelsasamechanicalphenomenon.How-ever,itisimportanttosituatethesediscussionswithinthecontextandpowerdynamicsofthewaythatNLPispracticed—bothinresearchandinapplication(Micelietal.,2022).Inthissection,wediscusssociopoliticalin-ﬂuencesonAIethicsandbiasresearchinNLP.WearguethatcontemporarydevelopmentsofLLMshavebeenanexerciseinﬁnancial,institutional,ecological,linguis-tic,andculturalprivilege.Theyaretheconsequenceofthepoliticalwilltocreatetotalizingtechnologiesandevaluationofbias,fairnessandsocialimpactshouldbeviewedasacountervailingpowermechanism,althoughinsomecasesservetoobscurethese.4.1LargeLanguageModelsareExpensiveThecurrentdominantparadigminnaturallanguagepro-cessingisdrivenbythecreationofever-largerpretrainedtransformermodels(Brownetal.,2020).AsthesizeofLLMsincreases,sodotherequirementsforhard-ware,energy,andtime.Forexample,GPT-NeoX20B(Blacketal.,2022)wastrainedfor1830hourson96A100GPUs,consuming43.92MWhofelectricityandemitting23metrictonsofCO2.Basedonthecurrentpricelistingofthecloudproviderthemodelwastrainedon,trainingsuchamodelwouldcostbetween250,000and500,000USD.4Whilethisisnotonthescaleofthe

4Thelowerendofthisrangereﬂectsthecommonpracticeofgivingdiscountsofupto50%forlargepurchases,while

30

largestresearchprograms,itisasigniﬁcantamountofmoneyandbeyondthefundingofmanyinstitutions,orbeyondtheirpoliticalwilltospend.Whilethedevelopmentofsuchmodelscancontributetowardsimprovingtheabilityofpeoplewithlessre-sourcestopursuecuttingedgedownstreamresearch,suchpursuitshavesigniﬁcantcostsandbarrierstoentryforupstreamresearch.Thiscreatesastratiﬁcationofresearch,whereinmoneyisabarrierofentryforsomeformsofresearchbutnotforothers.4.2LanguageisMulticultural,LanguageModelsareNotAlthoughtherearethousandsofspokenlanguagesintheworld,theoverwhelmingmajorityofLLMsaremonolingualandencodewhiterespectabilitypolitics(ThylstrupandTalat,2020;Kerrisonetal.,2018)ontominoritizedvariantsofEnglish(Gehmanetal.,2020).Inthisway,thecostofthedevelopingLLMsextendsfromexternalizingcomputationalandinfrastructuralcosts,toexternalizinglanguagesandlanguagevariants(Lau,2021).Speciﬁcally,thevastmajorityofLLMsaretrainedtooperateonanunspeciﬁedvariantof“English”(Bender,2019),andinsomecasesChinese(seeTable1foradetailedoverviewofthetop25LLMs).Thedomi-nanceofEnglish,andtoalesserdegreeChinese,reiﬁesculturalhegemoniesandprecipitatestechnologicalim-perialism.Evenwhenresearchersseektoincludeotherlanguages,thesepurportedlymultilingualmodelsoftenunderservecertainlanguagesandcommunities(Kerri-sonetal.,2018;Virtanenetal.,2019;Kreutzeretal.,2022;Gururanganetal.,2022).Wealsonotethatfewofthesemodelshavebeenassessedforbiasorfairness(seetable1).Thisactreliesontwofoundations.First,LLMsshouldonlybeusedforlanguagesthattheyhavebeendevelopedfor,withtheculturalstereotypesthattheyhavebeentrainedon,thuslimitingLLMstobeusedwithinasmallsetofculturalcontexts,orcastingculturalcontextsforwhichtheyaretrainedontoonesthattheyarenotdevelopedfor.Second,shouldamultilingualLLMbetrained,itsprimarydatasourceswillstillbeinEnglish,whereastheremaininglanguageswillonlybeincidentaltoit.Suchculturalimperialismisevidentfromthefactthatonly2ofthe14organizationsinvolvedindevelopingLLMshaveteamsinmultiplecountries(seetable1).Further,allmultinationalLLMefforts,exceptforone,drawtheirmembershipfromtheUSA,UK,Germany,&Australia.GPT-NeoX20B(Blacketal.,2022)isanexception,asitalsoincludesauthorsfromIndia.Acommonly-usedresourcefordevelop-ingLLMs,CommonCrawl,reliesondatathatprimarilystemsfromtheUS(Dodgeetal.,2021)andiswritteninprivilegeddialectsofEnglish(Dunn,2020).Thisprioritizationisreﬂectedby16teamsbeingphysicallylocatedintheU.S.Consequently,thecurrentstateofLLMdevelopmentisatotalizingendeavor(Talatetal.,

theupperendreﬂectsthestickerpriceofthesystems.2021),whichengagesinexternalizationacrossanum-berofaxes,asisapparentfromtheinfrastructuralanddevelopmentpracticesandtheeffortstoevaluateandmitigatesocialharmsthatarisefromsuchtechnologies.4.3LargeLanguageModelsAllowPowerfulActorstoControlNLPResearchDuetothecostsinvolvedwithtraininglargelanguagemodelsandthesmallnumberofactorswhohavede-cidedtotrainthem,theoverwhelmingmajorityofre-searchstudyingtheirpropertiesisnotcarriedoutbypeoplewhotrainLLMs.Whentheactorsthatdopos-sessthemodelschoosetonotpubliclyreleasethem,modeltrainersareaffordedcontrolovertheresearchthatcanbeconductedwithandbythesemodels.Famously,OpenAI’sinitialannouncementofGPT-3assertedthataccesstothemodelwouldbeheavilyrestrictedwhilethecompanycontinuedtoresearchethicalinterventionsintheirmodel.OpenAIisnotaloneinthis;theideathatitisinherentlydangeroustoreleasemodelstothepublichasbeenputforthbyseveralotheractorsinthisspace(Weidingeretal.,2021a;Askelletal.,2021).Itisessentialtorecognizethatthedecisionsregardingaccessandthekindofresearchthatcanbeconductedonlargelanguagemodels(oranyMLmodels,forthatmat-ter)isaninherentlypoliticalone(LeahyandBiderman,2021).Regardlessofthetruthoftheaforementionedclaims,theyarehighlycontentiouspoliticalclaimsandshouldbetreatedassuchratherthanpassivelyaccepted.DirectaccesstoLLMsisimportanttoperforminde-pendentresearchontheirdatasets,functions,andsoci-etalimpact(Kandpaletal.,2022;Carlinietal.,2022).Whilelanguagemodelsproducedbytheacademicre-searchcommunityarewidelyavailableforcriticalex-amination,commercialsystemsareoftenonlyavailablethroughAPIsprovidedbythedevelopers(seetable1foranoverviewonaccessforthe25largestpretrainedlanguagemodels.Suchrestrictionstoaccesstothemod-elsandresourcesthattheyaredevelopedforprovideasigniﬁcantbarriertoa)principlesofopenscienceandb)researchonhowthedatasetsandlanguagemodelsthemselvesembedandamplifysocialbiases.5AddressingBiasResearchershavedevelopedvariousstrategiestoad-dressbiasinlargelanguagemodels.Asdiscussedinearliersections,however,thesestrategiesareinsufﬁ-cienttotacklemultipledimensionsofbias.Below,weenumerateafewwaysinwhichbiascanbeaddressedbytheresearchcommunitytoeffectivelyengagewithouraforementionedconcerns:(1)movingtowardsamoretransparentwayofevaluatingbias,(2)focusingonthediversityofstereotypesandincreasinginclusivity,and(3)consideringtheimpactoflinguisticandculturaldifferencesontheidentiﬁcationandmitigationofbiasindesigningculturallycomparabledatasets.Wewouldliketohighlightthatthesesuggestionsarenotexhaus-tive.Theywill,however,guidetheworkinthisarea.

31

5.1TransparencyThroughDocumentationStereotypesandbiasescoverabroaddeﬁnitionandvaryinconceptualizationacrossgeographicalandculturalcontexts.Toensurethatthenuancesarewellcommuni-catedandthatpractitionersunderstandtheapplicabilityoftheevaluationapproach,wesuggestdocumentingathoroughanalysisofthescope.Below,weprovideastartingpointbasedonMitchelletal.(2019);Gebruetal.(2018);Devetal.(2021b);Blodgettetal.(2020).DeﬁningthescopeoftheapproachBlodgettetal.(2020)foundthatworksaroundbias"oftenfailtoex-plainwhatkindsofsystembehaviorsareharmful,inwhatways,towhom,andwhy."Itthusbecomesimper-ativetoquestionwhatunderrepresentedgroupswouldbeneﬁtmorefromagivenevaluationbenchmark.Wethereforeurgeresearchersandpractitionerstoclearlyspecifythedemographicaparticularmethodisrele-vantfor.Moreover,givenhowsocialhierarchiesinter-twinetightlywithlanguageandmaypresentthemselvesthroughitspeculiarities,wealsoencourageresearcherstospecifythelimitationsandscopeoftheirapproaches.Asanexample,weconsiderthegenderbiasevalu-ationinEnglish(Zhaoetal.,2018;Stanovskyetal.,2019;Levyetal.,2021;Sharmaetal.,2021),wherethebiasmightpresentitselfthroughstrongassociationsbe-tweengrammaticalconstructslikepronouns.Thesamedoesnotholdtrueforgenderlesslanguages,despitetheexistenceofthebias(Zmigrodetal.,2019).Thus,evaluationbenchmarksandapproachesdonotalwaystransferwelltootherlanguages.Additionally,whilesuchbenchmarksusegenderassociationstoprofessionsfortheirevaluation,thismethodcoversonlyoneaspectofthesocialhierarchy,anddoesnotaddressgenderbiasinlanguageinitsentirety.BybeingbinaryinnatureandtightlycoupledtoAnglo-centriccontexts(see§3)bench-marksarelimitedintheirscopeandrelevance.Whilemostrecentworksdoincludeethicalconsiderations,thelimitationsandscopeareonlyvaguelyspeciﬁed.Wead-vocateforsuchlimitationstobehighlightedandpointedoutforthecommunitytohaveaclearerpictureaboutthestepsthatneedtobetakentowardsgreaterinclusivity.DocumentingthedemographicsPreviousworkhashighlightedtheimportanceofengagingwithindividualsonthereceivingendofthebias(Benderetal.,2021).Itthusbecomesimportanttounderstandthedemographicsofthoseinvolvedinthecreationofthebenchmarks.Aspreviouslyshown(AlKuwatlyetal.,2020)thereexistsarelationbetweenannotators’identitiesandtoxicity/biasindataset.Onthisbasis,weurgetheresearcherstocollectanddocumentthedemographicinformationandannotatorattitudescores(Sapetal.,2021).Buildinguponthesame,weencouragethecollectionandreport-ingofthisinformationabouttheresearchersinvolved.5.2DiversityBeyondGenderBiasThemajorityofpreviousworkonbiashasfocusedpar-ticularlyongenderbias(Zhaoetal.,2018;Stanovskyetal.,2019;Levyetal.,2021;Sharmaetal.,2021)andtheveryfewworks(Nadeemetal.,2020;Nangiaetal.,2020)thattakeotherdimensionsofbiasesintoaccount,havetheirownshortcomings,asdiscussedinSection3.Itthusbecomesimportanttodiversifytherangeofbiasandstereotypesthatarebeinginvestigatedbyresearch,andcoveredbyacertainevaluationtechnique.Inex-tendingthecoveragetomoredimensions,contextstandsasanimportantaspectofbias.Thecontextualaspectsofbiasasrepresentedinlanguage,culture,andhistoryholdasigniﬁcantroleinformingandassessingthebiasitself.Hence,asapractice,weencourageresearcherstoconsiderthesethreeaspectswhenconstructingbiasmeasuresanddatasets.Indiscussingbias,itisimportanttonotethatdiscrim-inationdoesnotoccurinavacuum.Anactofdiscrimi-nationagainstapersonmaybedirectedtowardsseveralintersectingidentities.Consideringbiasusingasingle-axisframeworkmakesitimpossibletoengagewithandevaluatetheharmsextendedtothesocialgroupsthatlieattheintersectionofmultipleidentities(Crenshaw,1991).InanIndiancontext,forexample,eventhosewhoidentifyasbelongingtothe“same”caste(Maliketal.,2021),canhavevariedlivedexperiencesbasedonclass,gender,andotheridentities.Moreprecisely,itisimpossibletodisentanglewhichspeciﬁcidentityadiscriminatoryactisdirectedagainst.Previousworkshavehighlightedtheimportanceofstudyingintersec-tionalbias(Benderetal.,2021;BuolamwiniandGebru,2018;Fieldetal.,2021;Guoetal.,2019;Crenshaw,1991)butlittleresearchhasbeenconductedaroundaddressingsuchbiases(Mageeetal.,2021;GuoandCaliskan,2021).Wethusencourageresearcherstode-velopmeasuresandbenchmarkswhicharegroundedinintersectionalunderstandingofbiasandadequatelyaddressthelivedexperiencesofvarioussocialgroups,towardsincreasedinclusivityandfairness.Notonlycanthedimensionsandcontextinﬂuenceourdeﬁnitionsandapproachestobias,butthecategories(values)assignedtoeachdimension(e.g.,age)canalsolimitourunderstandingandsolutionofbias.Forin-stance,themajorityofgender-biasevaluationdatasetssolelydealwithbinarygender,i.e.,maleandfemale,withjustahandfulcoveringnon-binarygenderswithonlyminimalrepresentation(Devetal.,2021a;CaoandDauméIII,2020).Asaresult,categoryinclusivenessiscriticalinthedevelopmentofahigh-qualitybiasevalua-tiondataset.AsetofcategoriesthatcanactasastartingpointareprovidedbyQueerinAIinSection3.2.5.3AcknowledgingDifferencesStereotypeandbiasformationisinﬂuencedbyculture.Asaresult,whatmightbeastereotypeinagivenculturemightnotstandrelevantinanother.Forin-stance,thecharacterizationthatparentalleaveisformothersisconsideredstereotypicalintheUnitedStates,butnotinSweden,whereparentalleaveissplitbetweenbothparents.

32

OrganizationAuthorLocationLanguageParametersModelAccessBiasEval

MT-NLGMicrosoft,NVIDIAUSAEnglish530BClosedSmithetal.(2022)GopherDeepMindUSAEnglish280BClosedWeidingeretal.(2021b)ERNIE3.0BaiduChinaEnglish,Chinese260BClosed—Yuan1.0InspurAIChinaChinese245BClosed—HyperCLOVANAVERKoreaKorean204BClosed—PanGu-αHuaweiChinaChinese200BClosed—Jurassic-1AI21LabsIsraelEnglish178BCommercial—GPT-3OpenAIUSAEnglish175BCommercialBrownetal.(2020)LaMDAGoogleUSAEnglish137BClosedThoppilanetal.(2022)AnthropicLMAnthropicUSAEnglish52BClosedAskelletal.(2021)GPT-NeoX-20BEleutherAIMultinationalEnglish20BOpen(Gaoetal.,2020;Bidermanetal.,2022)TuringNLGMicrosoftUSAEnglish17BClosed—FairSeqDenseMetaAIMultinationalEnglish13BOpen—mT5GoogleUSAMultilingual13BOpen—ByT5GoogleUSAEnglish13BOpen—T5GoogleUSAEnglish11BOpen—CPM2.1TsinghuaUniversityChinaChinese11BOpen—Megatron11BNVIDIAUSAEnglish11BOpen—WuDao-GLM-XXLBeijingAcademyofAIChinaChinese10BOpen—WuDao-GLM-XXLBeijingAcademyofAIChinaEnglish10BOpen—BlenderBotMetaAIUSAEnglish9BOpen—Megatron-LMNVIDIAUSAEnglish8BClosed—XGLMMetaAIMultinationalMultilingual7BOpen—GPT-J-6BEleutherAIMultinationalEnglish6BOpen(Gaoetal.,2020;Bidermanetal.,2022)

Table1:The25largestpretraineddenselanguagemodels,rangingfrom6billionparametersto530billion.ModelsareoverwhelminglytrainedbyteamslocatedintheUSandonEnglishtext.Lessthanhalfofthelanguagemodelswereevaluatedforbiasbytheircreators.PrevioussectionshavecriticizedtheAnglo-centricityintheresearchofNLPbiasandtheinﬂuenceonlan-guagesotherthanEnglish.Inparticular,thelackofculturally-awaredatasetslimitsthedegreetowhichfu-tureNLPalgorithmscanbeevaluatedforbiases.Morecrucially,theseunspeciﬁedlanguagesandculturesareonthereceivingendofunmanagedeffects.Asare-sult,researchersareencouragedtodevelopbiasdatasetsandbenchmarksfornonAnglo-centricculturesandlan-guages(Benderetal.,2021).Involvingexpertsinre-latedareas,especiallyparticipantswithlivedexperi-encesoflanguage-relatedharms,mightaiddecisionsatallpartsofthisprocess,e.g.decidingwhatgroupsandcontenttoincludeinresearchordatasetdesign(LiaoandMuller,2019;Devetal.,2021a;McMillan-Majoretal.,2022).Overall,havingculturallydiverseandcom-parabledatasetsforadiversesetoflanguages(ideallycoveringalllanguages)iscriticalforevaluatingmul-tilingualmodels.Moreover,theapplicabilityofbiasmeasuresacrossvariouslanguagessuggeststheneces-sityforcross-linguisticmetricsormeasurementsthatcanbeextendedtodifferentlanguagesorcultures(Zhouetal.,2019;EscudéFontandCosta-jussà,2019;Maliketal.,2021).6ConclusionRecentimprovementsinLLMstomimichumantexthaveledtoasurgeinresearchthatseekstoidentifyandaddresstheharmsarisingfromtheirtrainingandde-ployment.However,theconsiderationsonsocialharmsthatarisehasbeenlimitedtonarrow,Anglo-centric,contradictory,andoftenunderspeciﬁeddeﬁnitionsoffairnessandbias.Furthermore,thedevelopmentofcontemporarymethodshasconﬂatedtask-speciﬁcandarchitecture-speciﬁcdesignations.Compoundedwiththestructuralinequalitiesaroundresources,language,andidentity,thishasyieldedanoverrelianceonprestigeformsofEnglishfordevelopingLLMsandinterrogat-ingandaddressingthesocialbiasesthattheyharbor.SituatingthesemethodswithinsuchEnglisheshashadtheconsequenceofover-emphasizingWestern-centricsocialcategories.Moreover,datasetsforevaluatingso-cialbiasesinLLMshavetraditionallyfailedtodenoteandspecifythecontextwithinwhichbiasesaresituated.Suchconcernshavebeenthecauseforquestionsaroundthevalidityofthedevelopedmeasures,andinparticularformultilingualLLMs.Toaddresssuchchallenges,weproposethatdevelop-ingmethodsformultilingualLLMsrequiresresearcherstoprovidethoroughdocumentationoftheirapproaches,includingdocumentingthescope,demographicsofspeakers,andpotentialannotators.Additionally,wealsorecommendthatresearcherssituatetheirbiasevaluationmethodswithinthespeciﬁccontextofthelanguagesthatthemodeloperateson.Indoingso,biasevaluationmethodscanbemadetospeciﬁcallyaddressbiasesun-dertheconditionsandcontextsthattheyoccurineachofthemodel’slanguages.Furthermore,werecommendthatresearchersexaminediversityissuesbeyondgen-derbias,withaparticularfocusonintersectionalissues(GuoandCaliskan,2021).Finally,werecommendthatresearchersarecognizantofthesocialandenvironmentalharmsthatdevelopingLLMshave.Forinstance,developingever-largerlan-guagemodelsthatachievemarginalimprovementsforEnglishmaybringasmallerbeneﬁtthandevelopingaLLMforotherlanguages.Thus,inaconsiderationofde-velopinganewlanguagemodel,weimploreresearcherstoconsiderwaysinwhichharmscanbelimited,orthebeneﬁtscancometocompensatefortheircosts.

33

ReferencesAbubakarAbid,MaheenFarooqi,andJamesZou.2021.PersistentAnti-MuslimBiasinLargeLan-guageModels.InAIES2021-Proceedingsofthe2021AAAI/ACMConferenceonAI,Ethics,andSoci-ety.DanielaAgostinho,CatherineD’Ignazio,AnnieRing,NannaBondeThylstrup,andKristinVeel.2019.Un-certainArchives:ApproachingtheUnknowns,Er-rors,andVulnerabilitiesofBigDatathroughCulturalTheoriesoftheArchive.Surveillance&Society,17(3/4):422–441.BlaiseAguerayArcas,AlexanderTodorov,andMar-garetMitchell.2018.Doalgorithmsrevealsexualorientationorjustexposeourstereotypes?HalaAlKuwatly,MaximilianWich,andGeorgGroh.2020.Identifyingandmeasuringannotatorbiasbasedonannotators’demographiccharacteristics.InPro-ceedingsoftheFourthWorkshoponOnlineAbuseandHarms,pages184–190,Online.AssociationforComputationalLinguistics.LizAllen,AlisonO’Connell,andVeroniqueKiermer.2019.Howcanweensurevisibilityanddiversityinresearchcontributions?HowtheContributorRoleTaxonomy(CRediT)ishelpingtheshiftfromau-thorshiptocontributorship.LearnedPublishing,32(1):71–74.SaleemaAmershi,EceKamar,KristinLauter,JennWortmanVaughan,andHannaWallach.2020.Re-searchSupportingResponsibleAI.AmandaAskell,YuntaoBai,AnnaChen,DawnDrain,DeepGanguli,TomHenighan,AndyJones,NicholasJoseph,BenMann,NovaDasSarma,NelsonEl-hage,ZacHatﬁeld-Dodds,DannyHernandez,Jack-sonKernion,KamalNdousse,CatherineOlsson,DarioAmodei,TomBrown,JackClark,SamMc-Candlish,ChrisOlah,andJaredKaplan.2021.Agenerallanguageassistantasalaboratoryforalign-ment.SolonBarocas,KateCrawford,AaronShapiro,andHannaWallach.2017.Theproblemwithbias:fromallocativetorepresentationalharmsinmachinelearn-ing.specialinterestgroupforcomputing.Informa-tionandSociety(SIGCIS),2.SolonBarocasandAndrewD.Selbst.2016.BigData’sDisparateImpact.CaliforniaLawReview,104(3).EmilyBender.2019.The#BenderRule:OnNamingtheLanguagesWeStudyandWhyItMatters.EmilyM.BenderandBatyaFriedman.2018.DataStatementsforNaturalLanguageProcessing:TowardMitigatingSystemBiasandEnablingBetterScience.TransactionsoftheAssociationforComputationalLinguistics,6:587–604.EmilyM.Bender,TimnitGebru,AngelinaMcMillan-Major,andShmargaretShmitchell.2021.Onthedangersofstochasticparrots:Canlanguagemodelsbetoobig?

.InProceedingsofthe2021ACMConferenceonFairness,Accountability,andTrans-parency,FAccT’21,page610–623,NewYork,NY,USA.AssociationforComputingMachinery.EmilyM.BenderandAlexanderKoller.2020.ClimbingtowardsNLU:OnMeaning,Form,andUnderstand-ingintheAgeofData.InProceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics,pages5185–5198,Online.AssociationforComputationalLinguistics.RuhaBenjamin.2019.Raceaftertechnology:aboli-tionisttoolsforthenewJimcode.Polity,Medford,MA.StellaBiderman,KieranBicheno,andLeoGao.2022.DatasheetforthePile.arXiv:2201.07311[cs].ArXiv:2201.07311.StellaBidermanandWalterScheirer.2020.Pitfallsinmachinelearningresearch:Reexaminingthedevel-opmentcycle.In“ICan’tBelieveIt’sNotBetter!”NeurIPS2020workshop.AbebaBirhane,PratyushaKalluri,DallasCard,WilliamAgnew,RavitDotan,andMichelleBao.2021.TheValuesEncodedinMachineLearningResearch.arXiv:2106.15590[cs].ArXiv:2106.15590.SidBlack,StellaBiderman,EricHallahan,QuentinAn-thony,LeoGao,LaurenceGolding,HoraceHe,Con-norLeahy,KyleMcDonell,JasonPhang,MichaelPieler,USVSNSaiPrashanth,ShivanshuPurohit,LariaReynolds,JonathanTow,BenWang,andSamuelWeinbach.2022.GPT-NeoX-20B:AnOpen-SourceAutoregressiveLanguageModel.SuLinBlodgett,SolonBarocas,HalDauméIII,andHannaWallach.2020.Language(Technology)isPower:ACriticalSurveyof“Bias”inNLP.InPro-ceedingsofthe58thAnnualMeetingoftheAssocia-tionforComputationalLinguistics,pages5454–5476,Online.AssociationforComputationalLinguistics.SuLinBlodgett,GilsiniaLopez,AlexandraOlteanu,RobertSim,andHannaWallach.2021.StereotypingNorwegianSalmon:AnInventoryofPitfallsinFair-nessBenchmarkDatasets.InProceedingsofthe59thAnnualMeetingoftheAssociationforComputationalLinguisticsandthe11thInternationalJointConfer-enceonNaturalLanguageProcessing(Volume1:LongPapers),pages1004–1015,Online.AssociationforComputationalLinguistics.SuLinBlodgett,JohnnyWei,andBrendanO’Connor.2018.TwitterUniversalDependencyparsingforAfrican-AmericanandmainstreamAmericanEnglish.InProceedingsofthe56thAnnualMeetingoftheAs-sociationforComputationalLinguistics(Volume1:LongPapers),pages1415–1425,Melbourne,Aus-tralia.AssociationforComputationalLinguistics.TolgaBolukbasi,Kai-WeiChang,JamesYZou,VenkateshSaligrama,andAdamTKalai.2016.Man

34

istocomputerprogrammeraswomanistohome-maker?debiasingwordembeddings.InAdvancesinNeuralInformationProcessingSystems,volume29.CurranAssociates,Inc.ShikhaBordiaandSamuelR.Bowman.2019.Iden-tifyingandReducingGenderBiasinWord-LevelLanguageModels.InProceedingsofthe2019Con-ferenceoftheNorth,pages7–15,Minneapolis,Min-nesota.AssociationforComputationalLinguistics.TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredD.Kaplan,PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,ArielHerbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,JeffreyWu,ClemensWinter,ChrisHesse,MarkChen,EricSigler,Ma-teuszLitwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRad-ford,IlyaSutskever,andDarioAmodei.2020.Lan-guageModelsareFew-ShotLearners.InAdvancesinNeuralInformationProcessingSystems,volume33,pages1877–1901.CurranAssociates,Inc.JoyBuolamwiniandTimnitGebru.2018.GenderShades:IntersectionalAccuracyDisparitiesinCom-mercialGenderClassiﬁcation.InProceedingsofthe1stConferenceonFairness,AccountabilityandTransparency,volume81ofProceedingsofMachineLearningResearch,pages77–91,NewYork,NY,USA.PMLR.AylinCaliskan,JoannaJ.Bryson,andArvindNarayanan.2017.Semanticsderivedautomaticallyfromlanguagecorporacontainhuman-likebiases.Science,356(6334).AlexCampolo,MadelynSanﬁlippo,MeredithWhit-taker,andKateCrawford.2018.AInow2017report.InAInow2017symposiumandworkshop.AINowInstituteatNewYorkUniversity.Edition:AINow2017SymposiumandWorkshop.YangTristaCaoandHalDauméIII.2020.TowardGender-InclusiveCoreferenceResolution.InPro-ceedingsofthe58thAnnualMeetingoftheAssocia-tionforComputationalLinguistics,pages4568–4595,Online.AssociationforComputationalLinguistics.NicholasCarlini,DaphneIppolito,MatthewJagielski,KatherineLee,FlorianTramer,andChiyuanZhang.2022.Quantifyingmemorizationacrossneurallan-guagemodels.arXivpreprintarXiv:2202.07646.AlanChan,ChinasaT.Okolo,ZacharyTerner,andAn-gelinaWang.2021.TheLimitsofGlobalInclusioninAIDevelopment.arXiv:2102.01265[cs].ArXiv:2102.01265.JohnChen,IanBerlot-Attwell,XindiWang,SafwanHossain,andFrankRudzicz.2020.Exploringtextspeciﬁcandblackboxfairnessalgorithmsinmulti-modalclinicalNLP.InProceedingsofthe3rdClini-calNaturalLanguageProcessingWorkshop,pages301–312,Online.AssociationforComputationalLin-guistics.YanChen,ChristopherMahoney,IsabellaGrasso,EsmaWali,AbigailMatthews,ThomasMiddleton,MariamaNjie,andJeannaMatthews.2021.GenderBiasandUnder-RepresentationinNaturalLanguageProcessingAcrossHumanLanguages.InProceed-ingsofthe2021AAAI/ACMConferenceonAI,Ethics,andSociety,pages24–34,VirtualEventUSA.ACM.LuCheng,AhmadrezaMosallanezhad,ParasSheth,andHuanLiu.2021.CausalLearningforSociallyRe-sponsibleAI.InProceedingsoftheThirtiethInterna-tionalJointConferenceonArtiﬁcialIntelligence.JennyCheshire.2007.Anuntitledreviewof“styleandsociolinguisticvariation”.Language,83(2):432–435.WendyHuiKyongChun.2021.Discriminatingdata:correlation,neighborhoods,andthenewpoliticsofrecognition.TheMITPress,Cambridge,Mas-sachusetts.SashaCostanza-Chock.2018.DesignJustice,A.I.,andEscapefromtheMatrixofDomination.JournalofDesignandScience.KimberleCrenshaw.1991.Mappingthemargins:In-tersectionality,identitypolitics,andviolenceagainstwomenofcolor.StanfordLawReview,43(6):1241–1299.PaulaCzarnowska,YogarshiVyas,andKashifShah.2021.QuantifyingSocialBiasesinNLP:AGen-eralizationandEmpiricalComparisonofExtrinsicFairnessMetrics.TransactionsoftheAssociationforComputationalLinguistics,9:1249–1267.DebajyotiDatta,JasonA.Fries,MichaelMcKenna,AurélieNévéol,VassilinaNikoulina,andMayaVarma.2021.Challengesinlanguagemodellingforbiomedicine.MariaDe-Arteaga,AlexeyRomanov,HannaWal-lach,JenniferChayes,ChristianBorgs,AlexandraChouldechova,SahinGeyik,KrishnaramKenthapadi,andAdamTaumanKalai.2019.BiasinBios:ACaseStudyofSemanticRepresentationBiasinaHigh-StakesSetting.InProceedingsoftheConferenceonFairness,Accountability,andTransparency,pages120–128,AtlantaGAUSA.ACM.DanieldeVassimonManela,DavidErrington,ThomasFisher,BorisvanBreugel,andPasqualeMinervini.2021.StereotypeandSkew:QuantifyingGenderBiasinPre-trainedandFine-tunedLanguageModels.InProceedingsofthe16thConferenceoftheEuro-peanChapteroftheAssociationforComputationalLinguistics:MainVolume,pages2232–2242,Online.AssociationforComputationalLinguistics.SunipaDev,TaoLi,JeffM.Phillips,andVivekSriku-mar.2020.Onmeasuringandmitigatingbiasedinfer-encesofwordembeddings.ProceedingsoftheAAAIConferenceonArtiﬁcialIntelligence,34(05):7659–7666.

35

SunipaDev,MasoudMonajatipoor,AnaeliaOvalle,Ar-junSubramonian,JeffPhillips,andKai-WeiChang.2021a.Harmsofgenderexclusivityandchallengesinnon-binaryrepresentationinlanguagetechnologies.InProceedingsofthe2021ConferenceonEmpiri-calMethodsinNaturalLanguageProcessing,pages1968–1994,OnlineandPuntaCana,DominicanRe-public.AssociationforComputationalLinguistics.SunipaDev,EmilySheng,JieyuZhao,JiaoSun,YuHou,MattieSanseverino,JiinKim,NanyunPeng,andKai-WeiChang.2021b.Whatdobiasmeasuresmeasure?JwalaDhamala,TonySun,VarunKumar,SatyapriyaKrishna,YadaPruksachatkun,Kai-WeiChang,andRahulGupta.2021.BOLD:DatasetandMetricsforMeasuringBiasesinOpen-EndedLanguageGener-ation.InProceedingsofthe2021ACMConferenceonFairness,Accountability,andTransparency,pages862–872,VirtualEventCanada.ACM.CatherineD’IgnazioandLaurenF.Klein.2020.Datafeminism.Strongideasseries.TheMITPress,Cam-bridge,Massachusetts.VirginiaDignum.2017.Responsibleartiﬁcialintelli-gence:Designingaiforhumanvalues.ICTDiscover-ies.JesseDodge,MaartenSap,AnaMarasovi´c,WilliamAgnew,GabrielIlharco,DirkGroeneveld,MargaretMitchell,andMattGardner.2021.Documentinglargewebtextcorpora:Acasestudyonthecolos-salcleancrawledcorpus.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages1286–1305,OnlineandPuntaCana,DominicanRepublic.AssociationforComputationalLinguistics.MaryDouglas.1978.Purityanddanger:ananalysisoftheconceptsofpollutionandtaboo,repredition.Routledge,London.OCLC:248038797.JonathanDunn.2020.Mappinglanguages:theCorpusofGlobalLanguageUse.LanguageResourcesandEvaluation,54(4):999–1018.JoelEscudéFontandMartaR.Costa-jussà.2019.Equalizinggenderbiasinneuralmachinetranslationwithwordembeddingstechniques.InProceedingsoftheFirstWorkshoponGenderBiasinNaturalLan-guageProcessing,pages147–154,Florence,Italy.AssociationforComputationalLinguistics.AnjalieField,SuLinBlodgett,ZeerakWaseem,andYuliaTsvetkov.2021.ASurveyofRace,Racism,andAnti-RacisminNLP.InProceedingsofthe59thAnnualMeetingoftheAssociationforComputationalLinguisticsandthe11thInternationalJointConfer-enceonNaturalLanguageProcessing(Volume1:LongPapers),pages1905–1925,Online.AssociationforComputationalLinguistics.NancyFraser.1990.RethinkingthePublicSphere:AContributiontotheCritiqueofActuallyExistingDemocracy.SocialText,(25/26):56.SorelleA.Friedler,CarlosScheidegger,andSureshVenkatasubramanian.2021.The(Im)possibilityoffairness:differentvaluesystemsrequiredifferentmechanismsforfairdecisionmaking.Communica-tionsoftheACM,64(4):136–143.LeoGao,StellaBiderman,SidBlack,LaurenceGold-ing,TravisHoppe,CharlesFoster,JasonPhang,HoraceHe,AnishThite,NoaNabeshima,ShawnPresser,andConnorLeahy.2020.ThePile:An800GBDatasetofDiverseTextforLanguageModel-ing.arXiv:2101.00027[cs].ArXiv:2101.00027.TianyuGao,AdamFisch,andDanqiChen.2021.Mak-ingpre-trainedlanguagemodelsbetterfew-shotlearn-ers.InProceedingsofthe59thAnnualMeetingoftheAssociationforComputationalLinguisticsandthe11thInternationalJointConferenceonNaturalLan-guageProcessing(Volume1:LongPapers),pages3816–3830,Online.AssociationforComputationalLinguistics.AparnaGarimella,AkhashAmarnath,KiranKumar,AkashPramodYalla,AnandhaveluN,NiyatiChhaya,andBalajiVasanSrinivasan.2021.Heisveryintel-ligent,sheisverybeautiful?OnMitigatingSocialBiasesinLanguageModellingandGeneration.InFindingsoftheAssociationforComputationalLin-guistics:ACL-IJCNLP2021,pages4534–4545,On-line.AssociationforComputationalLinguistics.TimnitGebru,JamieMorgenstern,BrianaVecchione,JenniferWortmanVaughan,HannaWallach,HalDauméIII,andKateCrawford.2018.DatasheetsforDatasets.arXiv:1803.09010[cs].ArXiv:1803.09010.SamuelGehman,SuchinGururangan,MaartenSap,YejinChoi,andNoahA.Smith.2020.RealToxi-cityPrompts:EvaluatingNeuralToxicDegenerationinLanguageModels.InFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pages3356–3369,Online.AssociationforComputationalLinguistics.LisaGitelman.2013.Rawdataisanoxymoron.MITpress.HilaGonenandYoavGoldberg.2019.LipstickonaPig:DebiasingMethodsCoverupSystematicGen-derBiasesinWordEmbeddingsButdonotRemoveThem.InProceedingsofthe2019ConferenceoftheNorth,pages609–614,Minneapolis,Minnesota.AssociationforComputationalLinguistics.AnhongGuo,EceKamar,JenniferWortmanVaughan,HannaWallach,andMeredithRingelMorris.2019.Towardfairnessinaiforpeoplewithdisabilities:Aresearchroadmap.WeiGuoandAylinCaliskan.2021.DetectingEmergentIntersectionalBiases:ContextualizedWordEmbed-dingsContainaDistributionofHuman-likeBiases.InProceedingsofthe2021AAAI/ACMConferenceonAI,Ethics,andSociety,pages122–133,VirtualEventUSA.ACM.

36

SuchinGururangan,DallasCard,SarahK.Dreier,EmilyK.Gade,LeroyZ.Wang,ZeyuWang,LukeZettlemoyer,andNoahA.Smith.2022.WhoseLan-guageCountsasHighQuality?MeasuringLanguageIdeologiesinTextDataSelection.arXiv:2201.10474[cs].ArXiv:2201.10474.DonnaHaraway.1988.SituatedKnowledges:TheSci-enceQuestioninFeminismandthePrivilegeofPar-tialPerspective.FeministStudies,14(3):575–599.DirkHovyandShrimaiPrabhumoye.2021.Fivesourcesofbiasinnaturallanguageprocessing.Lan-guageandLinguisticsCompass,15(8):e12432.PoSenHuang,HuanZhang,RayJiang,RobertStan-forth,JohannesWelbl,JackW.Rae,VishalMaini,DaniYogatama,andPushmeetKohli.2020.Reduc-ingsentimentbiasinlanguagemodelsviacounter-factualevaluation.InFindingsoftheAssociationforComputationalLinguisticsFindingsofACL:EMNLP2020.BenHutchinson,VinodkumarPrabhakaran,EmilyDen-ton,KellieWebster,YuZhong,andStephenDenuyl.2020.SocialbiasesinNLPmodelsasbarriersforpersonswithdisabilities.InProceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics,pages5491–5501,Online.AssociationforComputationalLinguistics.AbigailZ.JacobsandHannaWallach.2021.Mea-surementandFairness.InProceedingsofthe2021ACMConferenceonFairness,Accountability,andTransparency,pages375–385,VirtualEventCanada.ACM.Jigsaw.2017.Kaggle’sToxicityCommentClassiﬁca-tioncompetition.XisenJin,FrancescoBarbieri,BrendanKennedy,AidaMostafazadehDavani,LeonardoNeves,andXiangRen.2021.Ontransferabilityofbiasmitigationef-fectsinlanguagemodelﬁne-tuning.InProceedingsofthe2021ConferenceoftheNorthAmericanChap-teroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages3770–3783,Online.AssociationforComputationalLinguistics.PratikJoshi,SebastinSanty,AmarBudhiraja,KalikaBali,andMonojitChoudhury.2020.ThestateandfateoflinguisticdiversityandinclusionintheNLPworld.InProceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics,pages6282–6293,Online.AssociationforComputationalLinguistics.NikhilKandpal,EricWallace,andColinRaffel.2022.DeduplicatingTrainingDataMitigatesPrivacyRisksinLanguageModels.arXiv:2202.06539[cs].ArXiv:2202.06539.MichaelKearns,SethNeel,AaronRoth,andZhi-weiStevenWu.2018.Preventingfairnessgerryman-dering:Auditingandlearningforsubgroupfairness.InProceedingsofthe35thInternationalConferenceonMachineLearning,volume80ofProceedingsofMachineLearningResearch,pages2564–2572.PMLR.ErinM.Kerrison,JenniferCobbina,andKimberlyBen-der.2018.“YourPantsWon’tSaveYou”:WhyBlackYouthChallengeRace-BasedPoliceSurveillanceandtheDemandsofBlackRespectabilityPolitics.RaceandJustice,8(1):7–26.OsKeyes,ChandlerMay,andAnnabelleCarrell.2021.Youkeepusingthatword:Waysofthinkingaboutgenderincomputingresearch.Proc.ACMHum.-Comput.Interact.,5(CSCW1).JuliaKreutzer,IsaacCaswell,LisaWang,AhsanWahab,DaanvanEsch,NasanbayarUlzii-Orshikh,Allah-seraTapo,NishantSubramani,ArtemSokolov,Clay-toneSikasote,MonangSetyawan,SupheakmungkolSarin,SokharSamb,BenoîtSagot,ClaraRivera,An-netteRios,IsabelPapadimitriou,SalomeyOsei,Pe-droOrtizSuarez,IroroOrife,KelechiOgueji,An-dreNiyongaboRubungo,ToanQ.Nguyen,Math-iasMüller,AndréMüller,ShamsuddeenHassanMuhammad,NandaMuhammad,AyandaMnyak-eni,JamshidbekMirzakhalov,TapiwanasheMatan-gira,ColinLeong,NzeLawson,SnehaKudugunta,YacineJernite,MathiasJenny,OrhanFirat,Bonaven-tureF.P.Dossou,SakhileDlamini,NisansadeSilva,SakineÇabukBallı,StellaBiderman,AlessiaBat-tisti,AhmedBaruwa,AnkurBapna,PallaviBaljekar,IsraelAbebeAzime,AyodeleAwokoya,DuyguAta-man,OrevaogheneAhia,OghenefegoAhia,SwetaAgrawal,andMofetoluwaAdeyemi.2022.QualityataGlance:AnAuditofWeb-CrawledMultilingualDatasets.TransactionsoftheAssociationforCompu-tationalLinguistics,10:50–72.KeitaKurita,NidhiVyas,AyushPareek,AlanWBlack,andYuliaTsvetkov.2019.MeasuringBiasinCon-textualizedWordRepresentations.InProceedingsoftheFirstWorkshoponGenderBiasinNaturalLan-guageProcessing,pages166–172,Florence,Italy.AssociationforComputationalLinguistics.WilliamLabov.1986.Thesocialstratiﬁcationof(r)innewyorkcitydepartmentstores.InHaroldB.AllenandMichaelD.Linn,editors,DialectandLanguageVariation,pages304–329.AcademicPress,Boston.MandyLau.2021.Artiﬁcialintelligencelanguagemod-elsandthefalsefantasyofparticipatorylanguagepolicies.WorkingpapersinAppliedLinguisticsandLinguisticsatYork,1:4–15.ConnorLeahyandStellaBiderman.2021.ThehardproblemofaligningAItohumanvalues.InTheStateofAIEthicsReport(Volume4).TheMontrealAIEthicsInstitute.JoshLepawsky.2019.Noinsidesontheoutsides.Dis-cardStudies,0(0).ShaharLevy,KorenLazar,andGabrielStanovsky.2021.Collectingalarge-scalegenderbiasdatasetforcoref-erenceresolutionandmachinetranslation.

37

PaulPuLiang,ChiyuWu,Louis-PhilippeMorency,andRuslanSalakhutdinov.2021.TowardsUnderstandingandMitigatingSocialBiasesinLanguageModels.InProceedingsofthe38thInternationalConferenceonMachineLearning,volume139ofProceedingsofMachineLearningResearch,pages6565–6576.PMLR.Q.VeraLiaoandMichaelMuller.2019.Enablingvaluesensitiveaisystemsthroughparticipatorydesignﬁc-tions.StephanieLin,JacobHilton,andOwainEvans.2021.TruthfulQA:MeasuringHowModelsMimicHu-manFalsehoods.arXiv:2109.07958[cs].ArXiv:2109.07958.NishthaMadaan,InkitPadhi,NaveenPanwar,andDip-tikalyanSaha.2021.Generateyourcounterfactuals:Towardscontrolledcounterfactualgenerationfortext.ProceedingsoftheAAAIConferenceonArtiﬁcialIntelligence,35(15):13516–13524.LiamMagee,LidaGhahremanlou,KarenSoldatic,andShanthiRobertson.2021.IntersectionalBiasinCausalLanguageModels.arXiv:2107.07691[cs].ArXiv:2107.07691.VijitMalik,SunipaDev,AkihiroNishi,NanyunPeng,andKai-WeiChang.2021.Sociallyawarebiasmea-surementsforhindilanguagerepresentations.ChandlerMay,AlexWang,ShikhaBordia,SamuelR.Bowman,andRachelRudinger.2019.Onmeasuringsocialbiasesinsentenceencoders.InProceedingsofthe2019ConferenceoftheNorthAmericanChap-teroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,Volume1(LongandShortPapers),pages622–628,Minneapolis,Min-nesota.AssociationforComputationalLinguistics.AngelinaMcMillan-Major,ZaidAlyafeai,StellaBi-derman,KimboChen,FrancescoDeToni,GérardDupont,HadyElsahar,ChrisEmezue,AlhamFikriAji,SuzanaIli´c,NurulaqillaKhamis,ColinLeong,MaraimMasoud,AitorSoroa,PedroOrtizSuarez,ZeerakTalat,DanielvanStrien,andYacineJernite.2022.DocumentingGeographicallyandContextu-allyDiverseDataSources:TheBigScienceCatalogueofLanguageDataandResources.arXiv:2201.10066[cs].ArXiv:2201.10066.MilagrosMiceli,JulianPosada,andTianlingYang.2022.StudyingUpMachineLearningData:WhyTalkAboutBiasWhenWeMeanPower?Proceed-ingsoftheACMonHuman-ComputerInteraction,6(GROUP):1–14.MargaretMitchell,SimoneWu,AndrewZaldivar,ParkerBarnes,LucyVasserman,BenHutchinson,ElenaSpitzer,InioluwaDeborahRaji,andTimnitGebru.2019.ModelCardsforModelReporting.InProceedingsoftheConferenceonFairness,Account-ability,andTransparency,FAT*’19,pages220–229,Atlanta,GA,USA.AssociationforComputingMa-chinery.MikeMonteiroandVivianneCastillo.2019.Ruinedbydesign:howdesignersdestroyedtheworld,andwhatwecandotoﬁxit.MuleDesign,Fresno.MoinNadeem,AnnaBethke,andSivaReddy.2020.Stereoset:Measuringstereotypicalbiasinpretrainedlanguagemodels.NikitaNangia,ClaraVania,RasikaBhalerao,andSamuelR.Bowman.2020.CrowS-pairs:Achal-lengedatasetformeasuringsocialbiasesinmaskedlanguagemodels.InProceedingsofthe2020Con-ferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages1953–1967,Online.As-sociationforComputationalLinguistics.WilhelminaNekoto,VukosiMarivate,TshinondiwaMatsila,TimiFasubaa,TaiwoFagbohungbe,SolomonOluwoleAkinola,ShamsuddeenMuham-mad,SalomonKabongoKabenamualu,SalomeyOsei,FreshiaSackey,RubungoAndreNiyongabo,RickyMacharm,PerezOgayo,OrevaogheneAhia,MusieMeressaBerhe,MofetoluwaAdeyemi,MasabataMokgesi-Selinga,LawrenceOkegbemi,LauraMartinus,KolawoleTajudeen,KevinDegila,KelechiOgueji,KathleenSiminyu,JuliaKreutzer,JasonWebster,JamiilToureAli,JadeAbbott,IroroOrife,IgnatiusEzeani,IdrisAbdulkadirDan-gana,HermanKamper,HadyElsahar,GoodnessDuru,GhollahKioko,MurhabaziEspoir,ElanvanBiljon,DanielWhitenack,ChristopherOnyefuluchi,ChrisChinenyeEmezue,BonaventureF.P.Dossou,BlessingSibanda,BlessingBassey,AyodeleOlabiyi,ArshathRamkilowan,AlpÖktem,AdewaleAkin-faderin,andAbdallahBashir.2020.Participatoryre-searchforlow-resourcedmachinetranslation:AcasestudyinAfricanlanguages.InFindingsoftheAsso-ciationforComputationalLinguistics:EMNLP2020,pages2144–2160,Online.AssociationforComputa-tionalLinguistics.SaﬁyaUmojaNoble.2018.Algorithmsofoppression:howsearchenginesreinforceracism.NewYorkUni-versityPress,NewYork.AliciaParrish,AngelicaChen,NikitaNangia,VishakhPadmakumar,JasonPhang,JanaThompson,PhuMonHtut,andSamuelR.Bowman.2021.Bbq:Ahand-builtbiasbenchmarkforquestionanswering.InioluwaDeborahRaji,EmilyM.Bender,Amanda-lynnePaullada,EmilyDenton,andAlexHanna.2021.AIandtheEverythingintheWholeWideWorldBenchmark.arXiv:2111.15366[cs].ArXiv:2111.15366.InioluwaDeborahRajiandJoyBuolamwini.2019.Ac-tionableAuditing:InvestigatingtheImpactofPub-liclyNamingBiasedPerformanceResultsofCom-mercialAIProducts.InProceedingsofthe2019AAAI/ACMConferenceonAI,Ethics,andSociety,pages429–435,HonoluluHIUSA.ACM.MicahRajunovandScottDuane.2019.Nonbinary:MemoirsofGenderandIdentity.ColumbiaUniver-sityPress.

38

RachelRudinger,JasonNaradowsky,BrianLeonard,andBenjaminVanDurme.2018.Genderbiasincoreferenceresolution.InProceedingsofthe2018ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,Volume2(ShortPapers),pages8–14,NewOrleans,Louisiana.AssociationforComputationalLinguistics.NithyaSambasivan,ErinArnesen,BenHutchinson,TulseeDoshi,andVinodkumarPrabhakaran.2021.Re-imaginingAlgorithmicFairnessinIndiaandBe-yond.InProceedingsofthe2021ACMConferenceonFairness,Accountability,andTransparency,pages315–328,VirtualEventCanada.ACM.VictorSanh,AlbertWebson,ColinRaffel,StephenH.Bach,LintangSutawika,ZaidAlyafeai,AntoineChafﬁn,ArnaudStiegler,TevenLeScao,ArunRaja,MananDey,M.SaifulBari,CanwenXu,UrmishThakker,ShanyaSharmaSharma,ElizaSzczechla,TaewoonKim,GunjanChhablani,NihalNayak,De-bajyotiDatta,JonathanChang,MikeTian-JianJiang,HanWang,MatteoManica,ShengShen,ZhengXinYong,HarshitPandey,RachelBawden,ThomasWang,TrishalaNeeraj,JosRozen,AbheeshtSharma,AndreaSantilli,ThibaultFevry,JasonAlanFries,RyanTeehan,StellaBiderman,LeoGao,TaliBers,ThomasWolf,andAlexanderM.Rush.2021.Mul-titaskPromptedTrainingEnablesZero-ShotTaskGeneralization.arXiv:2110.08207[cs].ArXiv:2110.08207.MaartenSap,SwabhaSwayamdipta,LauraVianna,XuhuiZhou,YejinChoi,andNoahA.Smith.2021.AnnotatorswithAttitudes:HowAnnotatorBe-liefsAndIdentitiesBiasToxicLanguageDetection.arXiv:2111.07997[cs].ArXiv:2111.07997.DanielSchiff.2020.PrinciplestoPracticesforRe-sponsibleAI:ClosingtheGap.2020EuropeanCon-ferenceonAIWorkshoponAdvancingTowardstheSDGs.ShanyaSharma,MananDey,andKoustuvSinha.2021.Evaluatinggenderbiasinnaturallanguageinference.EmilySheng,Kai-WeiChang,PremNatarajan,andNanyunPeng.2021.SocietalBiasesinLanguageGeneration:ProgressandChallenges.InProceedingsofthe59thAnnualMeetingoftheAssociationforComputationalLinguisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing(Volume1:LongPapers),pages4275–4293,Online.AssociationforComputationalLinguistics.EmilySheng,Kai-WeiChang,PremkumarNatarajan,andNanyunPeng.2019.TheWomanWorkedasaBabysitter:OnBiasesinLanguageGeneration.InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNaturalLan-guageProcessing(EMNLP-IJCNLP),pages3405–3410,HongKong,China.AssociationforComputa-tionalLinguistics.ShadenSmith,MostofaPatwary,BrandonNorick,PatrickLeGresley,SamyamRajbhandari,JaredCasper,ZhunLiu,ShrimaiPrabhumoye,GeorgeZerveas,VijayKorthikanti,EltonZhang,RewonChild,RezaYazdaniAminabadi,JulieBernauer,XiaSong,MohammadShoeybi,YuxiongHe,MichaelHouston,SaurabhTiwary,andBryanCatanzaro.2022.UsingDeepSpeedandMegatrontoTrainMegatron-TuringNLG530B,ALarge-ScaleGen-erativeLanguageModel.arXiv:2201.11990[cs].ArXiv:2201.11990.DeanSpade.2015.NormalLife:AdministrativeVio-lence,CriticalTransPolitics,andtheLimitsofLaw.DukeUniversityPress.KarolinaStanczakandIsabelleAugenstein.2021.ASurveyonGenderBiasinNaturalLanguageProcess-ing.arXiv:2112.14168[cs].ArXiv:2112.14168.GabrielStanovsky,NoahA.Smith,andLukeZettle-moyer.2019.Evaluatinggenderbiasinmachinetranslation.InProceedingsofthe57thAnnualMeet-ingoftheAssociationforComputationalLinguistics,pages1679–1684,Florence,Italy.AssociationforComputationalLinguistics.ArjunSubramonian.2021.Allennlp:Fairnessandbiasmitigation.TonySun,AndrewGaut,ShirlynTang,YuxinHuang,MaiElSherief,JieyuZhao,DibaMirza,ElizabethBelding,Kai-WeiChang,andWilliamYangWang.2019.MitigatingGenderBiasinNaturalLanguageProcessing:LiteratureReview.InProceedingsofthe57thAnnualMeetingoftheAssociationforComputa-tionalLinguistics,pages1630–1640,Florence,Italy.AssociationforComputationalLinguistics.ZeerakTalat,SmarikaLulz,JoachimBingel,andIs-abelleAugenstein.2021.DisembodiedMachineLearning:OntheIllusionofObjectivityinNLP.ArXiv:2101.11974.SamsonTan,ShaﬁqJoty,Min-YenKan,andRichardSocher.2020.It’smorphin’time!Combatinglinguis-ticdiscriminationwithinﬂectionalperturbations.InProceedingsofthe58thAnnualMeetingoftheAsso-ciationforComputationalLinguistics,pages2920–2935,Online.AssociationforComputationalLinguis-tics.RomalThoppilan,DanielDeFreitas,JamieHall,NoamShazeer,ApoorvKulshreshtha,Heng-TzeCheng,AliciaJin,TaylorBos,LeslieBaker,YuDu,YaGuangLi,HongraeLee,HuaixiuStevenZheng,AminGhafouri,MarceloMenegali,YanpingHuang,MaximKrikun,DmitryLepikhin,JamesQin,De-haoChen,YuanzhongXu,ZhifengChen,AdamRoberts,MaartenBosma,VincentZhao,YanqiZhou,Chung-ChingChang,IgorKrivokon,WillRusch,MarcPickett,PraneshSrinivasan,LaicheeMan,Kath-leenMeier-Hellstern,MeredithRingelMorris,TulseeDoshi,RenelitoDelosSantos,TojuDuke,JohnnySo-raker,BenZevenbergen,VinodkumarPrabhakaran,

39

MarkDiaz,BenHutchinson,KristenOlson,Ale-jandraMolina,ErinHoffman-John,JoshLee,LoraAroyo,RaviRajakumar,AlenaButryna,MatthewLamm,ViktoriyaKuzmina,JoeFenton,AaronCo-hen,RachelBernstein,RayKurzweil,BlaiseAguera-Arcas,ClaireCui,MarianCroak,EdChi,andQuocLe.2022.LaMDA:LanguageModelsforDia-logApplications.arXiv:2201.08239[cs].ArXiv:2201.08239.NannaThylstrupandZeerakTalat.2020.Detecting‘Dirt’and‘Toxicity’:RethinkingContentModerationasPollutionBehaviour.SSRNElectronicJournal.NenadTomasev,KevinR.McKee,JackieKay,andShakirMohamed.2021.FairnessforUnobservedCharacteristics:InsightsfromTechnologicalImpactsonQueerCommunities,page254–265.AssociationforComputingMachinery,NewYork,NY,USA.AnttiVirtanen,JennaKanerva,RamiIlo,JouniLuoma,JuhaniLuotolahti,TapioSalakoski,FilipGinter,andSampoPyysalo.2019.Multilingualisnotenough:BERTforFinnish.arXiv:1912.07076[cs].ArXiv:1912.07076.EricWallace,ShiFeng,NikhilKandpal,MattGardner,andSameerSingh.2019.Universaladversarialtrig-gersforattackingandanalyzingNLP.InProceedingsofthe2019ConferenceonEmpiricalMethodsinNat-uralLanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages2153–2162,HongKong,China.AssociationforComputationalLinguistics.TianluWang,JieyuZhao,MarkYatskar,Kai-WeiChang,andVicenteOrdonez.2019.BalancedDatasetsAreNotEnough:EstimatingandMitigatingGenderBiasinDeepImageRepresentations.In2019IEEE/CVFInternationalConferenceonComputerVi-sion(ICCV),pages5309–5318,Seoul,Korea(South).IEEE.LauraWeidinger,JohnMellor,MaribethRauh,ConorGrifﬁn,JonathanUesato,Po-SenHuang,MyraCheng,MiaGlaese,BorjaBalle,AtoosaKasirzadeh,ZacKenton,SashaBrown,WillHawkins,TomStepleton,CourtneyBiles,AbebaBirhane,JuliaHaas,LauraRimell,LisaAnneHendricks,WilliamIsaac,SeanLegassick,GeoffreyIrving,andIasonGabriel.2021a.EthicalandsocialrisksofharmfromLan-guageModels.arXiv:2112.04359[cs].ArXiv:2112.04359.LauraWeidinger,JohnMellor,MaribethRauh,ConorGrifﬁn,JonathanUesato,Po-SenHuang,MyraCheng,MiaGlaese,BorjaBalle,AtoosaKasirzadeh,etal.2021b.Ethicalandsocialrisksofharmfromlanguagemodels.arXivpreprintarXiv:2112.04359.SarahWest,MeredithWhittaker,andKateCrawford.2019.DiscriminatingSystems:Gender,Race,andPowerinAI.Technicalreport,AINowInstitute,NewYork.LangdonWinner.1980.Doartifactshavepolitics?Daedalus,pages121–136.JieyuZhao,TianluWang,MarkYatskar,VicenteOr-donez,andKai-WeiChang.2018.Genderbiasincoreferenceresolution:Evaluationanddebiasingmethods.InProceedingsofthe2018ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTech-nologies,Volume2(ShortPapers),pages15–20,NewOrleans,Louisiana.AssociationforComputationalLinguistics.PeiZhou,WeijiaShi,JieyuZhao,Kuan-HaoHuang,MuhaoChen,RyanCotterell,andKai-WeiChang.2019.ExaminingGenderBiasinLanguageswithGrammaticalGender.InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLan-guageProcessingandthe9thInternationalJointCon-ferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages5275–5283,HongKong,China.As-sociationforComputationalLinguistics.RanZmigrod,SabrinaMielke,HannaWallach,andRyanCotterell.2019.CounterfactualDataAug-mentationforMitigatingGenderStereotypesinLan-guageswithRichMorphology.InProceedingsofthe57thAnnualMeetingoftheAssociationforComputa-tionalLinguistics,pages1651–1661,Florence,Italy.AssociationforComputationalLinguistics.AAcknowledgmentsTheworkpresentedinthispaperistheoutcomeofthediscussionsandworkwithintheBigScienceinitiative.Notably,wewouldliketoacknowledgeIoanaBaldiniandXudongSheng,whocontributedsigniﬁcantlytoanearlieriterationandwhoseworkservedasafounda-tionforthespeciﬁccontributionsandargumentsofthispaper.BCreditAuthorStatementWefollowtherecommendationsandtaxonomyprovidedbyAllenetal.(2019)todetermineandoutlineauthorcontributions.StellaBiderman:Writing—OriginalDraft(Sec-tion4),Writing—Review&Editing.MirunaClinciu:Conceptualization,Writing—OriginalDraft(Section3),Writing—Review&Editing(Section3.5).MananDey:Writing—Originaldraftpreparation(Section5),Writing—ReviewandEditing.ShayneLongpre:Writing—Originaldraftprepara-tion(Section1–2),Writing—Review&Editing(Sec-tion3).AlexandraSashaLuccioni:Writing—OriginalDraft(Section4),Writing—Review&Editing.MaraimMasoud:Conceptualization,Writing—Originaldraftpreparation(Section4),Writing—Re-view&Editing(Section4).MargaretMitchell:Writing—Originaldraft&Re-view&Editing.AurélieNévéol:Supervision,Writing—Originaldraftpreparation(Abstract,Section3),Writing—Re-view&Editing.

40

DragomirRadev:Writing—Originaldraft&Re-view&Editing.ShanyaSharma:Writing—Originaldraftprepara-tion(Section5),Writing—ReviewandEditing(Sec-tions3&5).ArjunSubramonian:Writing—Originaldraftpreparation(Sections2,3,&5),Writing—Review&Editing.JaesungTae:Writing—Originaldraftpreparation(Section1),Writing—Review&Editing.ZeerakTalat:Supervision,Conceptualization,Writ-ing—Originaldraftpreparation(Abstract,Section1–2,4,6),Writing—Review&Editing.SamsonTan:Supervision,Conceptualization,Writ-ing—Originaldraftpreparation(Sections3.2&4.2),Writing—Review&Editing.DeepakTunuguntla:Conceptualization,Writing—Originaldraftpreparation(Section1-2),Writing—Re-view&Editing.OskarvanderWal:Conceptualization,Writing—OriginalDraft(Section3),Writing—Review&Editing(Section3).CDeterminationofAuthorOrderContributorsarelistedalphabetically,exceptforZeerakTalatandAureliéNevéol,whomanagedpaperwritingandchairedtheworkinggroup,respectively.Allauthorscontributedtotheconceptualizingandwritingofthedocument.

41