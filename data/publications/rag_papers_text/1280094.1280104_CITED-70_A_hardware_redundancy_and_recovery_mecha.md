Graphics Hardware (2007) Timo Aila and Mark Segal (Editors)

A Hardware Redundancy and Recovery Mechanism for Reliable Scientiﬁc Computation on Graphics Processors

Jeremy W. Sheaffer1

David P. Luebke2

Kevin Skadron1

1University of Virginia

2NVIDIA Research

Abstract

General purpose computation on graphics processors (GPGPU) has rapidly evolved since the introduction of commodity programmable graphics hardware. With the appearance of GPGPU computation-oriented APIs such as AMD’s Close to the Metal (CTM) and NVIDIA’s Compute Uniﬁed Device Architecture (CUDA), we begin to see GPU vendors putting ﬁnancial stakes into this non-graphics, one-time niche market. Major supercomputing installations are building GPGPU clusters to take advantage of massively parallel ﬂoating point capabilities, and Folding@Home has even released a GPU port of its protein folding distributed computation client. But in order for GPGPU to truly become important to the supercomputing community, vendors will have to address the heretofore unimportant reliability concerns of graphics processors. We present a hardware redundancy-based approach to reliability for general purpose computation on GPUs that requires minimal change to existing GPU architectures. Upon detecting an error, the system invokes an automatic recovery mechanism that only recomputes erroneous results. Our results show that our technique imposes less than a 1.5× performance penalty and saves energy for GPGPU but is completely transparent to general graphics and does not affect the performance of the games that drive the market.

1. Introduction

Exponential device scaling has produced incredible ad- vances in the capabilities of today’s computing infrastruc- ture. Graphics processors have taken advantage of these scaling trends to achieve dramatic increases in throughput. Semiconductor devices, however, have now become so small that they are vulnerable to transient faults caused by cosmic and terrestrial radiation; and to noise due to crosstalk, di/dt induced voltage droop, and parameter variations. As the im- portance of these phenomena all grow exponentially with decreased feature size or supply voltage [SABR04], the ‘free lunch’ of Moore’s Law for graphics architects approaches its end. Future designs must be more aware of such low-level physical challenges.

1.1. The Case for Redundant GPUs

Graphics processors provide cheap, commodity access to ﬂoating point throughput that until recently was only avail- able in supercomputers. New GPGPU APIs like CUDA and CTM ease the retargeting of traditional supercomputing ap- plications, like cosmological n-body simulations and nu-

clear testing, to GPUs. Increased programmability will lead to adoption, and in turn to the development of new super- computing infrastructure built on GPU technology at much higher performance per dollar than can be achieved with cur- rent systems.

NVIDIA’s GeForce 8800 GTX GPU produces a theoreti- cal maximum of 346 GFLOPS, almost double the through- put of the fastest supercomputer in the world only a little over a decade ago [TOP94]. With this kind of processing power, even lower budget organizations are starting to look toward GPUs as a platform for highly parallel, compute in- tensive, vertical market applications with the aim of moving batch processing online and away from more costly, cluster based solutions. One example of a new market where GPUs are starting to make inroads is that of radiology, where GPUs are employed for medical image processing. This is a do- main in which errors are often very costly, both in ﬁnancial terms and liability, and potentially also in human life. Erro- neous computation can lead to death.

ECC in memory systems for these critical applications is a necessity, but it is not sufﬁcient. An error can occur in con-

Copyright c(cid:13) 2007 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or class- room use is granted without fee provided that copies are not made or distributed for commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for components of this work owned by others than ACM must be hon- ored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior speciﬁc permission and/or a fee. Re- quest permissions from Permissions Dept, ACM Inc., fax +1 (212) 869-0481 or e-mail permissions@acm.org. GH 2007, San Diego, California, August 04 - 05, 2007 c(cid:13) 2007 ACM 978-1-59593-625-7/07/0008 $ 5.00

J. W. Sheaffer, D. P. Luebke & K. Skadron / A Hardware Redundancy and Recovery Mechanism for Reliable GPGPU

trol or logic that silently and undetectably corrupts compu- tation outside of the auspices of any memory protections. Transient errors in logic are not yet prevalent, but rates are increasing exponentially with each generation and logic er- rors are expected to become a signiﬁcant concern in practice within the next three to ﬁve years [SABR04].

The Folding@Home GPU client has now been in distribu- tion for 6 months, running on approximately 500 GPUs. Over this sample set, Folding@Home has shown a failure rate of approximately 1% [Hou07]. This number may seem high; however, note that Folding@Home users are compet- ing to complete work units, thus many overclock their GPUs, which certainly impacts reliability.

To achieve reliability, logical structures must be protected with redundancy. Possible ways to build a redundantly reli- able GPU-based system include replicating the entire com- putation (temporal redundancy) or using two GPUs (spatial redundancy) and comparing the result. Both of these involve a 2× overhead, either in time or space respectively, plus comparison time, in the expected, no-error case. Architec- tural solutions place the redundancy on-chip, but must an- swer more complicated questions about what hardware will fall within or without the sphere of replication, and how to ensure that the likelihood of a silent data corruption is mini- mized.

A soft error is distinguished from a hard error by its tran- sient nature—a soft error is random, temporary, and unpre- dictable. Soft errors are referred to by several names, includ- ing transient fault, transient error, and single event upset (SEU). While these are often used interchangeably, there are subtle differences in meaning. ‘Soft error’ and ‘SEU’ have classically referred only to radiation-induced transient faults, which are not of great concern with respect to logic. ‘Tran- sient fault’ and ‘transient error’ are more general terms that include soft errors.

In this paper we explore the concept of the sphere of replication—the set of hardware which must be copied or replicated for parallel, redundant execution in a redundantly reliable processor—as it applies to general purpose and sci- entiﬁc computation on graphics hardware. We discuss the array of possibilities for the sphere of replication and choose one solution which uses the existing parallelism of the frag- ment shader cores as redundantly parallel processors. This paper shows how a commodity GPU architecture can be modiﬁed in a few simple ways to create a dual purpose GPU that loses no performance in graphics while providing redun- dancy for reliable GPGPU. With dual issue of each fragment from the rasterizer, our solution takes advantage of high temporal locality of identical fragments, pushing up texture cache hit rates and allowing computation to complete usu- ally with less than 1.5× overhead, all while saving energy when compared with a naïve reliable system. Furthermore, we provide a simple mechanism for reissue of detected er- rors that does not impact the critical path bandwidth of the system.

Not all errors are cause for concern. If errors do not matter for architecturally correct execution (ACE)—in other words, if they do not affect the ﬁnal outcome of a computation— they are harmless. An error might be harmless, for example, if it strikes a storage location that is not currently in use (i.e., not ACE). The greatest worry of a user who desires reliabil- ity is of a silent data corruption, or an error that corrupts a result but gives no indication that anything is amiss.

The dominant metric for quantifying the chance of an er- ror as a result of a transient fault is the Architectural Vulner- ability Factor or AVF [MER05]. The AVF of a structure is a fraction from zero to one which represents the likelihood that a transient fault in that structure will lead to a computa- tional error. AVF takes into account the total amount of time that each bit can contribute to a computation, the total num- ber of bits in the structure, and the size of the structure. More formally, Architectural Vulnerability Factor is:

AVF = ∑b∈Btb |B|×∆t

1.2. Overview

A transient, single bit corruption in a microelectronic cir- cuit is termed a soft error. Soft errors have long been an important design constraint in general purpose processor de- sign, especially in engineering reliable memory systems for enterprise servers. The common wisdom is that this prob- lem is a non-issue for graphics processors, and in previ- ous work we showed that this common wisdom usually, but not always, holds for GPUs in general graphics appli- cations [SLS06]; however, soft error rates are projected to continue their current trend of increasing at a rate of about 8% per technology generation [HKM∗03]—making soft er- ror rates at 16-nm nearly 100 times that of the 180-nm generation [Bor05]—and they are already causing notice- able problems and real concerns for GPGPU developers.

where B is the set of all bits in the structure, tb is the total time that bit b is ACE, and ∆t is the total time necessary to complete the computation.

In general-purpose computer systems, any ACE bit must be assumed important. Even a single error in a low-order bit in a commercial or scientiﬁc computation can invalidate a computation. What makes graphics hardware unusual is that most state on the graphics card—despite being tech- nically ACE—can tolerate some degree of error. In graph- ics domains, errors only matter if they affect the user’s per- ceived experience. An error in a single pixel, for example, may not be noticeable even if it changes the color from white to black. Errors in other state may create more visi- ble errors, but if those errors only last a single frame, the harm is minor. These observations led to the development of

c(cid:13) Association for Computing Machinery, Inc. 2007.

(1)

J. W. Sheaffer, D. P. Luebke & K. Skadron / A Hardware Redundancy and Recovery Mechanism for Reliable GPGPU

the Visual Vulnerability Spectrum for characterizing archi- tectural vulnerability for graphics hardware under graphics workloads [SLS06].

The Visual Vulnerability Spectrum makes it clear that full protection of the graphics pipeline is neither necessary nor desirable for reliable rendering; however, this observation obviously does not apply to graphics hardware used in non- visual applications, like GPGPU, where CPU error metrics are applicable. Games are still the driving economic force behind GPU development, but as the introduction of GPGPU APIs like CTM and CUDA indicate and as is evident by the work at various National Labs and other supercomput- ing facilities to build supercomputers of commodity graphics hardware, these GPGPU applications are now of sufﬁcient commercial value that it is worth thinking about what can be done to make GPUs more reliable for scientiﬁc computa- tion. Early adopters, like Folding@Home, are demonstrating sufﬁciently high error rates that it may deter followers until reliability can be assured.

2. CPU Reliability

CPU redundancy literature primarily focuses on minimiz- ing the computation necessary for reliability with techniques like Redundant Multithreading (RMT), where a primary thread of computation fully computes a result and a sec- ondary thread recomputes as little as possible while main- taining reliability before both results are sent to a com- parator. Recovery from detected logical errors is largely untouched in the literature, with some notable excep- tions [GSVP03, VPC02]. The concept of memoizing and caching instructions and results so that repeated computation can be bypassed is an important technique in redundancy because it actually reduces the number of instructions exe- cuted [PGS04], while more typical solutions do not.

Hybrid hardware/software solutions use replicated hard- ware for redundant computation with a software comparator. This offers the ﬁnancial advantage of not requiring any spe- cialized hardware and imposing a very small overhead, but still places heavy burdens on the programmer [RCV∗05].

Fully hardware solutions focus their efforts on reduc- ing the temporal and spatial overhead of error detection. Chip multiprocessors (CMPs) bring the idea of a Chip-level Redundantly Threaded Processor or CRT, which provides some minimal hardware support for redundant multithread- ing on a CMP. A slightly more sophisticated approach is found in Simultaneously and Redundantly Threaded Pro- cessors or SRTs [RM00,MKR02], which take advantage of hardware multithreading, but not of multiple cores.

There are several reasons why memoization [PGS04], or caching multiply computed results for reuse in a reliable store, is not suitable for application to graphics processors. The most important of these is that it depends upon the ex- istence of complex decode logic to cover the latency of a cache access. This logic simply is neither present nor de- sirable in current GPU architectures. Furthermore, the tech- nique is engineered for temporally redundant computation, but as is presented in Section 3, a reliable GPU solution is far more likely to use spatial redundancy rather than tempo- ral redundancy.

3. A Redundant GPU Architecture

The design of any reliable architecture requires the analysis of several key design tradeoffs. Among these are:

The employment of temporal or spatial redundancy in the solution

The size of the sphere of replication (in a spatially redun- dant solution)

The ways in which computational redundancy is used to protect logic vary over a wide spectrum. At one end lies triplicate replication in Triple Modular Redundancy, which includes both redundancy and recovery in one package. In a triple modularly redundant solution, all components are replicated in triplicate, and a comparator accepts votes from each module. This technique relies on the fact that there is a very low probability of two or more of the computational units succumbing to an error in the same calculation (by way of a simultaneous two-bit error). Triple modular redun- dancy is not unique in this reliance—in fact, all reliability techniques rely on a low incidence of simultaneous two-bit errors—it is simply more obvious about it.

Software solutions involve software creation of redundant threads and use a software comparator. A major advantage of a software only solution is the ease of comparison and re- computation. Unfortunately, such solutions place the burden of error detection and correction on the programmer. This burden is further compounded by the fact that threads with side effects are not so straightforwardly handled.

The location of the comparison mechanism • The datapath to signal an error or reissue a computation

In this section we explore these tradeoffs in more detail

3.1. Design Decisions

Implementing a reliable functional unit is not as straightfor- ward as simply replicating computational structure. Figure 1 depicts a an example implementation of a reliable ALU. The sphere of replication for this design fully encompasses two ALUs and partially contains the operand latch and compara- tor. The operand latch will protect the inputs with ECC, al- lowing correction of single bit errors and providing a high level of reliability for the inputs should the computation re- quire a reissue. Within the sphere of replication, all datapaths and logic hardware are fully redundant.

The comparator itself is necessarily outside of the sphere of replication. To place it within would require that it be replicated, then the results of the replicated comparators

c(cid:13) Association for Computing Machinery, Inc. 2007.

J. W. Sheaffer, D. P. Luebke & K. Skadron / A Hardware Redundancy and Recovery Mechanism for Reliable GPGPU

FUNC OP1 OP2

TYPICAL (UNRELIABLE) ALU

A L U

L A T C H

A L U

NOP OUT

C M P R

M U X

L A T C H

OUT

being of little import to GPGPU applications. In addition to providing a redundancy mechanism, we work under the additional constraint that a solution should require a mini- mal set of changes to existing hardware—we seek a solution that is zero-cost in terms of the performance overhead when processing graphics workloads and nearly zero-cost with re- spect to die space sacriﬁced to implement the solution—this means using existing logic and data paths whenever possi- ble.

SPHERE OF REPLICATION

Figure 1: A redundant ALU uses two parallel ALUs to calcu- late a result and a comparator to verify that both ALUs have come up with the same answer. The comparator result con- trols both the input latch and a multiplexor which chooses either the ALU result or a no-op result to be latched in the output.

would require a third comparator which would need to be ei- ther outside of the sphere of replication or itself replicated... The comparator output controls the input latch, either allow- ing new inputs to enter the ALU or causing the reissue of a failed computation. The comparator output also controls a multiplexor to select the appropriate output of the ALU: either the result of the computation or a NOP result.

The architect of a reliable system must make efforts to protect those logic units and data paths outside of the sphere of replication. For instance, since the comparator unit and its output data paths all fall outside of the set of replicated structures, they are signiﬁcantly more vulnerable than the redundantly protected state and logic. Architectural and cir- cuit level techniques, such as hardened transistors and care- ful wire routing, can be employed to increase their reliability.

Figure 1 is in some ways a gross simpliﬁcation. For in- stance, it creates no backpressure to correctly handle new inputs. The complexity in even this “simple” example serves well to illustrate both the non-triviality of correctly imple- menting a redundant functional unit and the space overhead requirements of introducing new hardware into the pipeline (indeed signal propagation delay increases, leading to a pos- sible reduction in clock frequency, due to the new hardware being placed in the critical path, are another concern).

3.1.1. Shader Redundancy

We implement a design for a redundantly reliable graphics processor with the explicit intent that this reliability is in- tended for GPGPU domains. We have previously shown that the type and level of reliability described in this paper is not necessary for general graphics [SLS06]. Speciﬁcally, we provide a redundancy mechanism for the fragment engine, other stages of the pipeline having small AVFs and therefore

While the solution presented in this paper is speciﬁcally aimed at redundancy in the fragment engine, it is straight- forward to see how the ideas apply more generally to all programmable stages in a uniﬁed shader architecture. The extensions necessary to provide redundancy to vertex and geometry shaders follow closely—though GPGPU APIs do not make use of these units—as do the concepts that apply to general multicore architectures.

Redundancy can be implemented over a range of struc- tures, each involving different tradeoffs. Most of these have important consequences that conﬂict with our goal of modi- fying the existing architecture minimally. We choose to im- plement our redundancy at the level the fragment shader core; that is, data are replicated coming in to the fragment array so that each element is computed twice. This solution takes advantage of the already highly parallel shader cores to implement execution of redundant computation without re- quiring any new computational logic. A mechanism is nec- essary to replicate the incoming fragments, whether this is dual issue of individual fragments or replication of the ac- tual fragments in the fragment queue. We choose to replicate coming out of the rasterizer, again because this minimizes the scope of the change. If shader cores are paired in a spa- tially coherent way, it is plausible that core pairs could be made to run in lockstep and share level-1 cache, minimizing off-chip memory bandwidth requirements. Our implemen- tation makes no efforts to ensure either spatial or temporal locality; however, we show very promising cache and mem- ory behavior in our results. Depending on implementation details of the fragment array, it may be necessary to enforce a policy governing allocation of data to shader cores.

Redundancy could be implemented at a ﬁner granularity; for instance, at the level of an ALU. Implementing redun- dancy at the ALU level means replicating logical structure within a shader unit. Not only would this constitute signiﬁ- cant change within each shader core, but unless scheduling opportunities are created to effectively handle this computa- tional bandwidth for general graphics shaders, it effectively halves the shader throughput of the processor for general graphics. Furthermore, as demonstrated in Figure 1, such ﬁne-grained redundancy is expensive in terms of hardware overhead.

Quads (2×2 arrays of fragments) and Warps (NVIDIA’s term for a minimum set of threads for SIMD execution in CUDA) both represent minimum SIMD execution blocks

c(cid:13) Association for Computing Machinery, Inc. 2007.

J. W. Sheaffer, D. P. Luebke & K. Skadron / A Hardware Redundancy and Recovery Mechanism for Reliable GPGPU

in their respective domains. Implementing redundancy over quads or warps is essentially equivalent to shader core redun- dancy, the primary difference being the size of the replicated computation blocks, i.e. pairs of quads or warps work in parallel on replicated data instead of pairs of cores, compli- cating the dual issue semantics and restricting its ﬂexibility. Again, a lockstepping and cache sharing mechanism might be plausible here, but seem less likely than at the shader core, as well as highly dependent on current hardware organiza- tion.

A software solution might replicate the entire computa- tion on the GPU by issuing it twice from the CPU. This re- quires either active involvement of the programmer to ex- plicitly write redundant code with a CPU side comparison, or a compiler that generates code to perform a calculation twice and compare the results in software. Clever solutions can do this entirely on the GPU, for example, rendering to a texture on the ﬁrst pass, then comparing to the stored tex- ture values on the second. Both computational and memory bandwidth overhead are greater than 2× here, but this has the advantage that it requires no hardware modiﬁcations. The reliability of the communications network does come into question, though, as a validated result still has a long way to go before it hits memory, and additional error pro- tection hardware may be necessary to protect validated com- puted results in their registers and as they are communicated from the shader core to memory.

Yet another solution involves dual GPUs. There are sev- eral viable ways that one might use multiple GPUs to im- plement redundancy, with or without hardware support. An on-board solution places two or more GPUs on one board. A comparator veriﬁes the result of the two computations by checking the contents of the output buffers in VRAM before allowing the computation to be uploaded to the CPU. Multi- board solutions would need to take advantage of a CPU-side comparator, SLI or CrossFire based solutions, or solutions that compare DVI output. This assumes driver support for the multi-board GPU redundancy.

extant on current or future AMD hardware. The ﬁrst is the datapath associated with the F-buffer [MP01]. The second is the path necessary to implement a uniﬁed shader model ar- chitecture, which is already present in NVIDIA’s G80 GPU, AMD’s R600, and the AMD GPU in the XBox 360.

The F-buffer was originally devised as a mechanism to easily enable multi-pass shader calculations when shader complexity was crucially limited by the hardware. AMD implemented the F-buffer in the R5XX hardware fam- ily [HPS05], though this hardware was never exposed. It is not publicly known at this writing whether NVIDIA has ever implemented an F-buffer, nor whether AMD retains the structure in R600. After each pass of a multi-pass shader on an architecture with an F-buffer, data is written to the F-buffer. At the beginning of the subsequent pass, data is pulled from the F-buffer in rasterization order to be used in the continuation of the computation. The complexities of the F-buffer, including the ordering requirements, are not nec- essary for the proposed work, only the datapath. Using the F-buffer datapath, a path from VRAM to the top of the frag- ment pipeline, fragments that are found to be erroneous by the comparator can be returned to the fragment processor for reissue.

A uniﬁed shader model architecture must have a datap- ath from the back to the front of the uniﬁed shader pipeline. This is necessary to carry transformed vertices to geome- try processing input and to move processed geometry to be rasterized. This suggests, in fact, that there are two such dat- apaths, since the rasterizer is still specialized hardware. It is less likely that these datapaths provide the necessary func- tionality, but the speciﬁcs depend on the location of the com- parator.

Other clever algorithms, Delay Streams [AMN03] being a prime example, have proposed and could capitalize on a similar datapath.

3.2. A Redundant Solution

3.1.2. Comparators

The choice of the location of the comparator is highly depen- dent on the choice of redundancy as discussed above. In gen- eral, design choices should be based on the typical case, not the exception. For this reason, it is inadvisable to include, as an example, a comparator per shader unit. This would only eat into (possibly unavailable) timing slack on the critical path of the shader computation.

3.1.3. Datapath

With error detection in place, a mechanism to reissue erro- neous computation must accompany it. This requires a dat- apath to transport the inputs back to the beginning of the compute stage. There are two suitable datapaths, applicable with little to no modiﬁcation (save signal decoding), already

Figure 2 depicts a block diagram of one possible solution that ﬁts within the constraints deﬁned above. This architec- ture makes use of the rasterizer as the ﬁrst stage in the reli- able pipeline. The rasterizer issues two copies of each frag- ment into the fragment queue for processing by the fragment cores. The fragment processor processes all fragments nor- mally and passes them on to the raster operations unit (ROP). ROP will write data to the framebuffer normally if the exist- ing data at that location is in the cleared state—i.e. there is no data in the framebuffer at that location—but will com- pare all of the calculated results (color, depth, etc.) if data is already present in the framebuffer. When any difference is found between two calculated results that map to the same framebuffer location, an error is signaled.

Reissue depends on a new structure we call the domain buffer and minor augmentation of the rasterizer. The domain

c(cid:13) Association for Computing Machinery, Inc. 2007.

J. W. Sheaffer, D. P. Luebke & K. Skadron / A Hardware Redundancy and Recovery Mechanism for Reliable GPGPU

3.2.1. Error-Correcting Codes

VS G P

D B

R A S T E R

Z E R

F R A G M E N T P R O C E S S

F C

F C

F C

. . .

F C

...

F C

F C

...

F C

... ...

F C

F C

. . .

I

N G

F C

F C

F C

SPHERE OF REPLICATION

R O P

To FB

A redundant multithreading approach such as this protects logic within the sphere of replication. Outside the sphere of replication, memory and communication paths must be pro- tected with ECC; this includes non-replicated caches, video memory, and the PCI-E bus. Furthermore, no solution can guarantee reliability. The goal of any implementation of a reliable system is to reduce the likelihood of a silent data corruption to a negligible value. Precisely what this value is is highly dependent on the application.

Figure 2: Our proposed reliable GPGPU system. We add a domain buffer to store data needed to set up the rasterizer for the domain in the event of a reissue, augment the raster- izer to produce two of each fragment and to handle reissue requests from the domain buffer, and repurpose raster op- erations as our comparator. In the ﬁgure, “VS” is the ver- tex stream, “GP” is the geometry processor, “DB” is the domain buffer, “FC” is a fragment core, and “FB” is the framebuffer.

4. Experiments and Results

We describe our simulation infrastructure and present results of performance and power studies to evaluate our solution.

Texture Reads per Fragment vs. Texture Cache Hit Rate

90

)

%

80

(

e t a R

70

buffer sits between geometry processing and the rasterizer. It stores the information necessary to set up the rasterizer to produce fragments over the computational domain; for typ- ical GPGPU applications, this is only two triangles, so the domain buffer can be very small When ROP sends a reissue signal, containing the fragment hx,yi for the offending data- point, the domain buffer reissues the geometry and the error location(s) to the rasterizer. The rasterizer applies a stencil to reissue only those fragments which need to be recomputed. In the case of NVIDIA CUDA, a direct compute environ- ment which does not use the rasterizer for thread creation, the Thread Execution Manager [NVI07] must be modiﬁed to reissue an appropriate subset of the compute domain. We leave issues related to CUDA and direct compute to future work.

Recall Equation 1: AVF = ∑b∈Btb

|B|×∆t . In a typical GPGPU computation, the early stages of the pipeline (primarily ver- tex and geometry processing) are very short (four vertices and two triangles, respectively). As a result, each respective tb is very small, leading to a small AVF for the structures, thus they can be ignored without unduly impacting the reli- ability of the solution. In the case of the rasterizer, however, much of the state is potentially ACE for a signiﬁcant por- tion of the application. For this reason, rasterizer state should probably be protected with ECC. We cannot make accurate statements about the importance of protecting the rasterizer without performing an ACE analysis on a simulator with much more faithful rasterizer implementation than we cur- rently have. Nonetheless, we believe that ECC on raster state may be desired. If the rasterizer has a high AVF, it might be necessary to replicate it, move it wholly inside the sphere of replication, and move the double issue to the geometry engine. All off-chip memory and busses must be fully pro- tected with ECC.

t i

H e h c a C e r u t x e T e g a r e v A

60

50

40

30

20

10

Unreliable Base System Redundantly Reliable System

0

0

2

4

8 Texture Reads per Fragment

6

10

12

14

Figure 3: On the x axis are memory operations per element, with texture cache hit rate on y. Texture cache hit rate is almost universally better in our reliable architecture than in the baseline architecture, the only exception occurring in the 16×16 simulations, where the cache did not have suf- ﬁcient time to warm up. Cache misses are only counted if they lead to a new memory transaction—e.g. misses on an address that will be serviced by a pending memory fetch are not counted—thus the difference in hit rates translates di- rectly to memory bandwidth. These data are averaged over all 16 cores. This graph shows the results over a 128×128 domain, though all domains produced qualitatively similar results.

4.1. Simulation

We implemented the solution depicted in Figure 2 in the GPU simulation infrastructure developed by Bill Mark’s group at the University of Texas, Austin [JLBM05]. This system is built in SystemC [Sys05], using an event-based model to drive a high-ﬁdelity fragment engine simulation. The fragment engine implements a useful subset of the OpenGL NV_fragment_program extension [BK05], in- cluding support for branching. Raster operations and the tex- turing and memory systems are also implemented in some

16

c(cid:13) Association for Computing Machinery, Inc. 2007.

J. W. Sheaffer, D. P. Luebke & K. Skadron / A Hardware Redundancy and Recovery Mechanism for Reliable GPGPU

detail, while the rest of the pipeline is purely functional. All fragment program instructions require one cycle to com- plete, save texturing operations which go through a cache to memory.

the fragment processors. An actual implementation would require the functionality to turn this feature on for reliable GPGPU operation and off for unreliable GPGPU or general graphics.

The system is engineered with 16 fragment pipelines, each SIMD Float4 with operand swizzling and masking. Each pipeline has 16 thread contexts, which are serviced in round-robin order, with context switches invoked on thread block. Context switches are free as long as there is a runnable thread on the core, else the core must block until a runnable thread is available (a texture reference is serviced or a new thread is instantiated on the core). Each fragment core is con- nected to memory through a crossbar and cache. Crossbar transactions are processed in two cycles—one cycle in and one cycle out—as long as there is bandwidth available. If there is no available bandwidth, the crossbar queues pend- ing transactions. The queue size is set at 512 elements. The texture caches are 16 kB, 8-way set associative with 64 byte blocks and 256 element queues. Textures are stored in a 128 MB address space, swizzled via a Morton space-ﬁlling curve for increased locality [Mor66]. Another crossbar provides a path to raster ops. ROP accesses the framebuffer through an identical interface to that of the fragment processors

We implemented our reliable GPU on top of this infras-

tructure by making the following changes:

Texture Reads per Fragment vs. Core Utilization

90

80

Unreliable Base System Redundantly Reliable System

Domain buffer: We implement our domain buffer as a lookup-table containing already rasterized fragments. As the rasterizer is not modeled faithfully, this solution creates less room for error while not changing performance characteris- tics. The domain buffer gets precedence over new geometry going into the rasterizer.

Reissue path: The simulator does not contain any of the datapaths discussed above for repurposing, so we imple- mented this with a SystemC channel with a one cycle trans- fer latency between ROP and the rasterizer (logically the do- main buffer).

Comparators: The comparators are implemented in ROP. We use full/empty bits on the framebuffer. If the location is empty, we use the standard semantics and set the bit. When the location is full, we issue reads on all channels (in this simulator, that is 3 channels of color, plus depth) and com- pare all components. These comparisons can be bitwise—it is not necessary to do ﬂoating point compares—as any re- sult that is not exactly, bitwise the same as the stored result ﬂags an error. Independent of the result of the comparisons, the data is discarded; however, if it fails, the pixel’s frame- buffer coordinates are sent over the reissue datapath to be recalculated and the full/empty bit is reset to empty. We do not impose delay to actually do the comparisons but do go through cache to get to stored color data for the comparison, thus accounting for that latency and bandwidth.

)

%

(

n o i t a z

70

60

i l i t

U e r o C e g a r e v A

50

40

30

20

10

0

2

4

8 Texture Reads per Fragment

6

10

12

14

Figure 4: On the x axis are memory operations per ele- ment, with shader core utilization on y. Due to the increased memory locality, core utilization is also better in the reli- able architecture. These statistics are averaged over all 16 cores.The graph shows the results over a 128×128 domain, though all domains produced qualitatively similar results.

16

This solution as we have presented it so far assumes there is no legitimate reason for any framebuffer location to be written twice. This is largely a valid assumption, though there are notable exceptions, such as the GPU database work by Govindaraju et al [GLW∗04]. One way to get around this problem is to modify the full/empty bit semantics such that each write toggles the bit and with it the semantics of the ROP operation. A third attempt to write a location will ﬁnd it empty and perform normal ROP operations. At the end of the computation, all full/empty bits must be checked. Any bit in the full state indicates an error and that domain element must be reissued. This solution depends on geometry order- ing requirements that are enforced by graphics APIs for cor- rect color blending and on precedence of the domain buffer over geometry processing. These requirements are probably propagated into GPGPU APIs as an artifact of the underly- ing architecture.

Double issue from the rasterizer: The rasterizer imple- mentation is functional only. Downstream from the raster- izer is a queue to the fragment engine. The rasterizer is in- voked every cycle and runs until this queue is full such that it is never a computational bottleneck. In order to implement double issue we only had to replicate each fragment created by the rasterizer and place them into the input queue for

This solution also fails to account for scatter operations which write to texture. We can potentially handle these by re- quiring that all texture writes in the reliable mode go through ROP, though that presents additional issues with respect to ROP bandwidth. This is clearly a problem for direct com- pute APIs. The details and evaluation are left to future work.

c(cid:13) Association for Computing Machinery, Inc. 2007.

J. W. Sheaffer, D. P. Luebke & K. Skadron / A Hardware Redundancy and Recovery Mechanism for Reliable GPGPU

Memory(cid:3) Domain(cid:1) Real Cache Perfect Cache Real Cache Perfect Cache Real Cache Perfect Cache Real Cache Perfect Cache Real Cache Perfect Cache Real Cache Perfect Cache

0 reads

1 read

2 reads

4 reads

8 reads

162 1.70 1.70 1.72 1.72 1.56 1.64 1.75 1.73 1.49 1.78 1.50 1.86

322 1.38 1.38 1.33 1.41 1.32 1.47 1.41 1.58 1.32 1.65 1.22 1.74

642 1.38 1.38 1.24 1.44 1.31 1.50 1.30 1.57 1.13 1.69 1.20 1.76

1282 1.41 1.41 1.22 1.43 1.22 1.50 1.22 1.62 1.21 1.71 1.36 1.85

2562 1.30 1.30 1.25 1.49 1.25 1.62 1.28 1.81 1.20 1.88 1.57 2.02

16 reads

Table 1: Summary of performance results. The data points in this table are the normalized ratios of simulation cycle counts in our reliable architecture to the baseline architec- ture, or in other words, the overhead imposed by our reli- ability implementation. The top number in each cell is the normalized simulation time with a realistic cache simulation (as described in 4.1). The bottom is the same value when the caches are perfect and zero-latency (all memory references take exactly one cycle). Simulation time made it impossible to collect data for the cells marked “N/A”.

4.2. Experimental Results

We present results of a series of performance and power studies comparing our reliable architecture with the baseline unreliable system.

5122 1.21 1.21 1.21 1.31 1.30 1.65 1.31 2.13 N/A 2.17 N/A 2.22

no errors, so we do not inject faults. More detail on the pro- grams are listed in Table 2. Results of the 8 and 16 memory reference simulations on the 512×512 domain could not be obtained due to simulation time. Table 1 summarizes the re- sults of this suite of experiments.

From the table we can see that in every case, save 16×16 with 4 memory references, the relative performance of the reliable architecture to the baseline with a real cache is at least as good as, and often much better than, that of the cor- responding perfect cache simulation. Further evidence for this appears in the plot in Figure 3. This suggests that our solution beneﬁts from improved memory locality with dual issue. We do not understand why four instances of perfect cache simulations show overheads greater than two; how- ever, in these simulations, texture operations are equivalent to ALU operations (in terms of timing), so we note that these programs effectively performed an unrealistic num- ber of consecutive ALU operations without a memory ref- erence [HP03] (15, 20, or 34, as per Table 2). Data show that the cores were poorly utilized in these instances. Fig- ure 4 shows average core utilization for both architectures (Utilization is measured by counting the number of cy- cles that some shader has the core, even if this is a “re- dundant shader”, and dividing by the total number of cy- cles, thus naïvely we would expect the redundant and non- redundant utilization to be about equal). The reliable archi- tecture makes better use of computational resources in al- most all cases, except when there is no texture activity.

4.2.1. Performance

Memory Reads Total Instructions

0 7

1 8

2 10

4 15

8 20

16 34

Table 2: Texture instructions and total instructions for each stressmark.

We implemented our redundant system as described above and veriﬁed the detection and correction implemen- tations. We then simulated both our system and the unmodi- ﬁed baseline system over a series of workloads constituting the cross of 16×16, 32×32, 64×64, 128×128, 256×256, and 512×512 computational domains and 0, 1, 2, 4, 8, and 16 memory reads per domain element simulating both real and perfect caches. Our programs are designed to stress the memory system, as memory behavior is an important and interesting concern of all GPUs, but especially of one that is designed to be reliable through redundancy. These stress- marks issue two memory instructions then immediately use them with a previous result via a MAD instruction, thus mak- ing as many memory reads as is reasonably possible such that all of the fetched results are actually used. They are worst case benchmarks with respect to memory. Each stress- mark completes with a sequence of 7 arithmetic instructions; the same seven that are used in the 0-memory-reference stressmark. Our simulations are designed to evaluate the per- formance of our solution in the common case where there are

4.2.2. Power

Memory(cid:3) Domain(cid:1)

0 reads

1 read

2 reads

4 reads

8 reads

16 reads

Power Energy Power Energy Power Energy Power Energy Power Energy Power Energy

162 1.15 1.97 1.14 1.97 1.24 1.94 1.12 1.96 1.27 1.90 1.27 1.91

322 1.40 1.94 1.44 1.93 1.45 1.92 1.37 1.93 1.43 1.90 1.52 1.87

642 1.40 1.94 1.54 1.92 1.46 1.92 1.47 1.91 1.63 1.85 1.54 1.86

1282 1.38 1.95 1.56 1.93 1.56 1.91 1.56 1.91 1.54 1.87 1.37 1.86

2562 1.50 1.95 1.54 1.94 1.53 1.93 1.50 1.92 1.56 1.88 1.21 1.90

Table 3: Relative power and energy of our redundantly re- liable GPU to the baseline, unreliable GPU. The reliable solution draws more power than the baseline system, which is to be expected, considering its increased core utilization. Note that in all cases our reliable solution uses less energy, however, typically by about 10%, than would a software or dual-gpu solution, which would require at least 2× energy.

We instrumented the simulator’s fragment engine with a power model based on PowerTimer [BBS∗03]. PowerTimer is an IBM power model, based on the Power 4 architecture. We scaled the PowerTimer model to be more in line with a modern GPU, built on a 90 nm process, running at 600

5122 1.61 1.95 1.59 1.94 1.49 1.94 1.47 1.93 N/A N/A N/A N/A

c(cid:13) Association for Computing Machinery, Inc. 2007.

J. W. Sheaffer, D. P. Luebke & K. Skadron / A Hardware Redundancy and Recovery Mechanism for Reliable GPGPU

MHz at 1.45V and assumed 25% leakage current. We used HP Labs CACTI [TTJ06] cache modeling tool with the cache parameters described in Section 4.1 to create a cache power model. CACTI allows computer architects to quickly and easily analyze cache design tradeoffs with respect to area, cycle and access times, and static and dynamic power. The results of our power analysis appear in Table 3. Our solution requires more power than the baseline, do to its improved core utilization, but we are able to save about 10% more en- ergy (power × time) than a naïve solution could.

and furthermore because the ideas presented in this paper apply directly to uniﬁed shader architectures—our simula- tion and testing was based on a more traditional graphics pipeline—we believe that it is near optimal and should be implemented in future GPUs. However, these techniques are not so directly applicable to direct compute architectures like G80 with CUDA, in which data go through neither the ras- terizer nor ROP. This observation exposes an important place for future work.

5. Discussion

The issues discussed above and our proposed solution com- bine to suggest an important concept: In a reliable multicore architecture, the sphere of replication should be as large as possible.

Our implementation demonstrates that a reliable GPU built as described in this paper beneﬁts greatly from increased memory locality inherent in the double issue method, allowing it to perform much better than the naïve expected overhead of 2×. In fact, our simulations show a measured overhead of less than 1.5× on most of our prob- lem domains.

Traditional out-of-order processors are designed to max- imize single thread performance, with much of the proces- sor real-estate dedicated not to computation, but to support structure, like branch predictors, that help the processor to achieve this goal. Early work on reliability on these archi- tectures recognizes that introducing hardware for redundant computation is expensive. By comparison, comparator hard- ware is cheap. For these reasons, it is desirable to keep the sphere of replication small.

We also make an important observation about the sphere of replication with respect to multicore architectures. When designing a reliable processor based on a traditional, single core architecture, engineers aim to make the sphere of repli- cation as small as possible, in order to minimize the extra costs implicit in replicating hardware. However in a multi- core environment, it is desirable to make the sphere of repli- cation as large as possible, because the redundant hardware is already present, so in essence its cost is free, while the cost of comparing results is expensive.

Conversely, in multicore architectures, where the compu- tational cores are small, simple and already available, the comparators become expensive (recall the costs in Figure 1). Similarly, as the tiling of multicore architectures increases, the applications written for them will likely come to resem- ble fragment shaders more than they will Microsoft Word, where the parallelism is primarily event-based as opposed to the data parallelism in scientiﬁc, image processing and other “traditional” GPGPU application domains. With fragment- shader-like programs, recomputing one result is akin to re- computing from a checkpoint in currently proposed solu- tions but with much lower overhead. Given this, and the fact that the larger the sphere of replication, the smaller the area in which transient errors are undetectable, it follows that the sphere of replication should encompass as much computa- tional hardware as possible.

We proposed a solution to the problem of reliable write to texture and by association to the more general memory op- erations in CUDA in Section 4.1. Completing the details of this solution and evaluating it poses a difﬁcult and important problem that must be solved. Detailed ACE analysis of raster state can help to make a more informed decision with respect to the rasterizer’s inclusion in the sphere of replication.

7. Acknowledgments

A portion of this work is built on top the GPU simula- tion infrastructure developed by Greg Johnson, Chris Burns, Alexander Joly, and William R. Mark at the University of Texas, Austin. Their simulator is still under development, and no papers speciﬁcally about the simulator have been published yet, but a limited description appears in their 2005 Transactions of Graphics paper [JLBM05].

6. Conclusions and Future Work

We have presented a set of modiﬁcations to existing graphics architectures to allow reliable performance in general pur- pose computation domains. We have made efforts to lever- age and repurpose existing hardware features as much as possible, with the addition of a domain buffer as the only new hardware, and minor repurposing of the rasterizer and ROP units to achieve our goals. Because this solution does not require any modiﬁcation of the fragment array, does not impact throughput for general graphics workloads, and re- quires negligible new logic to implement the functionality,

This work was supported in part by NSF grants CCF- 0429765, CCR-0306404, the Army Research Ofﬁce under grant no. W911NF-04-1-0288, a research grant from Intel MRL, and an ATI graduate fellowship. We would like to ex- tend out sincere thanks to the anonymous reviewers for their detailed and helpful comments.

One reviewer made the insightful suggestion that the ras- terizer stencil hardware could be employed for masking dur- ing reissue. Our previous solution was far more complicated in this respect.

c(cid:13) Association for Computing Machinery, Inc. 2007.

J. W. Sheaffer, D. P. Luebke & K. Skadron / A Hardware Redundancy and Recovery Mechanism for Reliable GPGPU

References

[AMN03] AILA T., MIETTINEN V., NORDLUND P.: De- lay Streams for Graphics Hardware. ACM Transactions on Graphics 22, 3 (2003), 792–800.

[BBS∗03] BROOKS D., BOSE P., SRINIVASAN V., GSCHWIND M., EMMA P. G., ROSENFIELD M. G.: New methodology for early-stage, microarchitecture- level power-performance analysis of microprocessors. IBM Journal of R & D 47, 5/6 (2003).

S. K.: Detailed Design and Evaluation of Redundant Multithreading Alternatives. In ISCA (2002), IEEE, IEEE Computer Society, pp. 99–110.

[Mor66] MORTON G. M.: A computer oriented geodetic data base and a new technique in ﬁle sequencing, 1966. IBM Canada.

[MP01] MARK W. R., PROUDFOOT K.: The F-Buffer: A Rasterization-Order FIFO Buffer for Multi-Pass Ren- dering. In Proceedings of the SIGGRAPH/Eurographics Graphics Hardware Workshop 2001 (2001).

[BK05] BROWN P., KILGARD M. J.: NV_Fragment_- http://www.opengl.org/registry/-

Program, May 2005. specs/NV/fragment_program.txt.

[Bor05] BORKAR S.: Designing Reliable Systems from Unreliable Components: The Challenges of Transistor Variability and Degradation. IEEE Micro 25, 6 (Nov./Dec. 2005), 10–16.

[GLW∗04] GOVINDARAJU N. K., LLOYD B., WANG W., LIN M., MANOCHA D.: Fast computation of database op- erations using graphics processors. In SIGMOD ’04: Pro- ceedings of the 2004 ACM SIGMOD international con- ference on Management of data (New York, NY, USA, 2004), ACM Press, pp. 215–226.

[NVI07] NVIDIA:

NVIDIA CUDA compute uni- ﬁed device architecture programming guide, 2007. http://developer.download.nvidia.com/compute/cuda/08/- NVIDIA_CUDA_Programming_Guide_0.8.pdf.

[PGS04] PARASHAR A., GURUMURTHI S., SIVASUBRA- MANIAM A.: A Complexity-Effective Approach to ALU Bandwidth Enhancement for Instruction-Level Temporal Redundancy. In ISCA (2004), IEEE, IEEE Computer So- ciety, pp. 376–386.

[RCV∗05] REIS G. A., CHANG J., VACHHARAJANI N., RANGAN R., AUGUST D. I., MUKHERJEE S. S.: De- sign and Evaluation of Hybrid Fault-Detection Systems. In ISCA (2005), pp. 148–159.

[GSVP03] GOMAA M. A., SCARBROUGH C., VIJAYKU- MAR T. N., POMERANZ I.: Transient-Fault Recovery for Chip Multiprocessors. IEEE Micro 23, 6 (2003), 76–83. [HKM∗03] HAZUCHA P., KARNIK T., MAIZ J., WAL- STRA S., BLOECHEL B., TSCHANZ J., DERMER G., HARELAND S., ARMSTRONG P., BORKAR S.: Neu- tron Soft Error Rate Measurements in a 90-nm CMOS Process and Scaling Trends in SRAM from 0.25-µm to In IEEE International Electron De- 90-nm Generation. vices Meeting 2003 Technical Digest (Dec. 2003), IEEE, pp. 523–526.

[Hou07] HOUSTON M.: Personal communication, Mar.

[RM00] REINHARDT S. K., MUKHERJEE S. S.: Tran- sient fault detection via simultaneous multithreading. In ISCA (2000), pp. 25–36.

[SABR04] SRINIVASAN J., ADVE S. V., BOSE P., RIVERS J. A.: The Impact of Technology Scaling on Lifetime Reliability. In DSN (2004), IEEE, IEEE Com- puter Society.

[SLS06] SHEAFFER J. W., LUEBKE D. P., SKADRON K.: The Visual Vulnerability Spectrum: Characterizing Archi- tectural Vulnerability for Graphics Hardware. In Proceed- ings of Graphics Hardware 2006 (Sept. 2006).

2007. Stanford University and Folding@Home.

[HP03] HENNESSY J. L., PATTERSON D. A.: Com- puter architecture: a quantitative approach, 3rd ed. Mor- gan Kaufmann Publishers Inc., San Francisco, CA, USA, 2003.

[HPS05] HOUSTON M., PREETHAM A.

J., SEGAL M. A.: A Hardware F-Buffer Implementation. Tech. rep., Stanford University., 2005.

[JLBM05]

JOHNSON G. S., LEE J., BURNS C. A., MARK W. R.: The Irregular Z-buffer: Hardware Accel- eration for Irregular Data Structures. ACM Trans. Graph. 24, 4 (2005), 1462–1482.

[MER05] MUKHERJEE S. S., EMER J. S., REINHARDT S. K.: The Soft Error Problem: An Architectural Per- spective. In HPCA (2005), IEEE, IEEE Computer Soci- ety, pp. 243–247.

[Sys05] SYSTEMC LANGUAGE REFERENCE MANUAL Draft standard SystemC lan- (version 2.1), April 2005.

WORKING GROUP: guage reference manual http://www.systemc.org/.

[TOP94] TOP500.ORG: June 1994|TOP500 Supercom- http://www.top500.org/lists/-

puting Sites, June 1994. 1994/06.

[TTJ06] TARJAN D., THOZIYOOR S., JOUPPI N. P.: CACTI 4.0. Tech. Rep. HPL-2006-86, HP Laboratories Palo Alto, June 2006.

[VPC02] VIJAYKUMAR T. N., POMERANZ I., CHENG K.: Transient-fault recovery using simultaneous multi- In ISCA ’02: Proceedings of the 29th an- threading. nual international symposium on Computer architecture (Washington, DC, USA, 2002), IEEE Computer Society, pp. 87–98.

[MKR02] MUKHERJEE S. S., KONTZ M., REINHARDT

c(cid:13) Association for Computing Machinery, Inc. 2007.