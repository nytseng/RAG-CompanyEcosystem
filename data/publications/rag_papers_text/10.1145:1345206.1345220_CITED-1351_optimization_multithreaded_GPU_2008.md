Optimization Principles and Application Performance Evaluation of a Multithreaded GPU Using CUDA

Shane Ryooy

Christopher I. Rodriguesy

Sara S. Baghsorkhiy

Sam S. Stoney

David B. Kirk(cid:3) Wen-mei W. Hwuy yCenter for Reliable and High-Performance Computing, University of Illinois at Urbana-Champaign (cid:3)NVIDIA Corporation fsryoo, cirodrig, bsadeghi, ssstone2, hwug @crhc.uiuc.edu, dk@nvidia.com

Abstract GPUs have recently attracted the attention of many application developers as commodity data-parallel coprocessors. The newest generations of GPU architecture provide easier programmability and increased generality while maintaining the tremendous mem- ory bandwidth and computational power of traditional GPUs. This opportunity should redirect efforts in GPGPU research from ad hoc porting of applications to establishing principles and strategies that allow ef(cid:2)cient mapping of computation to graphics hardware. In this work we discuss the GeForce 8800 GTX processor’s organiza- tion, features, and generalized optimization strategies. Key to per- formance on this platform is using massive multithreading to uti- lize the large number of cores and hide global memory latency. To achieve this, developers face the challenge of striking the right balance between each thread’s resource usage and the number of si- multaneously active threads. The resources to manage include the number of registers and the amount of on-chip memory used per thread, number of threads per multiprocessor, and global memory bandwidth. We also obtain increased performance by reordering accesses to off-chip memory to combine requests to the same or contiguous memory locations and apply classical optimizations to reduce the number of executed operations. We apply these strate- gies across a variety of applications and domains and achieve be- tween a 10.5X to 457X speedup in kernel codes and between 1.16X to 431X total application speedup.

Categories and Subject Descriptors D.1.3 [Software]: Program- ming Techniques(cid:151)Concurrent Programming

hardware interfaces, programming them does not require special- ized programming languages or execution through graphics appli- cation programming interfaces (APIs), as with previous GPU gen- erations. This makes an inexpensive, highly parallel system avail- able to a broader community of application developers.

The NVIDIA CUDA programming model [3] was created for developing applications for this platform. In this model, the system consists of a host that is a traditional CPU and one or more com- pute devices that are massively data-parallel coprocessors. Each CUDA device processor supports the Single-Program Multiple- Data (SPMD) model [8], widely available in parallel processing systems, where all concurrent threads are based on the same code, although they may not follow exactly the same path of execution. All threads share the same global address space.

CUDA programming is done with standard ANSI C extended with keywords that designate data-parallel functions, called ker- nels, and their associated data structures to the compute devices. These kernels describe the work of a single thread and typically are invoked on thousands of threads. These threads can, within developer-de(cid:2)ned bundles termed thread blocks, share their data and synchronize their actions through built-in primitives. The CUDA runtime also provides library functions for device memory management and data transfers between the host and the compute devices. One can view CUDA as a programming environment that enables software developers to isolate program components that are rich in data parallelism for execution on a coprocessor specialized for exploiting massive data parallelism. An overview of the CUDA programming model can be found in [5].

General Terms Design, Performance, Languages

Keywords

parallel computing, GPU computing

1. As a result of continued demand for programmability, modern graphics processing units (GPUs) such as the NVIDIA GeForce 8 Series are designed as programmable processors employing a large number of processor cores [20]. With the addition of new

Introduction

The (cid:2)rst version of CUDA programming tools and runtime for the NVIDIA GeForce 8 Series GPUs has been available through beta testing since February 2007. To CUDA, the GeForce 8800 GTX1 consists of 16 streaming multiprocessors (SMs), each with eight processing units, 8096 registers, and 16KB of on-chip mem- ory. It has a peak attainable multiply-add performance of 345.6 single-precision GFLOPS2,features 86.4 GB/s memory bandwidth, contains 768MB of main memory, and incurs little cost in creating thousands of threads. The architecture allows ef(cid:2)cient data sharing and synchronization among threads in the same thread block [18]. A unique aspect of this architecture relative to other parallel platforms is the (cid:3)exibility in the assignment of local resources, such as registers or local memory, to threads. Each SM can run a variable number of threads, and the local resources are divided among threads as speci(cid:2)ed by the programmer. This (cid:3)exibility

c(cid:13)ACM, 2008. This is the author’s version of the work. It is posted by permission of ACM for your personal use. Not for redistribution. The de(cid:2)nitive version was published in PPoPP ’08, http://doi.acm.org/10.1145/1345206.1345220 PPoPP’08, February 20(cid:150)23, 2008, Salt Lake City, Utah, USA Copyright c(cid:13) 2008 ACM 978-1-59593-960-9/08/0002...$5.00

1There are several versions of the GeForce 8800 GPU. References of GeForce 8800 are implied to be the GTX model. 2Particular mixes of instructions can achieve higher throughput, as will be explained in Section 3.

allows more tuning of application performance but changes the assumptions developers can make when performing optimizations. We discuss these issues in further detail.

Another question we address is how well applications can ex- ecute on the GeForce 8800 and what are the design features that contribute to or limit performance. As a collaborative effort be- tween industry and academia, a set of complete numerical appli- cations was ported and evaluated on the CUDA platform. Several application research groups in the areas of medical imaging, molec- ular dynamics, computational chemistry, electromagnetic analysis, and scienti(cid:2)c visualization contributed to this effort. The following are the major principles when choosing code to be executed on this platform:

1. Leverage zero-overhead thread scheduling to hide memory la- tency. On the GeForce 8800 there are 128 execution units avail- able for use, requiring hundreds of threads to completely oc- cupy them. In addition, threads can be starved of data due to the long latency to global memory. The general philosophy of CUDA for tolerating this latency is to generate and maintain thousands of threads in (cid:3)ight. This is in contrast with the use of large caches to hide memory latencies in CPU designs. De- velopers used to traditional multicore systems may need to de- (cid:2)ne threads at a (cid:2)ner granularity in order to generate enough threads. In addition, a high compute-to-memory-access ratio is necessary to avoid saturation of memory channels.

2. Optimize use of on-chip memory to reduce bandwidth usage and redundant execution. Working memory within a group of cores consists primarily of a register (cid:2)le and a software-managed on- chip memory called shared memory. These are high fan-out, low latency, limited-capacity memories which are partitioned among thread blocks that are assigned to the same SM at run- time. The data in shared memory can be shared among threads in a thread block, enabling interthread data reuse. An incre- mental increase in the usage of registers or shared memory per thread can result in a substantial decrease in the number of threads that can be simultaneously executed.

3. Group threads to avoid SIMD penalties and memory port/bank conﬂicts. CUDA is based on the SPMD model, but its cur- rent implementation on the GeForce 8800 imposes Single- Instruction, Multiple-Data (SIMD) mode among subsets of threads. The latter differs from the short-vector SIMD present in most contemporary processors. This is a cost-effective hard- ware model for exploiting data parallelism and allows the GeForce 8800 to share one instruction issue unit among eight execution units. However, it can be ineffective for algorithms that require diverging control (cid:3)ow decisions in data-parallel sections. In some algorithms, threads can be reorganized to avoid divergent control (cid:3)ow. Appropriate thread grouping can also preserve performance by avoiding port and bank con(cid:3)icts in memories.

4. Threads within a thread block can communicate via synchro- nization, but there is no built-in global communication mecha- nism for all threads. This avoids the need for virtualization of hardware resources, enables the execution of the same CUDA program across processor family members with a varying num- ber of cores, and makes the hardware scalable. However, it also limits the kinds of parallelism that can be utilized within a sin- gle kernel call.

We (cid:2)rst discuss related work in Section 2. Section 3 introduces the threading model and execution hardware. Section 4 demon- strates the optimization process with in-depth performance anal- ysis, using matrix multiplication kernels. Section 5 presents several studied applications with performance and optimization informa-

tion. We conclude with some (cid:2)nal statements and suggestions for future work.

2. Related Work Data parallel programming languages are considered an intermedi- ate approach between automatic parallelization efforts [7, 28] and explicit parallel programming models such as OpenMP [19] to sup- port parallel computing. Fortran 90 [6] was the (cid:2)rst widely used language and in(cid:3)uenced following data parallel languages by intro- ducing array assignment statements. Similar to array assignments in Fortran 90 is the lock step execution of each single instruction in threads executing simultaneously on a streaming multiprocessor in CUDA programming model. Later, High Performance Fortran (HPF) [15] was introduced as an standard data parallel language to support programs with SPMD. However, complexity of data dis- tribution and communication optimization techniques, as discussed in the (cid:2)nal two chapters of [13], were a hard-to-solve challenge. As a result application developers became more involved in explic- itly handling data distribution and communication; message pass- ing libraries such as [23] became a popular programming model for scalable parallel systems. Similarly in CUDA, the developer explic- itly manages data layout in DRAM memory spaces, data caching, thread communication within thread blocks and other resources.

The interest in GPGPU programming has been driven by rel- atively recent improvements in the programmability of graphics hardware. The release of Cg [16] signi(cid:2)ed the recognition that GPUs were programmable processors and that a higher-level lan- guage was needed to develop applications on them. Others felt that the abstractions provided by Cg and other shading languages were insuf(cid:2)cient and built higher-level language constructs. Brook [9] enables the usage of the GPU as a streaming coprocessor. Acceler- ator [26] is another system that uses data-parallel arrays to perform general-purpose computation on the GPU. A Microsoft C# library provides data types and functions to operate on data-parallel ar- rays. Data-parallel array computation is transparently compiled to shader programs by the runtime. Other efforts to provide a more productive stream processing programming environment for devel- oping multi-threaded applications include the RapidMind Stream- ing Execution Manager [17] and PeakStream Virtual Machine [4]. These mainly target HPC applications that are amenable to stream processing. The achieved performance may be behind customized GPU/CPU code due to the virtual machine and dynamic compila- tion overhead. We refer the reader to a review of the main body of work done to map general purpose computation to GPUs by Owens et al. in [21].

In general, previous GPU programming systems limit the size and complexity of GPU code due to their underlying graphics API- based implementations. CUDA supports kernels with much larger code sizes with a new hardware interface and instruction caching. Previous GPU generations and their APIs also restricted the al- lowed memory access patterns, usually allowing only sequential writes to a linear array. This is due primarily to limits in graph- ics APIs and corresponding limits in the specialized pixel and ver- tex processors. Accelerator does not allow access to an individual element in parallel arrays: operations are performed on all array elements. Brook also executes its kernel for every element in the stream, with some exceptions. The GeForce 8800 allows for gen- eral addressing of memory via a uni(cid:2)ed processor model, which enables CUDA to perform unrestricted scatter-gather operations.

Traditional GPUs also provided limited cache bandwidth. Fata- halian et al. discuss in [11] that low bandwidth cache designs on GPUs limit the types of applications from bene(cid:2)ting from the com- putational power available on these architectures. Work discussed in [12] uses an analytical cache performance prediction model for GPU-based algorithms. Their results indicate that memory opti-

mization techniques designed for CPU-based algorithms may not be directly applicable to GPUs. With the introduction of reasonably sized low-latency, on-chip memory in new generations of GPUs, this issue and its optimizations have become less critical.

A programming interface alternative to CUDA is available for the AMD Stream Processor, using the R580 GPU, in the form of the Close to Metal (CTM) compute runtime driver [1]. Like CUDA, CTM can maintain the usage of the GPU as a graphics engine; however, instead of abstracting away architecture-level in- structions, CTM completely exposes the ISA to the programmer for (cid:2)ne-grained control. Furthermore, the R580 continues to resemble previous generation GPUs with their divided architecture for ver- tex and pixel processing, whereas the GeForce 8800 has a more general, uni(cid:2)ed model. This is presented in the next section.

Intel’s C for Heterogeneous Integration (CHI) programming en- vironment [27] is a different approach to tightly integrate accelera- tors such as GPUs and general purpose CPU cores together based on the proposed EXOCHI [27] model. EXOCHI supports a shared virtual memory heterogeneous multi-threaded programming model with minimal OS intrusion. In CUDA execution model, GPU is a device with a separate address space from CPU. As a result, all data communication and synchronization between CPU and GPU is explicitly performed through the GPU device driver.

3. Architecture Overview The GeForce 8800 GPU is effectively a large set of processor cores with the ability to directly address into a global memory. This al- lows for a more general and (cid:3)exible programming model than pre- vious generations of GPUs, making it easier for developers to im- plement data-parallel kernels. In this section we discuss NVIDIA’s Compute Uni(cid:2)ed Device Architecture (CUDA) and the major mi- croarchitectural features of the GeForce 8800. A more complete description can be found in [3, 18]. It should be noted that this architecture, although more exposed than previous GPU architec- tures, still has details which have not been publicly revealed.

3.1 Threading Model

The CUDA programming model is ANSI C extended by several keywords and constructs. The GPU is treated as a coprocessor that executes data-parallel kernel code. The user supplies a single source program encompassing both host (CPU) and kernel (GPU) code. These are separated and compiled as shown in Figure 1. Each CUDA program consists of multiple phases that are executed on either the CPU or the GPU. The phases that exhibit little or no data parallelism are implemented in host (CPU) code, which is ex- pressed in ANSI C and compiled with the host C compiler as shown in Figure 1. The phases that exhibit rich data parallelism are im- plemented as kernel functions in the device (GPU) code. A kernel function de(cid:2)nes the code to be executed by each of the massive number of threads to be invoked for a data-parallel phase. These kernel functions are compiled by the NVIDIA CUDA C compiler and the kernel GPU object code generator. There are several re- strictions on kernel functions: there must be no recursion, no static variable declarations, and a non-variable number of arguments. The host code transfers data to and from the GPU’s global memory us- ing API calls. Kernel code is initiated by performing a function call.

Threads executing on the GeForce 8800 are organized into a three-level hierarchy. At the highest level, all threads in a data- parallel execution phase form a grid; they all execute the same ker- nel function. Each grid consists of many thread blocks. A grid can be at most 216 (cid:0) 1 blocks in either of two dimensions, and each block has unique coordinates. In turn, each thread block is a three- dimensional array of threads, explicitly de(cid:2)ned by the application developer, that is assigned to an SM. The invocation parameters of

(cid:14)(cid:15)(cid:16)(cid:9)(cid:17)(cid:18)(cid:18)(cid:4)(cid:19)(cid:20)(cid:21)(cid:22)(cid:9)(cid:23) (cid:24)(cid:4)(cid:6)(cid:2)(cid:4)(cid:21)(cid:9)(cid:25)(cid:11)(cid:8)(cid:4) (cid:1)(cid:8)(cid:9)(cid:10)(cid:4)(cid:26)(cid:12)

(cid:24)(cid:4)(cid:6)(cid:2)(cid:4)(cid:21)(cid:9)(cid:27)(cid:20)(cid:28)(cid:4)(cid:13)(cid:3)(cid:9) (cid:25)(cid:11)(cid:8)(cid:4)(cid:9) (cid:14)(cid:4)(cid:2)(cid:4)(cid:6)(cid:7)(cid:3)(cid:11)(cid:6)

(cid:24)(cid:4)(cid:6)(cid:2)(cid:4)(cid:21)(cid:9)(cid:27)(cid:20)(cid:28)(cid:4)(cid:13)(cid:3)(cid:9) (cid:25)(cid:11)(cid:8)(cid:4) (cid:1)(cid:8)(cid:9)(cid:10)(cid:4)(cid:27)(cid:21)(cid:11)(cid:12)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:3)(cid:4)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:6)(cid:13)(cid:4) (cid:1)(cid:2)(cid:3)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:4)(cid:5)(cid:11)(cid:12)

(cid:13)(cid:12)(cid:8)(cid:7)(cid:13)(cid:13) (cid:13)(cid:10)(cid:3)(cid:14)(cid:15)(cid:7)(cid:16)(cid:14)(cid:17)(cid:7)(cid:9)(cid:14)(cid:17)(cid:7) (cid:18)(cid:19)(cid:3)(cid:8)(cid:9)(cid:19)(cid:7)(cid:20)(cid:21)(cid:15)(cid:22)(cid:23)(cid:22)(cid:24)(cid:25)(cid:10)

(cid:16)(cid:28)(cid:25)(cid:5)(cid:11)(cid:15)(cid:9)(cid:8)(cid:19)(cid:25)

(cid:25)(cid:15)(cid:16)(cid:9)(cid:26)(cid:11)(cid:18)(cid:3)(cid:9)(cid:25)(cid:11)(cid:8)(cid:4) (cid:1)(cid:2)(cid:3)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:4)(cid:5)(cid:12)

(cid:26)(cid:11)(cid:18)(cid:3)(cid:9)(cid:25)(cid:11)(cid:19)(cid:29)(cid:30)(cid:21)(cid:4)(cid:6)

(cid:26)(cid:11)(cid:18)(cid:3)(cid:9)(cid:31)(cid:30)(cid:2)(cid:7)(cid:6)(cid:22) (cid:1)(cid:2)(cid:3)(cid:3)(cid:4)(cid:3)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:4)(cid:3)(cid:12)

Figure 1. CUDA Compilation Flow

a kernel function call de(cid:2)ne the organization of the sizes and di- mensions of the thread blocks in the grid thus generated. Threads also have unique coordinates and up to 512 threads can exist in a block. Threads in a block can share data through a low-latency, on- chip shared memory and can perform barrrier synchronization by syncthreads primitive. Threads are otherwise in- invoking the dependent; synchronization across thread blocks can only be safely accomplished by terminating a kernel. Finally, the hardware groups threads in a way that affects performance, which is discussed in Section 3.2.

An application developer for this platform can compile CUDA code to an assembly-like representation of the code called PTX. PTX is not natively executed, but is processed by a run-time en- vironment, making it uncertain what instructions are actually exe- cuted on a cycle-by-cycle basis. Two examples we have observed are simple cases of loop-invariant code that can be easily moved and branches which are split into condition evaluations and predi- cated jump instructions. However, PTX is generally suf(cid:2)cient in the initial stages of estimating resource requirements of an application and optimizing it.

3.2 Base Microarchitecture

Figure 2 depicts the microarchitecture of the GeForce 8800. It consists of 16 streaming multiprocessors (SMs), each containing eight streaming processors (SPs), or processor cores, running at 1.35GHz. Each core executes a single thread’s instruction in SIMD (single-instruction, multiple-data) fashion, with the instruction unit broadcasting the current instruction to the cores. Each core has one 32-bit, single-precision (cid:3)oating-point, multiply-add arithmetic unit that can also perform 32-bit integer arithmetic. Additionally, each SM has two special functional units (SFUs), which execute more complex FP operations such as reciprocal square root, sine, and cosine with low multi-cycle latency. The arithmetic units and the SFUs are fully pipelined, yielding 388.8 GLOPS (16 SMs * 18 FLOPS/SM * 1.35GHz) of peak theoretical performance for the GPU.

Each SM has 8192 registers which are dynamically partitioned among the threads running on it. Non-register memories with dis- tinctive capabilities and uses are described in Table 1 and depicted in Figure 2. Variables in the source code can be declared to reside in global, shared, local, or constant memory. Texture memory is accessed through API calls which compile to special instructions. Bandwidth to off-chip memory is very high at 86.4 GB/s, but mem- ory bandwidth can saturate if many threads request access within a short period of time. In addition, this bandwidth can be obtained only when accesses are contiguous 16-word lines; in other cases the achievable bandwidth is a fraction of the maximum. Optimizations to coalesce accesses into 16-word lines and reuse data are generally necessary to achieve good performance.

There are several non-storage limits to the number of threads that can be executed on the system. First, a maximum of 768 si- multaneously active thread contexts is supported per SM. Second, an integral number of up to eight thread blocks can be run per SM at one time. The number of thread blocks that are simultaneously resident on an SM is limited by whichever limit of registers, shared memory, threads, or thread blocks is reached (cid:2)rst.This has two con-

Table 1. Properties of GeForce 8800 Memories

Memory

Global

Local

Shared

Constant

Texture

Location

off-chip

off-chip

on-chip

on-chip cache

on-chip cache

Size

768MB total

up to global 16KB per SM

64KB total

up to global

Hit Latency 200-300 cycles

same as global ’register latency

’register latency

>100 cycles

Read- Only no

no

no

yes

yes

Program Scope global

function

function

global

global

Description

Large DRAM. All data reside here at the beginning of execution. Directly addressable from a kernel using pointers. Backing store for constant and texture memories. Used more ef(cid:2)ciently when multiple threads simultaneously access contiguous elements of memory, enabling the hardware to coalesce memory accesses to the same DRAM page. Space for register spilling, etc.

Local scratchpad that can be shared between threads in a thread block. Organized into 16 banks. Does not appear to have error detection. If instructions issued in the same cycle access different locations in the same bank, a bank con(cid:3)ict stall occurs. It is possible to organize both threads and data such that bank con(cid:3)icts seldom or never occur. 8KB cache per SM, with data originally residing in global memory. The 64KB limit is set by the programming model. Often used for lookup tables. The cache is single-ported, so simultaneous requests within an SM must be to the same address or delays will occur. 16KB cache per two SMs, with data originally residing in global memory. Capitalizes on 2D locality. Can perform hardware interpolation and have con(cid:2)gurable returned-value behavior at the edges of textures, both of which are useful in certain applications such as video encoders.

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:2)

(cid:6)(cid:7)(cid:8)(cid:9)(cid:10) ++ +

(cid:6)(cid:7)(cid:8)(cid:11)

(cid:6)(cid:7)(cid:8)(cid:9)

(cid:6)(cid:16)(cid:23)(cid:31)(cid:2)’(cid:8)(cid:7)(cid:2)!(cid:21)(cid:31)(

$(cid:2)%(cid:4)(cid:26)(cid:27)(cid:2)(cid:31)(cid:8)&(cid:4)(cid:20)(cid:2) +++

"(cid:31)(cid:21)(cid:5)(cid:2)(cid:26)(cid:26)(cid:21)(cid:31)(cid:8)(cid:9)

"(cid:31)(cid:21)(cid:5)(cid:2)(cid:26)(cid:26)(cid:21)(cid:31)(cid:8)#

(cid:15)(cid:21)(cid:25)(cid:26)(cid:27)(cid:23)(cid:25)(cid:27)(cid:8)(cid:15)(cid:23)(cid:5)(cid:16)(cid:2)

)(cid:25)(cid:26)(cid:27)(cid:31)(cid:30)(cid:5)(cid:27)(cid:4)(cid:21)(cid:25) *(cid:25)(cid:4)(cid:27)

(cid:6)&*(cid:8)(cid:9)

(cid:6)&*(cid:8)(cid:11)

quickly switch to a ready warp resident in the SM. The SM stalls only if there are no warps with ready operands available. Schedul- ing freedom is high in many applications because threads in dif- ferent warps are independent with the exception of explicit barrier synchronizations among threads in the same thread block.

In summary, there are hard limits to the memories, threads, and total bandwidth available to an application running on the GeForce 8800. Managing these limits is critical when optimizing applications, but strategies for avoiding one limit can cause other limits to be hit. They can also reduce the number of thread blocks that can run simultaneously. In addition, managing the behavior of threads so that those in the same warp follow the same control paths and load contiguous values from global memory can also improve performance.

(cid:28)(cid:2)(cid:29)(cid:27)(cid:30)(cid:31)(cid:2)(cid:8)(cid:15)(cid:23)(cid:5)(cid:16)(cid:2)

(cid:12)(cid:13)(cid:13)(cid:14)(cid:15)(cid:16)(cid:4)(cid:17)(cid:8)(cid:18)(cid:19)(cid:20)(cid:21)(cid:22)(cid:23)(cid:20)(cid:24)(cid:8)(cid:15)(cid:21)(cid:25)(cid:26)(cid:27)(cid:23)(cid:25)(cid:27)(cid:24)(cid:8)(cid:28)(cid:2)(cid:29)(cid:27)(cid:30)(cid:31)(cid:2) (cid:8)(cid:7)(cid:2)!(cid:21)(cid:31)(cid:4)(cid:2)(cid:26)

Figure 2. Basic Organization of the GeForce 8800

sequences. First, optimization may have negative effects in some cases because small changes have multiplicative resource usage ef- fects (due to the large number of threads) that cause fewer thread blocks and thus threads to be simultaneously executed. Second, it is relatively easy to be (cid:147)trapped(cid:148) in a local maximum when hand- optimizing code. Developers may need to try widely varying con- (cid:2)gurations to (cid:2)nd one with satisfactory performance.

During execution, threads within a block are grouped into warps of 32 parallel threads, which are the granular multi-threading scheduling unit. Warps are formed from continuous sections of threads in a thread block: the (cid:2)rst 32 threads in a block form the (cid:2)rst warp, etc. Although warps are not explicitly declared in CUDA code, knowledge of them can enable useful code and data optimiza- tions on the GeForce 8800. A scoreboard indicates when all of a warp’s operands are ready for execution. It then executes the same instruction for the 32 threads in the warp. An SM issues only one instruction at a time for all threads in a warp; when threads in a warp take different control paths, it is assumed that multiple passes with suppression of threads on divergent paths are required to com- plete execution. It is generally desirable to group threads to avoid this situation. If a thread block is not evenly divisible by the warp size, any remaining issue slots are wasted.

An SM can perform zero-overhead scheduling to interleave warps and hide the latency of global memory accesses and long- latency arithmetic operations. When one warp stalls, the SM can

4. Performance and Optimization This section uses a microbenchmark to demonstrate how the proper balancing of shared resource usage is critical to achieving ef(cid:2)cient execution resource utilization and thus high performance on the GeForce 8800. There are three basic principles to consider when optimizing an application for the platform. First, the ﬂoating point throughput of an application depends on the percentage of its in- structions that are ﬂoating point operations. The GPU is capable of issuing 172.8 billion operations per second on the SPs. These include fused multiply-add operations, which we count as two op- erations for throughput calculations. If 1/4 of an application’s in- struction mix are fused multiply-adds, then its performance can be at most 2 * 1/4 FP * 172.8 billion ops per second = 86.4 GFLOPS. This performance is reached when the SPs are fully oc- cupied, which is achievable in an application that has many threads, does not have many synchronizations, and does not stress global memory bandwidth. In this situation, reducing the number of in- structions that do not contribute to data computation generally re- sults in kernel speedup. However, maximizing computational ef(cid:2)- ciency can be challenging, due to discontinuities in the optimization space [22].

Second, when attempting to achieve an application’s maximum performance, the primary concern often is managing global mem- ory latency. This is done by creating enough threads to keep SPs oc- cupied while many threads are waiting on global memory accesses. As previously stated, threads may need to of a (cid:2)ner granularity than those for traditional multicore execution to generate enough threads. The required number of threads depends on the percentage of global accesses and other long-latency operations in an appli- cation: applications consisting of a small percentage of these op- erations require fewer threads to achieve full SP occupancy. The

limit on registers and shared memory available per SM can con- strain the number of active threads, sometimes exposing memory latency. We show one example where the use of additional regis- ters in an attempted optimization allows one fewer thread block to be scheduled per SM, reducing performance.

Finally, global memory bandwidth can limit the throughput of the system. Increasing the number of threads does not help performance in this situation. Alleviating the pressure on global memory bandwidth generally involves using additional registers and shared memory to reuse data, which in turn can limit the number of simultaneously executing threads. Balancing the usage of these resources is often non-intuitive and some applications will run into resource limits other than instruction issue on this architecture.

The example we use to illustrate these principles is a matrix multiplication kernel. In matrix multiplication, the value of an ele- ment in the result matrix is calculated by computing the dot product of the corresponding row of the (cid:2)rst matrix and column of the sec- ond matrix. For this example we assume densely populated input matrices. We analyze several code versions and their sustained per- formance when multiplying two square matrices with a height and width of 4096 elements. The stated resource usage is for CUDA version 0.8; later versions of CUDA may have different usages.

4.1 Initial Code Version

We begin with a simple version of matrix multiplication. The ma- trix multiplication kernel creates a thread for each result element for the multiplication, for a total of 4K*4K threads. Many threads are created in an attempt to hide the latency of global memory by overlapping execution. These threads loop through a sequence that loads two values from global memory, multiplies them, and accu- mulates the value. Figure 3(a) shows the core loops of the dot- product computation kernel; starting values for indexA, indexB, and indexC are determined by block and thread coordinates, which the hardware supports. This code uses ten registers per thread, al- lowing the maximum of 768 threads to be scheduled per SM. For convenience, we group them as three thread blocks of 256 threads each.

Performance for this code is 10.58 GFLOPS, which is lower than highly optimized libraries executing on a CPU using SIMD extensions. By examining the PTX for this code, we (cid:2)nd that there is approximately3 one fused multiply-add out of eight op- erations in the inner loop, for a estimated potential throughput of 43.2 GFLOPS. Because we have the maximum number of threads scheduled per SM, the bottleneck appears to be global memory bandwidth. 1/4 of the operations executed during the loop are loads from off-chip memory, which would require a bandwidth of 173 GB/s (128 SPs * 1/4 instructions * 4 B/instruction * 1.35GHz) to fully utilize the SPs.4 Thus, the strategy for optimizing this kernel is to improve data reuse and reduce global memory access.

4.2 Use of Local Storage

In Figure 3(a), although the computations of two result elements in the same row or column share half their input data (the same indexA or indexB values), the previous code accesses global memory for each datum in every thread. A common optimiza- tion for this type of access pattern is to enhance data sharing via tiling [14]. In the GeForce 8800, developers can utilize shared memory to amortize the global latency cost when values are reused.

3As previously mentioned, PTX code does not necessarily translate to executed instructions, so instruction counts are estimates. 4This is also an estimate. Threads can simultaneously load the same value from memory and the memory system may be able to combine these into a single request.

Using low-overhead block synchronization values, can be shared between threads: one thread loads a datum and then synchronizes so that other threads in the same block can use it. Finally, we can also take advantage of contiguity in main memory accesses when loading in values as a block, reducing the cycles needed to access the data.

Figure 3(b) shows the code for a tiled matrix multiplication, with a tile size of 16x16, or 256 result elements and threads. During execution, the threads work within two input tiles that stride across 16 contiguous rows or columns in the input matrices. Each of the 256 threads is tied to a speci(cid:2)c coordinate in a tile. It loads the element at that coordinate from the two input tiles into shared memory, so cooperatively the threads load the complete tiles. These loads are organized to take advantage of global access coalescing. The threads then synchronize to establish consistency, which enables each thread to load all of its inputs contained in the tiles from shared memory. Finally, the threads calculate the partial dot product for the inputs in shared memory within a loop.

The choice of tile shape and size is a key decision. For a given size, square tiles generally improve the computation to memory ac- cess ratio by improving data locality among threads. Larger tile sizes increase data sharing and thus global memory ef(cid:2)ciency. The tiling support code adds several overhead instructions per tile, which also makes larger sizes more ef(cid:2)cient. On this architec- ture, developers also need to consider whether a tile size provides enough threads to have good occupancy. Figure 4 shows the re- sults of experimenting with different tile sizes. 4x4 tiles use only 16 threads, so half of the issue slots in a warp would go unused. This inef(cid:2)ciency, coupled with the 8 thread block limit, causes performance to be worse than the non-tiled code. 8x8 tiles create thread blocks that occupy two warps, but would still need 12 thread blocks to fully occupy an SM, 50% more than the supported limit. 12x12 tiles use 144 threads, which is also not an integral number of warps, and also requires padding of the arrays to prevent over- run. 16x16 is the largest convenient size for this platform, and we can schedule three thread blocks of 8 warps each, for the maximum of 768 threads. Global memory coalescing also happens naturally with this con(cid:2)guration. Other applications may have higher perfor- mance with smaller tile sizes when they allow a larger number of threads to be scheduled.

The use of 16x16 tiles reduces global loads by a factor of 16 over the non-tiled con(cid:2)guration, so instead of the bandwidth re- quirement being twice what is available, it is now approximately an eighth, removing it as the bottleneck to performance. The ad- ditional instructions reduce the potential throughput slightly below that of the original code. The 16x16 tiled version of matrix multi- plication achieves 46.49 GFLOPS, or approximately 4.5X the exe- cution throughput of the initial version. This is slightly higher than the estimated potential throughput of the original code, so it appears that the application achieves full usage of the SPs.

Register usage must also be managed to avoid performance losses. Some versions of this code use 11 registers per thread instead of 10. To run three thread blocks, this requires 3 blocks/SM * 256 threads/block * 11 registers = 8488 registers, which is larger than an SM’s register (cid:2)le. Thus, each SM executes only two blocks simultaneously, which reduces performance.

4.3 Executed Instruction Reduction

As noted in the previous example, tiling reduces the global mem- ory accesses at the expense of additional instructions. Now that our code achieves its potential throughput, we can examine whether the same work can be done with fewer instructions to improve ef- (cid:2)ciency and performance. The obvious targets for reduction are those operations which are not part of the core data computation, such as branches and address calculations. Common subexpres-

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:6)(cid:8)(cid:9) (cid:10)(cid:11)(cid:12)(cid:6)(cid:13)(cid:30)(cid:30)(cid:30)(cid:21)(cid:6)(cid:22)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:6)(cid:8)(cid:9) (cid:10)(cid:11)(cid:12)(cid:6)(cid:13)(cid:30)(cid:30)(cid:30)(cid:21)(cid:6)(cid:22)

(cid:1)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:1)(cid:1) (cid:10)(cid:31)(cid:11) (cid:2)(cid:6)(cid:19)!(cid:23)"#(cid:26)(cid:23)"#(cid:26)(cid:9) (cid:1)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:1)(cid:1) (cid:10)(cid:31)(cid:11) (cid:2)(cid:6)(cid:28)!(cid:23)"#(cid:26)(cid:23)"#(cid:26)(cid:9)

(cid:1)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:1)(cid:1) (cid:10)(cid:31)(cid:11) (cid:2)(cid:6)(cid:19)!(cid:23)"#(cid:26)(cid:23)"#(cid:26)(cid:9) (cid:1)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:1)(cid:1) (cid:10)(cid:31)(cid:11) (cid:2)(cid:6)(cid:28)!(cid:23)"#(cid:26)(cid:23)"#(cid:26)(cid:9)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:6)(cid:8)(cid:9)

(cid:10)(cid:11)(cid:12)(cid:6)(cid:13)(cid:14)(cid:6)(cid:7)(cid:6)(cid:8)(cid:9)(cid:6)(cid:14)(cid:6)(cid:15)(cid:6)(cid:16)(cid:14)(cid:17)(cid:2)(cid:18)(cid:19)(cid:9) (cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:14)(cid:20)(cid:20)(cid:21) (cid:6)(cid:6)(cid:22) (cid:6)(cid:6)(cid:6)(cid:6)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:20)(cid:7)(cid:6)(cid:19)(cid:23)(cid:14)(cid:24)(cid:17)(cid:3)(cid:25)(cid:19)(cid:26) (cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:27)(cid:6)(cid:28)(cid:23)(cid:14)(cid:24)(cid:17)(cid:3)(cid:25)(cid:28)(cid:26)(cid:9) (cid:6)(cid:6)(cid:6)(cid:6)(cid:14)(cid:24)(cid:17)(cid:3)(cid:25)(cid:19)(cid:20)(cid:20)(cid:9) (cid:6)(cid:6)(cid:6)(cid:6)(cid:14)(cid:24)(cid:17)(cid:3)(cid:25)(cid:28)(cid:6)(cid:20)(cid:7)(cid:6)(cid:16)(cid:14)(cid:17)(cid:2)(cid:18)(cid:28)(cid:9) (cid:6)(cid:6)(cid:29)

(cid:1)(cid:23)(cid:14)(cid:24)(cid:17)(cid:3)(cid:25)(cid:1)(cid:26)(cid:6)(cid:7)(cid:6)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:9)

(cid:6)(cid:6)$$(cid:6)(cid:31)(cid:11) (cid:17)(cid:6)(cid:14)(cid:24)(cid:5)%(cid:2)(cid:6)(cid:2)(cid:14)(cid:31)(cid:3)(cid:6)(cid:3)(cid:31)(cid:3)(cid:4)(cid:3)(cid:24)(cid:2)! (cid:6)(cid:6)(cid:19)!(cid:23)(cid:2)&(cid:26)(cid:23)(cid:2)(cid:25)(cid:26)(cid:6)(cid:7)(cid:6)(cid:19)(cid:23)(cid:14)(cid:24)(cid:17)(cid:3)(cid:25)(cid:19)(cid:26)(cid:9) (cid:6)(cid:6)(cid:28)!(cid:23)(cid:2)&(cid:26)(cid:23)(cid:2)(cid:25)(cid:26)(cid:6)(cid:7)(cid:6)(cid:28)(cid:23)(cid:14)(cid:24)(cid:17)(cid:3)(cid:25)(cid:28)(cid:26)(cid:9) (cid:6)(cid:6)(cid:14)(cid:24)(cid:17)(cid:3)(cid:25)(cid:19)(cid:6)(cid:20)(cid:7)(cid:6)"#(cid:9) (cid:6)(cid:6)(cid:14)(cid:24)(cid:17)(cid:3)(cid:25)(cid:28)(cid:6)(cid:20)(cid:7)(cid:6)"#(cid:6)(cid:27)(cid:6)(cid:16)(cid:14)(cid:17)(cid:2)(cid:18)(cid:28)(cid:9)

(cid:1)(cid:1)(cid:2)(cid:8)(cid:9)(cid:10)(cid:11)(cid:3)(cid:5)(cid:6)(cid:4)(cid:7)(cid:2)(cid:12)(cid:13)(cid:9)

(cid:6)(cid:6)$$(cid:6)’(cid:11)(cid:4)(cid:5)%(cid:2)(cid:3)(cid:6)(cid:12)(cid:3)!%(cid:31)(cid:2)!(cid:6)(cid:10)(cid:11)(cid:12)(cid:6)(cid:2)(cid:14)(cid:31)(cid:3) (cid:6)(cid:6)(cid:10)(cid:11)(cid:12)(cid:6)(cid:13)(cid:14)(cid:6)(cid:7)(cid:6)(cid:8)(cid:9)(cid:6)(cid:14)(cid:6)(cid:15)(cid:6)"#(cid:9)(cid:6)(cid:14)(cid:20)(cid:20)(cid:21) (cid:6)(cid:6)(cid:6)(cid:6)(cid:22) (cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:20)(cid:7)(cid:6)(cid:19)!(cid:23)(cid:2)&(cid:26)(cid:23)(cid:14)(cid:26) (cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:27)(cid:6)(cid:28)!(cid:23)(cid:14)(cid:26)(cid:23)(cid:2)(cid:25)(cid:26)(cid:9) (cid:6)(cid:6)(cid:6)(cid:6)(cid:29)

(cid:6)(cid:6)$$(cid:6)(cid:31)(cid:11) (cid:17)(cid:6)(cid:14)(cid:24)(cid:5)%(cid:2)(cid:6)(cid:2)(cid:14)(cid:31)(cid:3)(cid:6)(cid:3)(cid:31)(cid:3)(cid:4)(cid:3)(cid:24)(cid:2)! (cid:6)(cid:6)(cid:19)!(cid:23)(cid:2)&(cid:26)(cid:23)(cid:2)(cid:25)(cid:26)(cid:6)(cid:7)(cid:6)(cid:19)(cid:23)(cid:14)(cid:24)(cid:17)(cid:3)(cid:25)(cid:19)(cid:26)(cid:9) (cid:6)(cid:6)(cid:28)!(cid:23)(cid:2)&(cid:26)(cid:23)(cid:2)(cid:25)(cid:26)(cid:6)(cid:7)(cid:6)(cid:28)(cid:23)(cid:14)(cid:24)(cid:17)(cid:3)(cid:25)(cid:28)(cid:26)(cid:9) (cid:6)(cid:6)(cid:14)(cid:24)(cid:17)(cid:3)(cid:25)(cid:19)(cid:6)(cid:20)(cid:7)(cid:6)"#(cid:9) (cid:6)(cid:6)(cid:14)(cid:24)(cid:17)(cid:3)(cid:25)(cid:28)(cid:6)(cid:20)(cid:7)(cid:6)"#(cid:6)(cid:27)(cid:6)(cid:16)(cid:14)(cid:17)(cid:2)(cid:18)(cid:28)(cid:9)

(cid:1)(cid:1)(cid:2)(cid:8)(cid:9)(cid:10)(cid:11)(cid:3)(cid:5)(cid:6)(cid:4)(cid:7)(cid:2)(cid:12)(cid:13)(cid:9)

(cid:6)(cid:6)$$(cid:6)’(cid:11)(cid:4)(cid:5)%(cid:2)(cid:3)(cid:6)(cid:12)(cid:3)!%(cid:31)(cid:2)!(cid:6)(cid:10)(cid:11)(cid:12)(cid:6)(cid:2)(cid:14)(cid:31)(cid:3) (cid:6)(cid:6)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:20)(cid:7) (cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:19)!(cid:23)(cid:2)&(cid:26)(cid:23)(cid:8)(cid:26)(cid:6)(cid:27)(cid:6)(cid:28)!(cid:23)(cid:8)(cid:26)(cid:23)(cid:2)(cid:25)(cid:26)(cid:9) (cid:6)(cid:6)(cid:30)(cid:30)(cid:30) (cid:6)(cid:6)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:20)(cid:7) (cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:19)!(cid:23)(cid:2)&(cid:26)(cid:23)"((cid:26)(cid:6)(cid:27)(cid:6)(cid:28)!(cid:23)"((cid:26)(cid:23)(cid:2)(cid:25)(cid:26)(cid:9)

(cid:1)(cid:1)(cid:2)(cid:8)(cid:9)(cid:10)(cid:11)(cid:3)(cid:5)(cid:6)(cid:4)(cid:7)(cid:2)(cid:12)(cid:13)(cid:9)

(cid:29) (cid:1)(cid:23)(cid:14)(cid:24)(cid:17)(cid:3)(cid:25)(cid:1)(cid:26)(cid:6)(cid:7)(cid:6)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:9)

(cid:1)(cid:1)(cid:2)(cid:8)(cid:9)(cid:10)(cid:11)(cid:3)(cid:5)(cid:6)(cid:4)(cid:7)(cid:2)(cid:12)(cid:13)(cid:9)

(cid:29) (cid:1)(cid:23)(cid:14)(cid:24)(cid:17)(cid:3)(cid:25)(cid:1)(cid:26)(cid:6)(cid:7)(cid:6)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:9)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:7)(cid:2)(cid:9)(cid:4)(cid:10)(cid:11)(cid:12)(cid:13)(cid:7)(cid:14)(cid:6)

(cid:1)(cid:15)(cid:3)(cid:4)(cid:16)(cid:7)(cid:9)(cid:11)(cid:17)(cid:4)(cid:10)(cid:11)(cid:12)(cid:13)(cid:7)(cid:14)(cid:6)

(cid:1)(cid:18)(cid:3)(cid:4)(cid:19)(cid:6)(cid:12)(cid:14)(cid:9)(cid:9)(cid:11)(cid:17)(cid:4)(cid:10)(cid:11)(cid:12)(cid:13)(cid:7)(cid:14)(cid:6)

Figure 3. Partial Kernel Codes for Matrix Multiplication. CUDA keywords are bold.

(cid:6) (cid:5) (cid:4) (cid:3) (cid:2) (cid:1)

100

90

80

70

60

50

40

30

all execution and have more effect on instruction cache ef(cid:2)ciency. In this case, shared memory usage is not affected and a register is saved by removing the unrolled loop’s induction variable, although it is not used for anything else. The performance of other tile sizes is only marginally improved by unrolling.

4.4 Balancing Applications and Optimization Interaction

20

10

0

d e

l i t

y n o

l

& d e

l i t

d e o r n u

l l

d e

l i t

y n o

l

& d e

l i t

d e o r n u

l l

d e

l i t

y n o

l

& d e

l i t

d e o r n u

l l

d e

l i t

y n o

l

& d e

l i t

not tiled 8x8 tiles Figure 4. Performance of Matrix Multiplication Kernels

4x4 tiles

12x12 tiles

16x16 tiles

sion elimination and loop unrolling are two classical compiler op- timizations that can achieve this goal. What is less clear is whether these operations increase or reduce the number of registers used per thread and thus affect the number of thread blocks that can be scheduled per SM. The compiler’s scheduler further complicates matters, as it may attempt to improve the execution speed of each thread at the cost of extra registers.

d e o r n u

l l

At this point, the matrix multiplication code appears to be well- optimized, with actual performance near that of the estimated po- tential. A large portion of the non-data computation instructions have been removed. Registers and threads are fully utilized. There is still a signi(cid:2)cant amount of shared memory available, but there is no obvious way to use it to increase performance.

In an effort to further improve performance, a developer can at- tempt to improve SP occupancy by reducing exposed intrathread global memory latency. We implemented a prefetching optimiza- tion that initiates loads to the next tiles prior to performing com- putation for the current tile. The optimization also increases the number of registers required by each thread by two, to 11. As pre- viously mentioned, this reduces the number of blocks that can be scheduled per SM by 1, reducing simultaneous thread count by a third. This version was capable of 87.10 GFLOPS performance, in- ferior to performing only tiling and unrolling.

For tiled matrix multiplication, the innermost loop that com- putes the partial dot product has a small body and constant itera- tion count. This can be unrolled by several different factors, each removing some test and branch instructions. However, the best per- formance can be achieved by completely unrolling the loop. This has the effect of removing all loop branches, induction variable in- crements, and inner loop address calculation instructions, since the offsets are now constants. It also reduces the register usage by one, to 9 registers, by eliminating an induction variable. The PTX code for the unrolled 16x16 tiled version shows that approximately 16 out 59 instructions, slightly higher than 1/4, are fused multiply- adds. From that, we can calculate potential throughput of this code at 93.72 GFLOPS, with memory bandwidth requirements still be- low the amount available. The achieved performance of the code is 91.14 GFLOPS, similar to highly-optimized CUDA 0.8 libraries provided by NVIDIA.

In this case, intra-thread latency reduction is insuf(cid:2)cient to make up for the reduction of simultaneously executed threads. However, the difference between the performances of the two con- (cid:2)gurations is only 5%. Although we have reduced the number of simultaneously active threads by a third, these threads take nearly a third less time to execute because the prefetching optimization eliminates much of the time threads wait on global memory. This illustrates the principle that although many threads are generally desirable, full utilization of execution resources is achieved when there are enough threads to avoid being stalled on global mem- ory access. These kinds of optimization interactions, plus the un- certainty of the architecture features and code executed, make it challenging to (cid:2)nd the peak performance of an application on this architecture.

In general the unrolling of small inner loops will produce pos- itive gain when memory bandwidth is not already an issue and scheduling does not trigger extra register usage that reduces the number of active thread blocks. Unrolling outer loops is less likely to provide bene(cid:2)t because they contribute fewer branches to over-

5. Application Study We performed an application study with the intent of testing the applicability and effectiveness of the principles in Section 4 on real applications. We have selected a suite of applications acquired from various sources that have different purposes and code behavior but are also reasonably well-suited for execution on the GeForce 8800.

These applications, even ones with kernels of a few hundred lines, often have a large variety of instructions, operate on larger data sets, and have more control (cid:3)ow than microbenchmarks. Many of these contribute to bottlenecks other than instruction issue bandwidth on this platform, enabling a better evaluation of the system. To our knowledge, this is the (cid:2)rst study of this breadth on a GPU.

Table 2 lists some of the applications that have been ported to CUDA, along with source and kernel lines of code (excluding com- ments and whitespace). Benchmark versions of the applications are available [2]. The larger codes often required more modi(cid:2)cation to port to CUDA; the most extreme case was H.264, which in- volved a large-scale code transformation to extract the motion esti- mation kernel from non-parallel application code. The percentage of single-thread CPU execution time spent in kernels is given to show the total application speedup that can be achieved as limited by Amdahl’s Law. For example, FDTD’s kernel takes only 16.4% of execution time, limiting potential application speedup to 1.2X. In general, kernel execution occupied the vast majority of CPU-only execution for these applications.

Table 3 shows characteristics of the optimized application im- plementations. The data for matrix multiplication is listed for com- parison.5 The maximum number of simultaneously active threads shows the amount of thread parallelism available on the hardware at a given time, taking resource constraints into account, with a maximum of 12288 across the 16 SMs. There is a wide range of values, with little correlation to actual speedup. The total threads in a given application often numbers in the millions. The number of registers and the amount of shared memory per thread show the degree of local resource utilization.

Other information in the table includes the ratio of global mem- ory cycles to computation cycles after shared memory and caches are utilized to their fullest extent, expressing the global memory bandwidth requirements of the most time-consuming kernel of each application. We discuss how this correlates to performance in Sec- tion 5.1. GPU execution time expresses how much of the total ex- ecution time the application kernels occupy after being ported to the GPU. CPU-GPU transfer time is shown for comparison with the computation time. One interesting case is H.264, which spends more time in data transfer than GPU execution. Finally, we list the architectural bottleneck(s) that appear to be limiting these imple- mentations from achieving higher performance.

The two rightmost columns of Table 3 list the performance of ported applications. The baseline, single-thread CPU performance is measured on an Opteron 248 system running at 2.2GHz with 1GB main memory. The choice was made with the intent of hav- ing a high-performance, single-core processor; similar CPU perfor- mance is found with newer, high clock rate multicore architectures. For applications with outstanding GPU speedup, we applied opti- mizations such as SIMD instructions and fast math libraries to the CPU-only versions to ensure that comparisons were reasonable. We measure both the speedup of CUDA kernel execution over single- thread CPU kernel execution and total application speedup, with all (cid:3)oating point numbers set to single-precision. Measurements were made with typical long-running inputs; e.g., for SPEC CPU bench- marks the reference inputs were used.

5.1 Performance Trends of Optimized Applications

In general, we obtain signi(cid:2)cant kernel and application speedup across our suite, as shown in Table 3. Compute-intensive kernels with relatively few global memory accesses achieve very high

5The GPU speedup for matrix multiplication uses a highly optimized li- brary with SSE2 support as comparison. Kernel speedup compared to a CPU binary without SIMD support and optimized only for cache usage is on the order of 100X.

performance. Even kernels which are not as compute-intensive still achieve respectable performance increases because of the GeForce 8800’s ability to run a large number of threads simultaneously. Low-latency (cid:3)oating-point execution is a major factor in speedup, as is the use of caches and shared memory to reduce latencies and global bandwidth usage. Loop unrolling is effective in improving performance for some applications. Careful organization of threads and data reduces or eliminates con(cid:3)icts in shared memory and caches, most notably in the MRI applications.

The applications in Table 3 with the highest performance gains, namely TPACF, RPES, MRI-Q, MRI-FHD, and CP, have low global access ratios and spend most of their execution time per- forming computation or accessing low-latency memories. They also generate enough threads to hide potential stalls on long-latency operations and maintain high pipelined (cid:3)oating point throughput.

The MRI applications achieve particularly high performance and require additional explanation. One major reason for their performance is that a substantial number of executed operations are trigonometry functions; the SFUs execute these much faster than even CPU fast math libraries. This accounts for approximately 30% of the speedup. We also spent signi(cid:2)cant effort improving the CPU versions (approximately 4.3X over the original code) to ensure that these comparisons were reasonable. The opposite effect, where the native instruction set must emulate functionality, exists in RC-5: the GeForce 8800 lacks a modulus-shift operation. Performance of the code if a native modulus-shift were available is estimated to be several times higher.

LBM, FEM, and FDTD are notable for being time-sliced sim- ulators, where a portion of the simulation area is processed per thread. For each time step, updates must propagate through the sys- tem, requiring global synchronization. Since there is no ef(cid:2)cient means to share data and perform barrier synchronization across thread blocks, a kernel is invoked for each time step to ensure that all data writes to global memory in the previous time step are re- liably visible to the next time step. This places high demand on global memory bandwidth since the kernel must fetch from and store back the entire system to global memory after performing only a small amount of computation. PNS does not have this issue because a separate simulation is performed per thread. One pos- sible solution to this issue is to relax the global synchronization requirement by changing application algorithms.

Memory-related bottlenecks appeared in LBM, FEM, PNS, SAXPY, and FDTD, all of which have high memory-to-compute ratios. This causes bottlenecks in two ways. First, LBM and PNS are limited in the number of threads that can be run due to memory capacity constraints: shared memory for the former, global memory for the latter. Second, FEM, SAXPY, and FDTD saturate memory bandwidth. Even though the latter two have the highest number of simultaneously active threads of the suite, this does not help the large memory to compute ratio, which is the primary performance bottleneck.

5.2 Optimizations

In this section we discuss some of the optimizations that have sig- ni(cid:2)cant effects on performance and the corresponding applications. In general, shared memory is useful for reducing redundant loads and thus pressure on memory bandwidth. Its use is straightforward when there are either no shared values between threads (each thread effectively has its own private space) or when neighboring threads share data in a simple pattern, similar to matrix multiplication. Care must be taken so that threads in the same warp access different banks of the shared memory. In addition, more complex applica- tions often use more sophisticated data structures.

One use of shared memory is buffering to improve the access pattern of global memory. As stated previously, memory band-

Table 2. Application Suite

Application Description

H.264

LBM

RC5-72

FEM RPES

PNS SAXPY

TPACF

FDTD

MRI-Q

MRI- FHD CP

A modi(cid:2)ed version of the 464.h264ref benchmark from SPEC CPU2006. This is an H.264 (MPEG-4 AVC) video encoder. A serial dependence between motion estimation of macroblocks in a video frame is removed to enable parallel execution of the motion estimation code. Although this modi(cid:2)cation changes the output of the program, it is allowed within the H.264 standard. A modi(cid:2)ed version of the 470.lbm benchmark from SPEC CPU2006. This uses the Lattice-Boltzman Method for simulating 3D (cid:3)uid dynamics. The program has been changed to use single-precision (cid:3)oating point and print fewer status reports. This application accelerates distributed.net’s RSA RC5-72 bit challenge, which performs brute-force encryption key generation and matching. Finite Element Modeling. Simulation of dynamic behavior of 3D graded materials. Rys Polynomial Equation Solver. Calculates 2-electron repulsion integrals, which are a sub-problem of molecular dynamics. Petri Net Simulation. Simulation of a mathematical representation of a distributed system. Single-precision (cid:3)oating-point implementation of saxpy from High-Performance LINPACK, used as part of a Gaussian elimination routine. Implementation of Two Point Angular Correlation Function, used to (cid:2)nd the probability of (cid:2)nding an astronomical body at a given angular distance from another astronomical body. Finite-Difference Time-Domain. 2D electromagnetic wave propagation simulation in an arbitrary, user- de(cid:2)ned medium. Computation of a matrix Q, representing the scanner con(cid:2)guration, used in a 3D magnetic resonance image reconstruction algorithm in non-Cartesian space. Computation of an image-speci(cid:2)c matrix F Hd, used in a 3D magnetic resonance image reconstruction algorithm in non-Cartesian space. Computation of electric potential in a volume containing point charges. Based on direct Coulomb summation, as described in [24].

Source Lines

34811

1481

1979

1874 1104

322 952

536

1365

490

343

409

Kernel Lines

194

285

218

146 281

160 31

98

93

33

39

47

CPU Execution Parallelized 35%

> 99%

> 99%

99% 99%

> 99% > 99%

96%

16.4%

> 99%

> 99%

> 99%

Table 3. Application Implementation Performance For Typical, Long-Running Execution Pro(cid:2)les GPU Exec %

Application Max Simul-

Registers per Thread

Shared Mem per Thread (B) 8.1 55.1

Global Memory to Computation Cycles Ratio 0.276 0.006

CPU- GPU Transfer % 4% 4.5%

Architectural Bottleneck(s)

taneously Active Threads 12288 3936

Mat Mul H.264

16.2% 2.6%

Instruction issue Register (cid:2)le capacity and cache latencies Shared memory capacity Instruction issue Global memory bandwidth Instruction issue Global memory capacity Global memory bandwidth Shared memory capacity Global memory bandwidth Instruction issue Instruction issue Instruction issue

9 30

84.2 0.3 61 24.8 9.9 0.3 52.2 8.1 20.1 20.1 0.4

32 42 18 23 32 7 24 11 11 12 20

3200 3072 4096 4096 2048 12288 4096 12288 8192 8192 6144

LBM RC5-72 FEM RPES PNS SAXPY TPACF FDTD MRI-Q MRI-FHD CP

0.415 ’0 1.135 0.01 0.241 0.375 0.0002 0.516 0.008 0.006 0.0005

0.4% 98.3% 0.5% 64.3% (cid:28) 1% 91.4% 1% 37.5% (cid:28) 1% 98% 4.5% 88% (cid:28) 1% 34.3% 0.9% 1.8% > 99% (cid:28) 1% 99% > 99% (cid:28) 1%

1%

Kernel Speedup on GPU

7.0X 20.2X

12.5X 17.1X 11.0X 210X 24.0X 19.4X 60.2X 10.5X 457X 316X 102X

Application Speedup

2.0X 1.47X

12.3X 11.0X 10.1X 79.4X 23.7X 11.8X 21.6X 1.16X 431X 263X 102X

width is easily saturated when requests are not performed at 16- word granularities. LBM, FEM, FDTD, and other lattice compu- tations use arrays of small structures in global memory. Threads simultaneously read or write a given (cid:2)eld of multiple elements, but these (cid:2)elds are not contiguous in memory. Each non-contiguous access is a separate DRAM access request, overwhelming the de- vice’s memory bandwidth. In LBM we alleviated the problem us- ing contiguous accesses to prefetch the arrays into shared memory. Figure 5 illustrates the access patterns before and after the opti- mization. Before computation, threads cooperatively load blocks of memory into shared memory, as shown in Figure 5(b). They then synchronize, after which each thread operates on its own data. The buffering optimization may also be possible with FDTD if a sub- stantial amount of data reorganization is performed, but FEM uses

an irregular mesh data structure that has few contiguous accesses even with data reorganization.

On-chip caches are useful in several applications; we focus on two here. For the MRI applications, we placed data in constant memory, which reduced average access time [25]. We also per- formed a loop interchange to make all threads in a warp simultae- nously access the same value in the table to remove con(cid:3)icts. Con- stant memory is generally intended for small lookup tables, but any data that is read-only and has the same location simultaneously read by all threads is appropriate for it. Our implementation of H.264 uses texture memory for part of the input data, since the data use has 2D locality and the hardware provides boundary-value calcu- lation support that would otherwise need to be calculated in soft- ware. However, a lack of registers restricts the number of threads

(cid:16)(cid:20)(cid:21)

(cid:16)(cid:20)(cid:22)

(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)

(cid:1)(cid:1)

(cid:1)(cid:1)

(cid:16)(cid:20)(cid:21)

(cid:16)(cid:20)(cid:22)

(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:8)

(cid:1)(cid:1)

(cid:1)(cid:1)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:7)(cid:9)(cid:2)(cid:10)(cid:4)(cid:10)(cid:11)(cid:2)(cid:12)(cid:4)(cid:2)(cid:12)(cid:12)(cid:6)(cid:13)(cid:14)(cid:14)(cid:4)(cid:15)(cid:2)(cid:16)(cid:16)(cid:13)(cid:6)(cid:9)

(cid:16)(cid:20)(cid:21)(cid:4)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)(cid:24)(cid:24)(cid:23)(cid:24)(cid:4)(cid:16)(cid:20)(cid:22)(cid:25)

(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)

(cid:1)(cid:1)

(cid:1)

(cid:1)(cid:1)

(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:8)

(cid:1)(cid:1)

(cid:16)(cid:20)(cid:21)(cid:4)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)(cid:24)(cid:24)(cid:23)(cid:24)(cid:4)(cid:16)(cid:20)(cid:22)(cid:25) (cid:1)

(cid:1)(cid:1)

(cid:1)(cid:1)

(cid:1)(cid:17)(cid:3)(cid:4)(cid:5)(cid:15)(cid:16)(cid:7)(cid:18)(cid:7)(cid:19)(cid:13)(cid:12)(cid:4)(cid:10)(cid:11)(cid:2)(cid:12)(cid:4)(cid:2)(cid:12)(cid:12)(cid:6)(cid:13)(cid:14)(cid:14)(cid:4)(cid:15)(cid:2)(cid:16)(cid:16)(cid:13)(cid:6)(cid:9)

Figure 5. LBM Global Load Access Patterns

that could be scheduled, exposing the latency of texture memory. Even so, kernel performance improves by 2.8X over global-only access by the use of texture memory.

Loop unrolling and other (cid:147)classic(cid:148) compiler optimizations can have unexpected results, but in general local optimizations on the most frequently executed parts of the code has bene(cid:2)cial effects. Bene(cid:2)t is due to the reduction in the number of operations or strength reduction of individual operations such as integer mul- tiply, thus increasing overall computational ef(cid:2)ciency. In H.264, complete unrolling of the innermost loop obtains signi(cid:2)cant perfor- mance increase, as did register tiling [10] for the next two higher- level loops.

The common case of compiler optimizations having negative effects is when they increase the number of registers per thread as a side effect, forcing the GeForce 8800 to schedule fewer thread blocks per SM and thus degrading performance. The cases where this is most often seen are common subexpression elimination and redundant load elimination, the latter often storing thread and block coordinates in registers. Even relatively simple instruction schedul- ing can change the live ranges of variables and increase the reg- ister usage. Register pressure-sensitive code scheduling algorithms and optimization strategies have been investigated in the context of instruction-level parallelism compilers. Additional research is needed to apply these strategies to massively threaded environ- ments like CUDA. We will address the control of register usage in future work.

6. Conclusion and Future Work We present a performance evaluation of the GeForce 8800 GTX architecture using CUDA. Although its primary market is graph- ics processing, this GPU is also capable of impressive performance on a set of disparate non-graphics applications. This work presents general principles for optimizing applications for this type of archi- tecture, namely having ef(cid:2)cient code, utilizing many threads to hide latency, and using local memories to alleviate pressure on global memory bandwidth. We also present an application suite that has been ported to this architecture, showing that application kernels that have low global memory access after optimization have sub- stantial speedup over CPU execution if they are not limited by local resource availability. We are currently performing research on automated optimiza- tions for this architecture. Although many of the optimizations are classical ones, the effects they have on this architecture can be dif- ferent from the effects on traditional superscalar processors. It is also possible to get stuck in local maximums of performance when attempting to follow a particular optimization strategy. These max- imums may be signi(cid:2)cantly lower than the peak achievable per- formance. Better tools and compilers that allow programmers to specify the types of reorganizations desired and automatically ex- periment with their performance effects would greatly reduce the

optimization effort. In addition, two updated versions of CUDA have been released between the original and (cid:2)nal submission of this paper, changing resource usages and optimal con(cid:2)gurations of many applications. We are exploring methods to preserve or en- hance performance of applications when shifts in the underlying architecture or runtime occur.

Acknowledgments The authors thank John Nickolls, Mark Harris, and Michael Cox at NVIDIA for their advice on drafts of the paper. We also thank Sanjay Patel, Kurt Akeley, Pradeep Dubey, John Kelm, Hillery Hunter, and the anonymous reviewers for their feedback. We thank the Spring 2007 class of ECE498AL at the University of Illinois at Urbana-Champaign for their work on initial versions of many of the applications presented in this paper. We also thank the other members of the IMPACT research group for their support.

Sam Stone is supported under a National Science Foundation Graduate Research Fellowship. This work was supported by the Gi- gascale Systems Research Center, funded under the Focus Center Research Program, a Semiconductor Research Corporation pro- gram. Experiments were made possible by generous hardware loans from NVIDIA and NSF CNS grant 05-51665. Any opin- ions, (cid:2)ndings, conclusions, or recommendations expressed in this publication are those of the authors and do not necessarily re(cid:3)ect the views of the NSF.

References

[1] AMD Stream Processor. http://ati.amd.com/products/ streamproces-

sor/index.html.

[2] CUDA benchmark suite.

http://www.crhc.uiuc.edu/impact/cudabench.html.

[3] NVIDIA CUDA. http://developer.nvidia.com/object/cuda.html. [4] The PeakStream platform: High productivity software development

for multi-core processors. Technical report, 2006.

[5] ECE 498AL1: Programming massively parallel processors, Fall 2007.

http://courses.ece.uiuc.edu/ece498/al1/.

[6] J. C. Adams, W. S. Brainerd, J. T. Martin, B. T. Smith, and J. L. Wagener. Fortran 90 handbook: complete ANSI/ISO reference. Intertext Publications, Inc.,/McGraw-Hill, Inc., 1992.

[7] R. Allen and K. Kennedy. Automatic translation of Fortran programs to vector form. ACM Transactions on Programming Langugages and Systems, 9(4):491(cid:150)542, 1987.

[8] M. J. Atallah, editor. Algorithms and Theory of Computation

Handbook. CRC Press LLC, 1998.

[9] I. Buck. Brook Speciﬁcation v0.2, October 2003. [10] D. Callahan, S. Carr, and K. Kennedy. Improving register allocation

for subscripted variables. ACM SIGPLAN Notices, 9(4):328(cid:150)342, 2004.

[11] K. Fatahalian, J. Sugerman, and P. Hanrahan. Understanding the ef(cid:2)ciency of GPU algorithms for matrix-matrix multiplication. In Proceedings of the ACM SIGGRAPH/EUROGRAPHICS Conference on Graphics Hardware, pages 133(cid:150)137, 2004.

[12] N. K. Govindaraju, S. Larsen, J. Gray, and D. Manocha. A

memory model for scienti(cid:2)c algorithms on graphics processors. In Proceedings of the 2006 ACM/IEEE Conference on Supercomputing, number 89, 2006.

[13] K. Kennedy and J. R. Allen. Optimizing compilers for modern architectures: a dependence-based approach. Morgan Kaufmann Publishers Inc., 2002.

[14] M. S. Lam, E. E. Rothberg, and M. E. Wolf. The cache performance and optimizations of blocked algorithms. In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, pages 63(cid:150)74, April 1991.

IEEE Parallel & Distributed Technology: Systems & Technology, 1(1):25(cid:150)42, 1993. [16] W. R. Mark, R. S. Glanville, K. Akeley, and M. J. Kilgard. Cg: a system for programming graphics hardware in a C-like language. In ACM SIGGRAPH 2003 Papers, pages 896(cid:150)907, 2003.

[15] D. B. Loveman. High Performance Fortran.

[17] M. D. McCool, K. Wadleigh, B. Henderson, and H.-Y. Lin.

Performance evaluation of GPUs using the RapidMind development platform. Supercomputing, 2006.

In Proceedings of the 2006 ACM/IEEE Conference on

[18] J. Nickolls and I. Buck. NVIDIA CUDA software and GPU parallel

computing architecture. Microprocessor Forum, May 2007.

[19] OpenMP Architecture Review Board. OpenMP application program

interface, May 2005.

[20] J. Owens. Streaming architectures and technology trends. GPU Gems

2, pages 457(cid:150)470, March 2005.

[21] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kr(cid:127)uger, A. E. Lefohn, and T. J. Purcell. A survey of general-purpose computation on graphics hardware. Computer Graphics Forum, 26(1):80(cid:150)113, 2007.

[22] S. Ryoo, C. I. Rodrigues, S. S. Stone, S. S. Baghsorkhi, S.-Z. Ueng, and W. W. Hwu. Program optimization study on a 128-core GPU. In The First Workshop on General Purpose Processing on Graphics Processing Units, October 2007.

[23] M. Snir, S. W. Otto, D. W. Walker, J. Dongarra, and S. Huss- Lederman. MPI: The Complete Reference. MIT Press, 1995.

[24] J. E. Stone, J. C. Phillips, P. L. Freddolino, D. J. Hardy, L. G. Trabuco, and K. Schulten. Accelerating molecular modeling applications with graphics processors. Journal of Computational Chemistry, 28(16):2618(cid:150)2640, December 2007.

[25] S. S. Stone, H. Yi, W. W. Hwu, J. P. Haldar, B. P. Sutton, and Z.-P. Liang. How GPUs can improve the quality of magnetic resonance imaging. In The First Workshop on General Purpose Processing on Graphics Processing Units, October 2007.

[26] D.Tarditi, S. Puri, and J.Oglesby. Accelerator: Using data parallelism to program GPUs for general-purpose uses. In Proceedingsofthe 12th International Conference on Architectural Support for Programming Languages and Operating Systems, pages 325(cid:150)335, 2006.

[27] P. H. Wang, J. D. Collins, G. N. Chinya, H. Jiang, X. Tian, M. Girkar, N. Y. Yang, G.-Y. Lueh, and H. Wang. EXOCHI: architecture and programming environment for a heterogeneous multi-core multithreaded system. In Proceedings of the 2007 ACM SIGPLAN Conference on Programming Language Design and Implementation, pages 156(cid:150)166, 2007.

[28] M. J. Wolfe. Optimizing Supercompilers for Supercomputers. MIT

Press, 1990.