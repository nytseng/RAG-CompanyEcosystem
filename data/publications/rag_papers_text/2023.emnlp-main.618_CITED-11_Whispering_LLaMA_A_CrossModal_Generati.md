WhisperingLLaMA:ACross-ModalGenerativeErrorCorrectionFrameworkforSpeechRecognitionSrijithRadhakrishnan1,2,6,Chao-HanHuckYang1,3,4,SumeerAhmadKhan1,6,RohitKumar1,NarsisA.Kiani5,DavidGomez-Cabrero1,JesperN.Tegner1,51KingAbdullahUniversityofScienceandTechnology2ManipalInstituteofTechnology3GeorgiaInstituteofTechnology4NVIDIAResearch5KarolinskaInstitute6SDAIA-KAUSTCenterofExcellenceinDataScienceandArtificialIntelligencesrijithrkr@gmail.com;huckiyang@gatech.eduAbstractWeintroduceanewcross-modalfusiontech-niquedesignedforgenerativeerrorcorrec-tioninautomaticspeechrecognition(ASR).Ourmethodologyleveragesbothacousticin-formationandexternallinguisticrepresenta-tionstogenerateaccuratespeechtranscrip-tioncontexts.Thismarksasteptowardsafreshparadigmingenerativeerrorcorrectionwithintherealmofn-besthypotheses.Unliketheexistingranking-basedrescoringmethods,ourapproachadeptlyusesdistinctinitializa-tiontechniquesandparameter-efficientalgo-rithmstoboostASRperformancederivedfrompre-trainedspeechandtextmodels.ThroughevaluationacrossdiverseASRdatasets,weassessourfusiontechnique,demonstratinga37.66%improvementinworderrorrate(WER)relativeperformancecomparedtothen-bestOracle.Toencouragefutureresearch,wehavemadeourcodeandpre-trainedmod-elsopensourceathttps://github.com/Srijith-rkr/Whispering-LLaMA.1IntroductionEnd-to-end(E2E)trainedspeechmodelshavedemonstratedstate-of-the-artperformanceonAu-tomaticspeechrecognition(ASR)tasks.Severalmethods(Xiaetal.,2017;Guoetal.,2019;Huetal.,2021b;Yangetal.,2021a;Salazaretal.,2020)havewidelyadoptedatwo-passrescoringparadigmtoleverageuponlanguagemodelstofur-therenhancethecapabilitiesofthesemodels.Inthetwo-passparadigm,thefirstpassASRsystem‚Äúgen-erates‚Äùn-besthypothesesusinganE2Eacousticmodel,whilethesecondpass‚Äúre-ranks‚Äùthesehy-pothesesbyincorporatingalanguagemodel(LM).Thistwo-passrerankingapproachhasseveralnotableadvantagesoversingle-passEnd-to-End(E2E)ASRsystems(Amodeietal.,2016;Chanetal.,2016).Firstly,thesubsequentlargelanguagemodeloftencapturesamorecomprehensiveun-derstanding(Stookeetal.,2023;TurandDeMori,2011)oflanguagestructuresbeyondtheknowledgeoftranscribedaudiopresentintheASRmodel‚Äôspre-trainingdata,therebyimprovingperformanceonunseenwords.Furthermore,adaptingthetwo-passparadigmtoaccommodatedomainshifts(Lietal.,2023;Liuetal.,2021;Yuetal.,2023)ismucheasierasonlythelanguagemodelneedstobefine-tunedonthenewdataset.Thisalleviatestheneedforaspokentranscriptioncorpus,whichcanbeparticularlybeneficialforunder-resourcedorendangeredspokenlanguages.Therecentemergenceofconversationalabilitiesinlargelanguagemodels,suchasChatGPT(Ope-nAI,2023a)andGPT-4(OpenAI,2023b),hasfurthersparkedinterestinleveragingtherepre-sentationalpoweroflargepre-trainedmodelsformorecomplextasksinvolvingdiversedatamodali-ties(Yangetal.,2021b;Changetal.,2023).More-over,thisnewresearchdirectionalsointroducesasetofuniquechallengesrelatedtoconsideringinformationfromotherinputmodalities,suchasacousticandvisualconditions(Pengetal.,2023;Zhangetal.,2023),inwhichcouldenrichusingcontextbeyondtext-onlyinput.Recognizingspeechsignalsisataskthatneces-sitatesbothacousticinformation(Huetal.,2021a;Hungetal.,2023)(e.g.,speakingenvironments)andlinguisticinformation(Mengetal.,2023;Chenetal.,2023b,c)(e.g.,contextanddomains).Ef-ficientlyamalgamatingorintegratingrepresenta-tionlearningfromacousticmodelingintolanguagemodellingtobolsteritsperformancerepresentsanotablyintricateresearchdomainthatwarrantsfur-therexploration.Inthispaper,wepresentatoken-levelfusionframework,mergingtwofoundation(large-scalepre-trained)modelsintoarecognitionerrorcorrectionparadigm,withtheobjectiveofenhancingtheperformanceofASRsystems.

10007 Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 10007‚Äì10016 December 6-10, 2023 ¬©2023 Association for Computational Linguistics

Œª

W

Learnable Matrix

QLLaMA

KLLaMA

VLLaMA

KWhisper

VWhisper

Padding

Mdown

Mup

Œª

L

AiW (AdapterWhisper)

Layer Output

Frozen

Trainable

Self-AttentionLLaMA

SAL

Self-AttentionWhisper

SAW

LanguageEmbeddings (HL)

Audio Features (HA)

SiLU

MùúÉ

üî•

Figure1:IllustrationofproposedgenerativeASRerrorcorrectionwithatrainabletoken(MŒ∏)andfusionmechanisminsideaself-attentionlayerdescribedinSection3.2.Adetailedmodel-wiseillustrationisdiscussedinFig2.2RelatedWorkonASRPost-processingTransformer-basedlanguagemodels(Shinetal.,2019;Salazaretal.,2020)approachthetwo-passparadigmbyutilizingthesummationofnegativelog-likelihoodsofindividualtokensfromthelan-guagemodeltore-scorethen-bestoutput.Recentworksondeliberationmethod(Huetal.,2020;Prabhavalkaretal.,2018)andaudio-attentionbasedrescoring(Futamietal.,2021;GandheandRastrow,2020;Tanakaetal.,2021)inimprov-ingASR-LMrescoringwiththeincorporationofacousticfeatures.Recentworksondecoderprompt-ing(Yangetal.,2023a)andencoder-decoderbasederrorcorrection(Chenetal.,2023a;Maetal.,2023)havedemonstratedbenefitsinusinganexternallan-guagemodelforreducingthetranscriptionerrorrate.Meanwhile,howtoinjectorfuserepresen-tationsfromalargeacousticmodelintoanotherlanguagemodelremainsunderinvestigation.3MethodWediscussthemodelarchitectureandtheintuitionbehindtheproposedfeaturecombinationinSec-tion3.1.Thecross-modalfusionmechanismandweightinitializationareexplainedinSection3.2andSection3.3,respectively.3.1GenerativeErrorCorrectionforASROurapproachcombinestwopre-trainedmodels,Whisper(Radfordetal.,2022)andLLaMA(Tou-vronetal.,2023),tofacilitategenerativeerrorcor-rection(Yangetal.,2023a;Chenetal.,2023a).Firstly,weemployWhisper,amulti-taskencoder-decoder-basedtransformer(Vaswanietal.,2017)speechmodeltrainedon680,000hoursofmulti-lingualdata,toencodeaudiorepresentationsandgeneratetranscriptsofn-besthypotheses.Sec-ondly,weutilizeLLaMA,adecoder-basedlargelanguagetransformermodel,togenerateerror-correctedtranscriptsbyutilizingthen-besthy-pothesesviaprompt(illustratedinAppendix,Fig5)andaudiorepresentationsviaourproposedframe-workasinput.WhisperutilizestheencoderofaTransformermodeltoderivefeaturesfromaudioinput,whicharethenfedintothedecoderthroughmulti-headedcross-attention,enablingauto-regressivetexttokenprediction(Wangetal.,2023;Irieetal.,2022).Theencodedfeaturesprovideinformationfromaudioinputviacross-attention,whilethedecoder‚Äôsself-attentionattendsprevioustokensusingakey-valuecachingmechanism.WefusetheaudiofeaturesandtheWhisperlin-earlayersthatgeneratethekeyandvaluepairsinthedecoder‚Äôscross-attentionmechanismtotheLLaMAmodeltoinjectaudioinformation.Theinherentself-attentionmodulesinLLaMAcom-binedwiththeaddedcross-attentionmodulemakeitanalogoustotheWhisperdecoder.AnoverviewoftheproposedmethodispresentedinAppendix,Fig.2.3.2Cross-ModalFusionMechanismWeintroduceourmechanisminFig1.Toefficientfine-tunelargemodels,weincorporatetworesid-ualadapter(Houlsbyetal.,2019;Radhakrishnanetal.,2023;Chenetal.,2023d;Yangetal.,2023b)modules(AiLandAiW)aftertheself-attentionmod-ules(SAiL)ofthefrozenLLaMAmodelateachlayer.ThefirstvariableAiLrepresentstheadapterinlayeriusedtofine-tunetheLLaMAmodelusingascaleddotproductattentionmechanism.Thesec-ondvariableAiWreferstoanotheradapterinlayeriusedtofuseWhisperfeatureswiththeLLaMAmodelbyfollowinganautoencodermechanism.IneachAiL,weincorporatealearnablematrix

10008

MiŒ∏‚ààRNŒ∏√óNL.NŒ∏denotesthedimensionalityoftheadapterembeddings,whileNLindicatesthedimensionalityofLLaMAembeddings.Thelanguageembeddingfeatureextractedfromthepre-trainedLLMisrepresentedbyHiLforeachlayer.WerepurposethefrozenLLaMAlinearlayersKillamaandLillamafromtheLLaMAself-attentionSAiLtotransformMiŒ∏intokeyandvaluepairs,thusreducingthenumberoftrainableparameters.WealsoreusethequerytensorfromthefrozenLLaMAself-attentionmoduleSAiLtocomputeAiL,asshownbelow;SrepresentstheSoftmax:S Qillama(HiL)¬∑Killama(cid:0)MiŒ∏(cid:1)T

‚àö

dk!Villama(cid:0)MiŒ∏(cid:1)(1)Tointegratetheaudiorepresentationsandkey-valuetensorsfromtheWhisperdecodercross-attentionmoduleintotheLLaMAmodel,wein-troducetwoadditionallinearfrozentransforma-tions(KiwhisperandViwhisper)ateachlayeroftheLLaMAmodel.Thesetransformationsareinitial-izedwiththerespectiveweightsfromthecross-attentionmoduleoftheWhisperdecoder.Byap-plyingtheaudiorepresentationstotheseadditionallineartransformations,wegeneratethekey-valuepairsthatmirrortheonesproducedbyWhisper.WethenutilizethesecondadaptermoduleAiW,toaddtrainablecomponentstolearncross-modalrep-resentation.WeapplyalearnableprojectionmatrixMidown‚ààRNW√óNW

rtodownprojecttheobtainedkeyandvaluepairs.WhereNWdenotesthesizeoftheWhisperencodedaudiorepresentations(x).WethenapplytheSiLUactivationfunction(Elfwingetal.,2018)followedbyalearnableup-projectionMiup‚ààRNW

r√óNW,tocomputetrainableoutput:AiW(x)‚ÜêSiLU(cid:0)x¬∑Midown(cid:1)Miup.(2)Usingthissetup,wetransformthekey-valuepairateachlayertomergethehiddenrepresentation(HA)fromtheoutputoftheWhisperfrozenpre-trainedencoderwithdecoderfromLLaMA:ÀÜKiwhisper‚ÜêAiW(Kiwhisper(HA));(3)ÀÜViwhisper‚ÜêAiW(cid:0)Viwhisper(HA(cid:1)).(4)OnceweobtainthecorrespondingWhisperkeyandvaluepairs,weapplythepaddingmechanismdescribedin3.3topreservethelatentstructureoftheWhisperKeyandValueembeddingsandTable1:Datasetsamplestatisticsareprovidedwithaliasnames.TheScience&TechnologycategoryofGigaSpeech(Chenetal.,2021)isdividedintotwosub-sets:GSSS(small)andGSSM(medium),toevaluateperformancedifferenceswithrespecttodatasize.

Dataset

Train

Test

ATIS(Hemphilletal.,1990)

4978

893

GigaSpeech:Entertainment(GSE)

4709

1000

People&Blogs(GSP)

6802

1000

Science&Technology(GSSS)

6908

1000

Science&Technology(GSSM)

10323

1000

adjusttheshapeofÀÜKiwhisperandÀÜViwhispertoen-ablecomputationofMultiHeadAttention(MHA)withQillama(HL)fromthefrozenLLaMAmodelasbeforetoobtainitsadaptableself-attentionhead(SAiW)as:SÔ£´Ô£¨Ô£≠Qillama(HiL)¬∑(cid:16)ÀÜKiwhisper(cid:17)T

‚àö

dkÔ£∂Ô£∑Ô£∏ÀÜViwhisper(5)Then,weutilizeagatedfusionmechanism,Whispering-LLaMA(WL),tofuseallthemod-ulestogetherasshownbelow:SAiWL‚ÜêSAiL+ŒªL¬∑AiL+ŒªW¬∑SAiW,(6)whereŒªLandŒªWarelearnablescalars.3.3WeightInitializationThelatentdimensionsoftheWhisperandLLaMAmodelsaredifferent,makingitnecessarytore-shapetheWhispertensorstomatchtheshapeoftheLLaMAmodelwhilepreservingthelatentstructureandinformationinherentintheWhis-permodel.Tensorsareshapedintheformatof[B,NH,T,HS],whichdenotestheBatchsize,Numberofheads,contextlengthandHeadSize,respectively.Thelasttwodimensionsundergotransformationduringtheattentionmechanism.HenceinordertopreservetheWhisperlatentstructure,Weinitializeamatrixofzerosofshape‚ààRNHllama√óTwhisper√óHSllamaandfilltheprincipaldiagonalofthelasttwodimensionswithones.WethenplaceÀÜKiandÀÜVionthetopleftcornerofthepaddingtemplate.Wefurtherinitializetheprojec-tionmatricesMidown,MiuponthesecondadaptermoduleAiWasidentitymatrices.Theproposedframeworkencounterssignificantlossesandfailstoconvergeunlessthisinitializationstrategyisfol-lowedtopreserveWhisper‚Äôslatentrepresentations.

10009

Table2:TheexperimentalresultsarepresentedintermsofWERwithouttextnormalization.Theperformanceofourproposedframeworkisreportedinrows2‚àí4.Oraclereferstothecandidateamongthen-besthypothesiswiththelowestworderrorratecomparedtothegroundtruth.Rows5‚àí9representdifferentablationexperimentsonthebest-performingmodel,WLM.TheWERRismeasuredrelativetotheoracleperformanceasshowninB.2

#

Method

#Para.

ATIS

GSEGSPGSSSGSSM

WERAvg(‚Üì)

WERR(‚Üë)

1

Oracle(1st-pass)



13.76

28.2222.8423.9319.5

21.64



2

WLL

26.40M

2.04

21.7619.2120.5511.6

15.03

30.52

3

WLM

7.97M

1.77

21.6116.2018.029.82

13.48

37.66

4

WLS

4.89M

1.89

22.2417.2319.15710.185

14.144

34.62

5

WLMw/omasking

4.89M

3.94

27.5618.1021.7112.79

20.04

22.25

6

WLMw/oHA

4.89M

253.20

123.19203.44376.81256.44

242.61

1020.68

7

WLMw/oinit.

4.89M

405.83

500.58414.34461.63390.64

434.60

1907.45

8

WLMw/oSAW

1.22M

1.66

24.9918.73420.7310.86

15.39

28.83

9

Big-scaleAdapter

4.91M

1.45

23.6516.5919.9310.62

14.45

33.21

4ExperimentalSetup4.1ModelsForourexperiments,weutilizetheLLaMA-7Bmodelarchitecture.Asweinstructthelanguagemodelwiththegeneratedhypotheses(asexplainedinSection4.3.1)toperformgenerativeerrorcor-rection,weinitializeourmodelweightswithAl-paca(Taorietal.,2023),amodelfine-tunedfromLLaMA-7B,utilizing52,000instruction-followingdemonstrationstoenableinstructionfollowingabil-ities.Toextractaudiorepresentationsfrominputaudioclips,weemployWhisper-LargeV2,amodelwith1.55Bparameterstrainedon620,000hoursofaudiodata.Additionally,weemployWhisper-Tiny,amodelwith70Mparameters,forgeneratingourtranscripts,asdescribedinthesubsequentsec-tion4.2.WenameourmodelWhisperingLLaMA(WL)andtrainthreevariantswithourproposedframeworkwithNŒ∏=10andr=8,16,32namedWLL(large),WLM(medium),WLS(small),re-spectively.WedesignWLLwithtwoseparateAWadaptermodulesforkeyandvalue,respectively.WLMandWLSusethesameAWadapterinsec-tion3.2toreducetrainableparameters.4.2DatasetWecurateourowntranscriptsbyleveragingtwodatasets:theAirlineTravelInformationSystem(Hemphilletal.,1990)(ATIS)andGi-gaSpeech(Chenetal.,2021).ATISconsistsofaudiorecordingsofindividualsqueryingflightin-formation.GigaSpeech,containsaudiofromaudio-books,podcastsandYouTubevideosondiversetopics.ATISrepresentsasemanticallycorrect,domain-specificdataset,whileGigaSpeechrepre-sentsamorenoisy,real-worldsettinginoureval-uation.Weselectdomain-specificsubsetsinGi-gaSpeechandfocusonthreespecificcategories:Entertainment,PeopleandBlogs,andScienceandTechnology.Toexploreperformancevariationswithrespecttothenumberofdatapoints,wefur-therdividetheScienceandTechnologycategoryintotwosubsets.Table1providesdetailedinfor-mationonthenumberoftrainingpointsperdataset.WechoseWhisper-Tinytogeneratethen-besthy-pothesisbaselinetoestablisharobustevaluationen-vironmentthatalignsmorecloselywithreal-worldsettingsdealingwithsub-optimalhypotheses.ByemployingWhisper-Tiny,wemimicaweakacous-ticmodelwithlower-qualityhypotheses.FeedingLMswithbetter-qualityhypothesesfromWhisper-LargewouldmakethegenerativeerrorcorrectiontasklesschallengingforLMadaptationanddoesnotexplorethemodel‚Äôsperformanceunderpracti-calsettingswhereourmethodisintendedtobeem-ployed.However,weemphasizethatourmethodremainseffectivewhenstartingwithaWhisper-LargehypothesisinAppendixE.Foreachaudioclip,wegenerate200hypothesesusingatop-kvalueof200andarandomlyselectedtemperaturebetweentherangeof[0.7,0.8].Sub-sequently,wefilteroutredundantsentencesandselectthetop15withthehighestlogprobability.4.3TrainingPipelineTheinputtoourmodelconsistsoftheencodedaudiorepresentationsextractedfromtheWhisper-Largemodel,accompaniedbythe15-besttran-scriptsgeneratedbyWhisper-Tiny.WeemploytheprompttemplateusedbytheAlpacamodelasshowninAppendixFig5.WeutilizetheAdamoptimizer(KingmaandBa,2014)andexperimentwithlearningratesof1√ó10‚àí2,1√ó10‚àí3,and

10010

5√ó10‚àí4,selectingtheoptimalvalue.Themodelistrainedfor25epochs,employingearlystoppingtopreventoverfitting.TrainingisconductedontwoNvidiaA100GPUstoleverageefficientparallelprocessing.Aneffectivebatchsizeof32isused,andaweightdecayof1√ó10‚àí2isapplied.4.3.1LLMPromptingExamplesforASRWeemploytheAlpaca(Taorietal.,2023)prompttemplate,asillustratedinFig.5oftheAppendix,togeneratethen-besthypotheses.ThistemplatefeaturesaninstructionalsegmentdesignatedbytheInstructiontag,whichoffersguidancetothemodel.EssentialcontextualdatarequiredbythemodelishousedundertheInputtag.ThepromptconcludeswiththeResponsetag,directingthemodeltoenactthespecifiedinstructionwithinthesuppliedinputcontext.Ratherthanadoptingthere-centadvancesofTask-ActivatingPrompting(Yangetal.,2023a)(TAP),weopttofeedtheLLMwithitstask-specificdata(e.g.,speechrecognitioninourinstance).Ouralternativeapproachfacilitatessecond-passerrorcorrection,mitigatingthelatencyissuesobservedintheextensivecontextwindowsoftheTAP-basedgenerativeASRerrorcorrection.4.4PerformanceStudiesResultsfromourexperimentshavebeenreportedinTable2.TheWLMmodelachievesthebestper-formancewithaword-error-raterelative(WERR)of37.66%,asdefinedinB.2.Acomparisonbe-tweenWLLandWLMindicatesthathavingsepa-rateadaptermodulesforkeyandvaluepairsdoesNOTresultinperformanceimprovements.Furtherdataset-specificanalysesaredetailedinAppendixB.ThemodelsexhibitbetterperformanceontheGigaspeechwithmorein-domaindata.4.5AblationStudiesWeempiricallydiscoverthatmaskingthepromptexceptforthegroundtruthinthecrossentropylossfunctionsignificantlyimprovestheperfor-mance.Weattributethisimprovementtothemodel‚Äôsenhancedcapacitytograspaccuratese-mantics,achievedbyrefrainingfrompenalizingthemodelforerroneoussentencesfoundinthen-besthypotheses.Row5representstheperformanceofWLMwithoutmasking.WefurtherinvestigateiftheproposedframeworkisutilizingtheaudiorepresentationsfromWhisperbysubstitutingthemwithrandomtensorsgeneratedfromanormaldis-tributionastheinput(Row6).Additionally,weexplorethesignificanceofourweightinitializationmechanismbyreplacingitwithrandominitializa-tion(Row7).Bothoftheseablationstudiesvali-dateourintuition,demonstratingthatthemethodutilizesacousticfeatureseffectivelyandhighlighttheimportanceoftheinitializationmechanisminpreservingthelatentstructureoftheacousticem-beddings.Forfurtherinsights,pleaserefertoAp-pendixD.WealsoremovetheWhisperadapter(SAW)moduleforantextfeatureonlybaselineperformanceusingadapters(Row8).Sincethedis-paritybetweenthenumberoftrainableparametersishigh,wetrainanothermodelwithanincreasedadaptercontextdimensionofN‚Ä≤Œ∏=4NŒ∏(Row9).5ConclusionWeproposeanovelframeworktoleveragetheex-ternalknowledgefromLLMtoimprovethetran-scriptionaccuracyofASRsystems.Ourframe-workpresentsaparameter-efficientwaytointegratelargefoundationalSpeechandLanguagemodelstoachievecompetitiveWERRimprovements.Wefurtherconductextensiveablationexperimentstovalidateourintuitionsandopensourceourcodeandpretrained-weightstotheresearchcommunity.6LimitationUsinglargemodelssuchasLLaMAisintuitive,asitprovidesacomprehensivecomprehensionoflanguagestructureowingtoitsinternet-scaledpre-training.However,deployingthesesystemsandconductingresearchwiththeminreal-worldsce-nariosischallengingduetotheircomputationallyintensivenature.Inourapproach,weaimtode-signourframeworktobeparameter-efficientbyre-usingmultiplemodelcomponentswithadaptersformodelfusion.Nonetheless,incorporatingau-diorepresentationsintothetrainingpipelineex-tendsthetrainingdurationby394.76%.Thisun-derscoresthesignificanceofalignmentissues(Yenetal.,2023).Furthermore,ourproposedsolutiondemonstratesaneedforalargervolumeofdatatoachieveoptimalperformancedespitehavingamodestparametercountofonly7.97Mtointegratefoundationalmodels.Duringourexperimentation,weencounteredissuesrelatedtoover-fittingondatasets.Tomitigatethisproblem,wetrainedwithareducedlearningrateandmonitoredtheWordEr-rorRate(WER)performancethroughoutthetrain-ingprocessandselectedthemodelcheckpointwiththebestperformancetoimplementearlystopping.

10011

ReferencesDarioAmodei,SundaramAnanthanarayanan,RishitaAnubhai,JingliangBai,EricBattenberg,CarlCase,JaredCasper,BryanCatanzaro,etal.2016.Deepspeech2:End-to-endspeechrecognitioninenglishandmandarin.InInternationalconferenceonma-chinelearning,pages173‚Äì182.PMLR.WilliamChan,NavdeepJaitly,QuocLe,andOriolVinyals.2016.Listen,attendandspell:Aneuralnetworkforlargevocabularyconversationalspeechrecognition.InProc.ICASSP,pages4960‚Äì4964.IEEE.Kai-WeiChang,Yu-KaiWang,HuaShen,Iu-thingKang,Wei-ChengTseng,Shang-WenLi,andHung-yiLee.2023.Speechpromptv2:Prompttun-ingforspeechclassificationtasks.arXivpreprintarXiv:2303.00733.ChenChen,YuchenHu,Chao-HanHuckYang,SabatoMacroSiniscalchi,Pin-YuChen,andEngSiongChng.2023a.Hyporadise:Anopenbase-lineforgenerativespeechrecognitionwithlargelan-guagemodels.arXivpreprintarXiv:2309.15701.GuoguoChen,ShuzhouChai,GuanboWang,JiayuDu,Wei-QiangZhang,ChaoWeng,DanSu,DanielPovey,JanTrmal,JunboZhang,etal.2021.Gi-gaspeech:Anevolving,multi-domainasrcorpuswith10,000hoursoftranscribedaudio.InProc.Interspeech2021.Zih-ChingChen,Chin-LunFu,Chih-YingLiu,Shang-WenDanielLi,andHung-yiLee.2023b.Exploringefficient-tuningmethodsinself-supervisedspeechmodels.InProc.SLT,pages1120‚Äì1127.IEEE.Zih-ChingChen,Yu-ShunSung,andHung-yiLee.2023c.Chapter:Exploitingconvolutionalneuralnetworkadaptersforself-supervisedspeechmodels.InProcICASSPWorkshop,pages1‚Äì5.IEEE.Zih-ChingChen,Chao-HanHuckYang,BoLi,YuZhang,NanxinChen,etal.2023d.Howtoes-timatemodeltransferabilityofpre-trainedspeechmodels?InProc.Interspeech2023.StefanElfwing,EijiUchibe,andKenjiDoya.2018.Sigmoid-weightedlinearunitsforneuralnetworkfunctionapproximationinreinforcementlearning.NeuralNetworks,107:3‚Äì11.HayatoFutami,HirofumiInaguma,MasatoMimura,ShinsukeSakai,andTatsuyaKawahara.2021.Asrrescoringandconfidenceestimationwithelectra.InProc.ASRU,pages380‚Äì387.IEEE.AnkurGandheandAriyaRastrow.2020.Audio-attentiondiscriminativelanguagemodelforasrrescoring.InProc.ICASSP,pages7944‚Äì7948.IEEE.JinxiGuo,TaraNSainath,andRonJWeiss.2019.Aspellingcorrectionmodelforend-to-endspeechrecognition.InProc.ICASSP,pages5651‚Äì5655.IEEE.CharlesTHemphill,JohnJGodfrey,andGeorgeRDoddington.1990.Theatisspokenlanguagesys-temspilotcorpus.InSpeechandNaturalLanguage:ProceedingsofaWorkshopHeldatHiddenValley,Pennsylvania,June24-27,1990.NeilHoulsby,AndreiGiurgiu,StanislawJastrzebski,BrunaMorrone,QuentinDeLaroussilhe,AndreaGesmundo,MonaAttariyan,andSylvainGelly.2019.Parameter-efficienttransferlearningfornlp.InIn-ternationalConferenceonMachineLearning,pages2790‚Äì2799.PMLR.HuHu,XuesongYang,etal.2021a.Redat:Accent-invariantrepresentationforend-to-endasrbydomainadversarialtrainingwithrelabeling.InProc.ICASSP,pages6408‚Äì6412.IEEE.KeHu,RuomingPang,TaraNSainath,andTrevorStrohman.2021b.Transformerbaseddeliberationfortwo-passspeechrecognition.InProc.SLT,pages68‚Äì74.IEEE.KeHu,TaraNSainath,RuomingPang,andRohitPrab-havalkar.2020.Deliberationmodelbasedtwo-passend-to-endspeechrecognition.InProc.ICASSP,pages7799‚Äì7803.IEEE.Yun-NingHung,Chao-HanHuckYang,Pin-YuChen,andAlexanderLerch.2023.Low-resourcemusicgenreclassificationwithcross-modalneuralmodelreprogramming.InProc.ICASSP,pages1‚Äì5.IEEE.KazukiIrie,R√≥bertCsord√°s,andJ√ºrgenSchmidhu-ber.2022.Thedualformofneuralnetworksre-visited:Connectingtesttimepredictionstotrainingpatternsviaspotlightsofattention.InInternationalConferenceonMachineLearning,pages9639‚Äì9659.PMLR.DiederikPKingmaandJimmyBa.2014.Adam:Amethodforstochasticoptimization.arXivpreprintarXiv:1412.6980.KeLi,JayMahadeokar,JinxiGuo,YangyangShi,GilKeren,OzlemKalinli,MichaelLSeltzer,andDucLe.2023.Improvingfast-slowencoderbasedtransducerwithstreamingdeliberation.InProc.ICASSP,pages1‚Äì5.IEEE.LindaLiu,YileGu,AdityaGourav,AnkurGandhe,ShashankKalmane,DenisFilimonov,AriyaRas-trow,andIvanBulyko.2021.Domain-awareneurallanguagemodelsforspeechrecognition.InProc.ICASSP,pages7373‚Äì7377.IEEE.RaoMa,MarkJFGales,KateKnill,andMengjieQian.2023.N-bestt5:Robustasrerrorcorrectionusingmultipleinputhypothesesandconstraineddecodingspace.Proc.Interspeech.ZhongMeng,WeiranWang,RohitPrabhavalkar,etal.2023.Jeit:Jointend-to-endmodelandinternallan-guagemodeltrainingforspeechrecognition.InProc.ICASSP,pages1‚Äì5.IEEE.

10012

OpenAI.2023a.Chatgpt.https://openai.com/blog/chatgpt/.OpenAI.2023b.Gpt-4technicalreport.PuyuanPeng,BrianYan,ShinjiWatanabe,andDavidHarwath.2023.Promptingthehiddentalentofweb-scalespeechmodelsforzero-shottaskgeneralization.arXivpreprintarXiv:2305.11095.RohitPrabhavalkar,TaraNSainath,YonghuiWu,PatrickNguyen,ZhifengChen,Chung-ChengChiu,andAnjuliKannan.2018.Minimumworderrorratetrainingforattention-basedsequence-to-sequencemodels.InProc.ICASSP,pages4839‚Äì4843.IEEE.AlecRadford,JongWookKim,TaoXu,GregBrock-man,ChristineMcLeavey,andIlyaSutskever.2022.Robustspeechrecognitionvialarge-scaleweaksu-pervision.arXivpreprintarXiv:2212.04356.SrijithRadhakrishnan,Chao-HanHuckYang,etal.2023.Aparameter-efficientlearningapproachtoarabicdialectidentificationwithpre-trainedgeneral-purposespeechmodel.InProc.Interspeech2023,pages1958‚Äì1962.JulianSalazar,DavisLiang,ToanQNguyen,andKa-trinKirchhoff.2020.Maskedlanguagemodelscor-ing.InProceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics,pages2699‚Äì2712.JoonboShin,YoonhyungLee,andKyominJung.2019.Effectivesentencescoringmethodusingbertforspeechrecognition.InAsianConferenceonMachineLearning,pages1081‚Äì1093.PMLR.AdamStooke,KheChaiSim,MasonChua,TsendsurenMunkhdalai,andTrevorStrohman.2023.Internallanguagemodelpersonalizationofe2eautomaticspeechrecognitionusingrandomencoderfeatures.InProc.SLT,pages213‚Äì220.IEEE.TomohiroTanaka,RyoMasumura,ManaIhori,AkihikoTakashima,TakafumiMoriya,TakanoriAshihara,ShotaOrihashi,andNaokiMakishima.2021.Cross-modaltransformer-basedneuralcorrectionmodelsforautomaticspeechrecognition.InProc.Inter-speech2021.ISCA.RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,XuechenLi,CarlosGuestrin,PercyLiang,andTatsunoriB.Hashimoto.2023.Stanfordalpaca:Aninstruction-followingllamamodel.https://github.com/tatsu-lab/stanford_alpaca.HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timoth√©eLacroix,BaptisteRozi√®re,NamanGoyal,EricHambro,FaisalAzhar,etal.2023.Llama:Openandeffi-cientfoundationlanguagemodels.arXivpreprintarXiv:2302.13971.GokhanTurandRenatoDeMori.2011.Spokenlan-guageunderstanding:Systemsforextractingseman-ticinformationfromspeech.JohnWiley&Sons.AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,≈ÅukaszKaiser,andIlliaPolosukhin.2017.Attentionisallyouneed.Advancesinneuralinformationprocessingsystems,30.SiyinWang,Chao-HanHuckYang,JiWu,andChaoZhang.2023.Canwhisperperformspeech-basedin-contextlearning.arXivpreprintarXiv:2309.07081.ShinjiWatanabe,TakaakiHori,etal.2018.Espnet:End-to-endspeechprocessingtoolkit.Proc.Interspeech2018.YingceXia,FeiTian,LijunWu,JianxinLin,TaoQin,NenghaiYu,andTie-YanLiu.2017.Deliberationnetworks:Sequencegenerationbeyondone-passde-coding.Advancesinneuralinformationprocessingsystems,30.Chao-HanHuckYang,YileGu,Yi-ChiehLiu,ShaliniGhosh,IvanBulyko,andAndreasStolcke.2023a.Generativespeechrecognitionerrorcorrectionwithlargelanguagemodelsandtask-activatingprompting.IEEEProc.ASRU2023.Chao-HanHuckYang,BoLi,YuZhang,NanxinChen,RohitPrabhavalkar,TaraNSainath,andTrevorStrohman.2023b.Fromenglishtomorelanguages:Parameter-efficientmodelreprogrammingforcross-lingualspeechrecognition.InProc.ICASSP,pages1‚Äì5.IEEE.Chao-HanHuckYang,LindaLiu,AnkurGandhe,YileGu,AnirudhRaju,DenisFilimonov,andIvanBulyko.2021a.Multi-tasklanguagemodelingforimprovingspeechrecognitionofrarewords.InProc.ASRU,pages1087‚Äì1093.IEEE.Chao-HanHuckYang,Yun-YunTsai,andPin-YuChen.2021b.Voice2series:Reprogrammingacousticmod-elsfortimeseriesclassification.InInternationalConferenceonMachineLearning,pages11808‚Äì11819.PMLR.HaoYen,Pin-JuiKu,Chao-HanHuckYang,HuHu,SabatoMarcoSiniscalchi,Pin-YuChen,andYuTsao.2023.Neuralmodelreprogrammingwithsimilaritybasedmappingforlow-resourcespokencommandclassification.Proc.Interspeech2023.YuYu,Chao-HanHuckYang,JariKolehmainen,etal.2023.Low-rankadaptationoflargelanguagemodelrescoringforparameter-efficientspeechrecognition.IEEEProc.ASRU2023.ZhuoshengZhang,AstonZhang,MuLi,HaiZhao,GeorgeKarypis,andAlexSmola.2023.Multi-modalchain-of-thoughtreasoninginlanguagemod-els.arXivpreprintarXiv:2302.00923.

10013

AAppendixInthisAppendix,WeinvestigatetheperformancedifferencebetweendatasetsinSectionB,andpro-videillustrationsofthemodel-levelarchitecturaldesigninSectionC.SectionDprovidesmorein-sightintotheresultsfromablationstudiesandwereportaWhisperLargeHypothesisbaselineinSec-tionE.BDatasetAnalysisWereporttheWERfromourexperimentsbeforeandaftertextnormalizationonTable3.Wecon-vertthemodelpredictionandthegroundtruthtolower-caseandremovepunctuationduringtextnor-malization.TheATISdatasetisnotimpactedbytextnormalizationbecausethedatasetdoesnotcon-tainanypunctuation.Itonlycontainscontractionssuchas‚ÄúI‚Äôdlike‚Äùinsteadof‚ÄúIwouldlike‚Äù.ATISconsistsofaudiorecordingsofindividualsquery-ingautomatedairlinetravelinquirysystemsforflightinformation.Webelievethelackofpunctua-tionandtheconsistentstructurepresentwithintheATISdatasetenablesimprovedWERperformancecomparedtoGigaSpeech.TheGigaspeechdatasetcontainspunctuationandlacksconsistencywithinthedatasetbecauseithasdiversecategoriesandsourcessuchasaudiobooks,podcastsandYouTubevideos.B.1MoreDiscussiononGroundTruthMatchRateDuringdatasetgeneration,weremovethegroundtruthifitispresentamongtheWhispergeneratedn-besthypotheses.ThisallowsustointroduceanewmetriccalledGroundTruthMatchRate(GTMR).GTMRcalculatesthepercentageofpredictionsgeneratedbythemodelthatexactlymatchthegroundtruth.Thismetricindicatesthemodel‚Äôsabilitytolearnthestructureofthedataset.TheGTMRofourexperimentsbeforeandaftertextnormalizationisreportedinTable5.Themodelisabletolearnthestructureofthedatasetbetterwithmoredatapoints,asobservedfromtheperfor-mancedifferencebetweenGSSSandGSSM.ItcanalsobeobservedthatthemodelisabletolearnthesimplerstructureofATISmuchbetterthanotherGigaSpecchdatasets.B.2WERRWorderrorraterelativeiscalculatedasWERR(i)‚ÜêOracle(i)‚àíWER(i)

Oracel(i)√ó100(7)whereOracle(i)referstotheaverageOracleper-formanceintermsofWERandWER(i)referstotheaverageperformanceofaparticularmethod.CProposedArchitectureIllustrationsWepresentamodel-leveloverviewofourproposedmethoddescribedinSection3.2inFig2.WeaddtwomodulesintoeachlayeroftheLLaMAmodel.TheLLaMAadapterandtheFusionadapterwhichrefertoALandAW,respectively.WeinitializetheFusionadapterwiththeweightsfromtheWhis-percross-attentionmoduleinthedecodermodel.LLaMAtakestheencodedfeaturesgeneratedbytheWhisperencoderandthen-besthypothesisgen-eratedbytheWhisperinapromptformatasinputtogeneratetheerror-correctedresponse.DFailureCaseStudiesofGenerativeASRwithWhispering-LLaMASincetheWERerrorrateinrow6(WLMw/oaudiorepresentations)androw7(WLMw/oini-tialization)oftable2ishighandprovidesnoinsightintomodelperformance,wepresentthetraininglossgraphsofthebestperformingmodel(WLM)withandwithoutaudiorepresentationsinFigure4.Themodelisnotabletoconvergebelowacertainthresholdwithoutaudiorepresentations.Addition-ally,weincludethetraininglossgraphsofWLMwithandwithoutourinitializationmechanisminFigure3.Withoutourinitializationmechanism,thelatentstructureoftheWhisperencoderembeddingisnotpreserved,leadingtoaninabilitytoconverge.EWhisperLargeDecodingBaselineWereporttheresultsofusingthehypothesisgeneratedbyWhisperLargetotrainourbest-performingmodel(WLM)onGigaSpeechEn-tertainment(GSE)andScienceandTechnology(GSSS)datasetsonTable4.ByleveragingtheLLaMAmodelwiththeproposedgenerativeerrorcorrectionmechanism,weareabletomatchtheperformanceoftheWhisperLargemodelwith1.5billionparametersbyusingaWhisper-Tinymodelwithjust70millionparameters.Usingthehypothe-sesgeneratedbyWhisperLargeresultsinahigher

10014

WhisperSelf-Attention

WhisperMLP

WhisperSelf-Attention

WhisperMLP

‚Ä¶

WhisperSelf-Attention

WhisperMLP

WhisperCross-Attention

WhisperSelf-Attention

WhisperMLP

WhisperCross-Attention

‚Ä¶

Input Audio Clip

Whisper Encoder

Whisper Decoder

Audio Representations

N-Best HypothesesHello, How are you?hello how how youHello How were youHell are you?

LLaMASelf-Attention

LLaMAMLP

Fusion Adapter (AW)

LLaMASelf-Attention

LLaMAMLP

Fusion Adapter (AW)

‚Ä¶

LLaMA Decoder

LLaMA Adapter (AL)

LLaMA Adapter (AL)

Generative Speech Transcription

Prompt Inputs

Figure2:Whispering-LLaMAmodel-overviewofproposedadaptationpipelinedescribedinSection3.2

0

2500

5000

7500

10000

12500

15000

17500Steps

0

2

4

6

8

10

12Train loss

Train loss of WL-M (Row 3) vs WL-M without initialization (Row 7) on the Entertainment dataset

WL-M

WL-M without initializationFigure3:TrainlossofWLM(Row3)vsWLMwithoutinitialization(Row7)ontheEntertainmentdataset

0

2500

5000

7500

10000

12500

15000

17500Steps

0.00

0.25

0.50

0.75

1.00

1.25

1.50

1.75

2.00Train loss

Train loss of WL-M (Row 3) vs WL-M without audio features (Row 6) on the Entertainment dataset

WL-M

WL-M without audio featuresFigure4:TrainlossofWLM(Row3)vsWLMwithoutaudiorepresentations(Row6)ontheEntertainmentdatasetWERRasexpected.Thisfindingconfirmstheef-fectivenessoftheproposedapproach,particularlyinthecontextofWhisperLargegeneratedN-besthypotheses.Table3:TheexperimentalresultsintermsofWordEr-rorRate(WER),beforeandaftertextnormalization.Weconvertalltexttolowercaseandremovethefollow-ingpunctuation[".","-","?","‚Äô"].Rows1-3representbeforetextnormalization,andRows4-6representaftertextnormalization.

#

Method

ATIS

GSE

GSP

GSSS

GSSM

WERAvg(‚Üì)

1

WLL

2.04

21.76

19.21

20.55

11.6

15.03

2

WLM

1.77

21.61

16.20

18.02

9.82

13.48

3

WLS

2.11

23.60

17.13

20.16

10.73

14.75

4

WLL

2.04

14.71

13.36

14.23

7.44

10.35

5

WLM

1.77

17.71

10.83

12.00

6.07

9.67

6

WLS

1.89

15.21

11.49

13.03

6.41

9.61

FReproducibilityResourcesWehaveopen-sourcedthepre-trainedmodelweightsandcode,availableathttps://github.com/Srijith-rkr/Whispering-LLaMA.Ourfu-tureplanincludesintegratingthisbaselineintobothEspnet(Watanabeetal.,2018)andHyPo-radise(Yangetal.,2023a;Chenetal.,2023a)toaccommodateabroaderrangeofusecases.AcknowledgementTheauthorsthankMaximLvovandanonymousreviewersfortheirfeedbackonthedraft.

10015

### Instruction:

You are an ASR transcript selector. You have a few transcripts generated by an automatic

speech

recognition model. Your task is to generate the most likely transcript from them. If the generated

transcripts have grammatical or logical errors, you will modify them accordingly to produce the

most accurate and coherent transcript.

### Input:

so that it can carry its momentum to the logic

that he can carry his moment on tour with the logic

that he can carry his momentum through with the logic

that he can carry his momentum to with an logic

that he can carry his momentum true within logic

that he carries momentum through with logic

that it can carry a moment and through with the logic

that it can carry a moment of truth with the logic

that it can carry a moment on tour with the logic

that it can carry a moment on true with the logic

that it can carry a momentum tool within logic

that it can carry as a moment on tour with the logic

that it can carry at moments and through with the logic

that it can carry his moment on tour with a logic

that it can carry his moment on tour with the logic

### Response:

Model Input

that it can carry its momentum through with a logic.

Model OutputFigure5:IllustrationoftheAlpacaprompttemplateusedinourproposedframeworkTable4:ResultsofemployingaWhisperLargegeneratedhypothesisebaselineincomparisontotheproposedWhisperTinyhypothesisbaseline

Method

GSE

GSSS

WERAvg(‚Üì)

WERR(‚Üë)

OraclewithWhisperTiny

28.22

23.93

26.08



WLMwithWhisperTiny

21.61

18.02

19.82

24.01

OraclewithWhisperLarge

21.78

18.09

19.93



WLMwithWhisperLarge

15.59

12.77

14.18

28.86

Table5:TheexperimentalresultsintermsofGroundTruthMatchRate(GTMR),Beforeandaftertextnormalization.Rows1-3representbeforetextnormalization,andRows5-7representaftertextnormalization.Row4and6denotetheaverageGTMRacrossdatasets,andcolumnWERAvg‚ÜìdenotestheaverageGTMRacrosseachmethod

#

Method

ATIS

GSE

GSP

GSSS

GSSM

WERAvg(‚Üì)

1

WLL

86.1

26.5

24.1

24.8

36.2

39.5

2

WLM

88.3

26.3

31.4

28.8

41.0

43.17

3

WLS

87.5

25.2

27.6

26.6

38.6.1

41.10

4

Avg

87.3

26.0

27.7

26.7

38.6



5

WLL

86.1

46.9

46

45.5

56.4

56.2

6

WLM

88.3

47.0

54.3

49.2

62.7

60.3

7

WLS

87.5

44.3

49.5

46.3

60.0

57.5

8

Avg

87.3

46.0

49.9

47.0

59.7



10016