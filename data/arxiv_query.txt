<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3A%28%22NVIDIA%22%20OR%20%22CUDA%22%20OR%20%22TensorRT%22%29%20AND%20%28cat%3Acs.LG%20OR%20cat%3Acs.AI%20OR%20cat%3Acs.CV%20OR%20cat%3Acs.DC%20OR%20cat%3Acs.NE%20OR%20cat%3Acs.AR%20OR%20cat%3Acs.CL%29%26id_list%3D%26start%3D0%26max_results%3D50" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:("NVIDIA" OR "CUDA" OR "TensorRT") AND (cat:cs.LG OR cat:cs.AI OR cat:cs.CV OR cat:cs.DC OR cat:cs.NE OR cat:cs.AR OR cat:cs.CL)&amp;id_list=&amp;start=0&amp;max_results=50</title>
  <id>http://arxiv.org/api/Jvxvh+CHl4mvJXmKVH+8gxzc3P8</id>
  <updated>2025-10-19T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">2553</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">50</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2510.14719v1</id>
    <updated>2025-10-16T14:20:00Z</updated>
    <published>2025-10-16T14:20:00Z</published>
    <title>Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous
  References</title>
    <summary>  Modern GPUs feature specialized hardware units that enable high-performance,
asynchronous dataflow execution. However, the conventional SIMT programming
model is fundamentally misaligned with this task-parallel hardware, creating a
significant programmability gap. While hardware-level warp specialization is
the key to unlocking peak performance, it forces developers to manually
orchestrate complex, low-level communication and software pipelines--a process
that is labor-intensive, error-prone, and unsustainable. To address this
challenge, we present Tawa, an automated compiler that systematically generates
high-performance, warp-specialized code from a high-level, tile-based program.
Central to our approach is a novel IR abstraction, asynchronous references
(aref), which expresses warp-level communication without exposing low-level
hardware details. Using this abstraction, Tawa automatically partitions
programs into producer-consumer roles and manages the intricate dataflow
pipeline, relieving developers of invasive kernel rewriting. Evaluation on
NVIDIA H100 GPUs across representative LLM kernels shows that Tawa delivers
high hardware utilization, achieving up to 1.1$\times$ speedup over highly
optimized cuBLAS GEMM kernels. For attention workloads, Tawa attains
1.2$\times$ speedup over Triton and matches the performance of the
hand-optimized CUTLASS C++ FlashAttention-3 kernel with far less programming
effort.
</summary>
    <author>
      <name>Hongzheng Chen</name>
    </author>
    <author>
      <name>Bin Fan</name>
    </author>
    <author>
      <name>Alexander Collins</name>
    </author>
    <author>
      <name>Bastian Hagedorn</name>
    </author>
    <author>
      <name>Evghenii Gaburov</name>
    </author>
    <author>
      <name>Masahiro Masuda</name>
    </author>
    <author>
      <name>Matthew Brookhart</name>
    </author>
    <author>
      <name>Chris Sullivan</name>
    </author>
    <author>
      <name>Jason Knight</name>
    </author>
    <author>
      <name>Zhiru Zhang</name>
    </author>
    <author>
      <name>Vinod Grover</name>
    </author>
    <link href="http://arxiv.org/abs/2510.14719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.14719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.14564v1</id>
    <updated>2025-10-16T11:16:58Z</updated>
    <published>2025-10-16T11:16:58Z</published>
    <title>BalanceGS: Algorithm-System Co-design for Efficient 3D Gaussian
  Splatting Training on GPU</title>
    <summary>  3D Gaussian Splatting (3DGS) has emerged as a promising 3D reconstruction
technique. The traditional 3DGS training pipeline follows three sequential
steps: Gaussian densification, Gaussian projection, and color splatting.
Despite its promising reconstruction quality, this conventional approach
suffers from three critical inefficiencies: (1) Skewed density allocation
during Gaussian densification, (2) Imbalanced computation workload during
Gaussian projection and (3) Fragmented memory access during color splatting.
  To tackle the above challenges, we introduce BalanceGS, the algorithm-system
co-design for efficient training in 3DGS. (1) At the algorithm level, we
propose heuristic workload-sensitive Gaussian density control to automatically
balance point distributions - removing 80% redundant Gaussians in dense regions
while filling gaps in sparse areas. (2) At the system level, we propose
Similarity-based Gaussian sampling and merging, which replaces the static
one-to-one thread-pixel mapping with adaptive workload distribution - threads
now dynamically process variable numbers of Gaussians based on local cluster
density. (3) At the mapping level, we propose reordering-based memory access
mapping strategy that restructures RGB storage and enables batch loading in
shared memory.
  Extensive experiments demonstrate that compared with 3DGS, our approach
achieves a 1.44$\times$ training speedup on a NVIDIA A100 GPU with negligible
quality degradation.
</summary>
    <author>
      <name>Junyi Wu</name>
    </author>
    <author>
      <name>Jiaming Xu</name>
    </author>
    <author>
      <name>Jinhao Li</name>
    </author>
    <author>
      <name>Yongkang Zhou</name>
    </author>
    <author>
      <name>Jiayi Pan</name>
    </author>
    <author>
      <name>Xingyang Li</name>
    </author>
    <author>
      <name>Guohao Dai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ASP-DAC 2026</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.14564v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.14564v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.14143v1</id>
    <updated>2025-10-15T22:22:06Z</updated>
    <published>2025-10-15T22:22:06Z</published>
    <title>cubic: CUDA-accelerated 3D Bioimage Computing</title>
    <summary>  Quantitative analysis of multidimensional biological images is useful for
understanding complex cellular phenotypes and accelerating advances in
biomedical research. As modern microscopy generates ever-larger 2D and 3D
datasets, existing computational approaches are increasingly limited by their
scalability, efficiency, and integration with modern scientific computing
workflows. Existing bioimage analysis tools often lack application programmable
interfaces (APIs), do not support graphics processing unit (GPU) acceleration,
lack broad 3D image processing capabilities, and/or have poor interoperability
for compute-heavy workflows. Here, we introduce cubic, an open-source Python
library that addresses these challenges by augmenting widely used SciPy and
scikit-image APIs with GPU-accelerated alternatives from CuPy and RAPIDS cuCIM.
cubic's API is device-agnostic and dispatches operations to GPU when data
reside on the device and otherwise executes on CPU, seamlessly accelerating a
broad range of image processing routines. This approach enables GPU
acceleration of existing bioimage analysis workflows, from preprocessing to
segmentation and feature extraction for 2D and 3D data. We evaluate cubic both
by benchmarking individual operations and by reproducing existing deconvolution
and segmentation pipelines, achieving substantial speedups while maintaining
algorithmic fidelity. These advances establish a robust foundation for
scalable, reproducible bioimage analysis that integrates with the broader
Python scientific computing ecosystem, including other GPU-accelerated methods,
enabling both interactive exploration and automated high-throughput analysis
workflows. cubic is openly available at
https://github$.$com/alxndrkalinin/cubic
</summary>
    <author>
      <name>Alexandr A. Kalinin</name>
    </author>
    <author>
      <name>Anne E. Carpenter</name>
    </author>
    <author>
      <name>Shantanu Singh</name>
    </author>
    <author>
      <name>Matthew J. O'Meara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted to BioImage Computing workshop @ ICCV 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.14143v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.14143v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92C55, 68U10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.0; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.14050v1</id>
    <updated>2025-10-15T19:41:26Z</updated>
    <published>2025-10-15T19:41:26Z</published>
    <title>Anonymized Network Sensing using C++26 std::execution on GPUs</title>
    <summary>  Large-scale network sensing plays a vital role in network traffic analysis
and characterization. As network packet data grows increasingly large, parallel
methods have become mainstream for network analytics. While effective,
GPU-based implementations still face start-up challenges in host-device memory
management and porting complex workloads on devices, among others. To mitigate
these challenges, composable frameworks have emerged using modern C++
programming language, for efficiently deploying analytics tasks on GPUs.
Specifically, the recent C++26 Senders model of asynchronous data operation
chaining provides a simple interface for bulk pushing tasks to varied device
execution contexts.
  Considering the prominence of contemporary dense-GPU platforms and
vendor-leveraged software libraries, such a programming model consider GPUs as
first-class execution resources (compared to traditional host-centric
programming models), allowing convenient development of multi-GPU application
workloads via expressive and standardized asynchronous semantics. In this
paper, we discuss practical aspects of developing the Anonymized Network
Sensing Graph Challenge on dense-GPU systems using the recently proposed C++26
Senders model. Adopting a generic and productive programming model does not
necessarily impact the critical-path performance (as compared to low-level
proprietary vendor-based programming models): our commodity library-based
implementation achieves up to 55x performance improvements on 8x NVIDIA A100
GPUs as compared to the reference serial GraphBLAS baseline.
</summary>
    <author>
      <name>Michael Mandulak</name>
    </author>
    <author>
      <name>Sayan Ghosh</name>
    </author>
    <author>
      <name>S M Ferdous</name>
    </author>
    <author>
      <name>Mahantesh Halappanavar</name>
    </author>
    <author>
      <name>George Slota</name>
    </author>
    <link href="http://arxiv.org/abs/2510.14050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.14050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.13757v1</id>
    <updated>2025-10-15T17:05:55Z</updated>
    <published>2025-10-15T17:05:55Z</published>
    <title>A Complete Pipeline for deploying SNNs with Synaptic Delays on Loihi 2</title>
    <summary>  Spiking Neural Networks are attracting increased attention as a more
energy-efficient alternative to traditional Artificial Neural Networks for edge
computing. Neuromorphic computing can significantly reduce energy requirements.
Here, we present a complete pipeline: efficient event-based training of SNNs
with synaptic delays on GPUs and deployment on Intel's Loihi 2 neuromorphic
chip. We evaluate our approach on keyword recognition tasks using the Spiking
Heidelberg Digits and Spiking Speech Commands datasets, demonstrating that our
algorithm can enhance classification accuracy compared to architectures without
delays. Our benchmarking indicates almost no accuracy loss between GPU and
Loihi 2 implementations, while classification on Loihi 2 is up to 18x faster
and uses 250x less energy than on an NVIDIA Jetson Orin Nano.
</summary>
    <author>
      <name>Balázs Mészáros</name>
    </author>
    <author>
      <name>James C. Knight</name>
    </author>
    <author>
      <name>Jonathan Timcheck</name>
    </author>
    <author>
      <name>Thomas Nowotny</name>
    </author>
    <link href="http://arxiv.org/abs/2510.13757v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.13757v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.13546v1</id>
    <updated>2025-10-15T13:40:55Z</updated>
    <published>2025-10-15T13:40:55Z</published>
    <title>Accelerated Feature Detectors for Visual SLAM: A Comparative Study of
  FPGA vs GPU</title>
    <summary>  Feature detection is a common yet time-consuming module in Simultaneous
Localization and Mapping (SLAM) implementations, which are increasingly
deployed on power-constrained platforms, such as drones. Graphics Processing
Units (GPUs) have been a popular accelerator for computer vision in general,
and feature detection and SLAM in particular.
  On the other hand, System-on-Chips (SoCs) with integrated Field Programmable
Gate Array (FPGA) are also widely available. This paper presents the first
study of hardware-accelerated feature detectors considering a Visual SLAM
(V-SLAM) pipeline. We offer new insights by comparing the best GPU-accelerated
FAST, Harris, and SuperPoint implementations against the FPGA-accelerated
counterparts on modern SoCs (Nvidia Jetson Orin and AMD Versal).
  The evaluation shows that when using a non-learning-based feature detector
such as FAST and Harris, their GPU implementations, and the GPU-accelerated
V-SLAM can achieve better run-time performance and energy efficiency than the
FAST and Harris FPGA implementations as well as the FPGA-accelerated V-SLAM.
However, when considering a learning-based detector such as SuperPoint, its
FPGA implementation can achieve better run-time performance and energy
efficiency (up to 3.1$\times$ and 1.4$\times$ improvements, respectively) than
the GPU implementation. The FPGA-accelerated V-SLAM can also achieve comparable
run-time performance compared to the GPU-accelerated V-SLAM, with better FPS in
2 out of 5 dataset sequences. When considering the accuracy, the results show
that the GPU-accelerated V-SLAM is more accurate than the FPGA-accelerated
V-SLAM in general. Last but not least, the use of hardware acceleration for
feature detection could further improve the performance of the V-SLAM pipeline
by having the global bundle adjustment module invoked less frequently without
sacrificing accuracy.
</summary>
    <author>
      <name>Ruiqi Ye</name>
    </author>
    <author>
      <name>Mikel Luján</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.13546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.13546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.3; C.4; I.4.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.13250v1</id>
    <updated>2025-10-15T07:58:46Z</updated>
    <published>2025-10-15T07:58:46Z</published>
    <title>Real-Time Crowd Counting for Embedded Systems with Lightweight
  Architecture</title>
    <summary>  Crowd counting is a task of estimating the number of the crowd through
images, which is extremely valuable in the fields of intelligent security,
urban planning, public safety management, and so on. However, the existing
counting methods have some problems in practical application on embedded
systems for these fields, such as excessive model parameters, abundant complex
calculations, etc. The practical application of embedded systems requires the
model to be real-time, which means that the model is fast enough. Considering
the aforementioned problems, we design a super real-time model with a
stem-encoder-decoder structure for crowd counting tasks, which achieves the
fastest inference compared with state-of-the-arts. Firstly, large convolution
kernels in the stem network are used to enlarge the receptive field, which
effectively extracts detailed head information. Then, in the encoder part, we
use conditional channel weighting and multi-branch local fusion block to merge
multi-scale features with low computational consumption. This part is crucial
to the super real-time performance of the model. Finally, the feature pyramid
networks are added to the top of the encoder to alleviate its incomplete fusion
problems. Experiments on three benchmarks show that our network is suitable for
super real-time crowd counting on embedded systems, ensuring competitive
accuracy. At the same time, the proposed network reasoning speed is the
fastest. Specifically, the proposed network achieves 381.7 FPS on NVIDIA GTX
1080Ti and 71.9 FPS on NVIDIA Jetson TX1.
</summary>
    <author>
      <name>Zhiyuan Zhao</name>
    </author>
    <author>
      <name>Yubin Wen</name>
    </author>
    <author>
      <name>Siyu Yang</name>
    </author>
    <author>
      <name>Lichen Ning</name>
    </author>
    <author>
      <name>Yuandong Liu</name>
    </author>
    <author>
      <name>Junyu Gao</name>
    </author>
    <link href="http://arxiv.org/abs/2510.13250v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.13250v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.12705v1</id>
    <updated>2025-10-14T16:39:29Z</updated>
    <published>2025-10-14T16:39:29Z</published>
    <title>A GPU-resident Memory-Aware Algorithm for Accelerating Bidiagonalization
  of Banded Matrices</title>
    <summary>  The reduction of a banded matrix to a bidiagonal form is a crucial step in
the Singular Value Decomposition (SVD), a cornerstone of scientific computing
and AI. Despite being a highly parallel algorithm, it was previously believed
to be unsuitable for GPU computation because it is memory bandwidth-bound.
Recent developments in GPU hardware, including larger L1 memory per Streaming
Multiprocessor/Compute Unit, have changed that. We present the first GPU
algorithm for reducing a banded matrix to bidiagonal form as part of the
NextLA$.$jl open-source software package. Our algorithm is based on previous
CPU-based multicore parallel cache-efficient bulge chasing algorithms and
adapted to optimize for GPU throughput. We leverage Julia Language's Array
abstractions and KernelAbstractions to implement a single hardware- and data
precision-agnostic function on NVIDIA, AMD, Intel, and Apple Metal GPUs for
half, single, and double precision, and examine performance optimization across
hardware architectures and data precision. We also develop a hardware-aware
performance model and identify key hyperparameters, such as inner tilewidth and
block concurrency, that govern optimal GPU execution for bandwidth-bound
workloads. We demonstrate highly parallel bandwidth-bound algorithm on the GPU
can outperform CPU-based implementations: the GPU algorithm outperforms
multithreaded CPU High-Performance libraries PLASMA and SLATE as of matrix size
1024 x 1024 and by a factor over 100 for matrices of 32k x 32k. In addition,
the performance of the algorithm increases linearly with matrix bandwidth size,
making faster reduction of larger matrix bandwidths now also possible. With
this work, we break memory bandwidth barriers, as well as matrix bandwidth
barriers, resulting in orders-of-magnitude faster algorithms for the reduction
of banded matrices to bidiagonal form on the GPU.
</summary>
    <author>
      <name>Evelyne Ringoot</name>
    </author>
    <author>
      <name>Rabab Alomairy</name>
    </author>
    <author>
      <name>Alan Edelman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 7 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.12705v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.12705v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.12208v1</id>
    <updated>2025-10-14T06:59:51Z</updated>
    <published>2025-10-14T06:59:51Z</published>
    <title>The Impact of Synthetic Data on Object Detection Model Performance: A
  Comparative Analysis with Real-World Data</title>
    <summary>  Recent advances in generative AI, particularly in computer vision (CV), offer
new opportunities to optimize workflows across industries, including logistics
and manufacturing. However, many AI applications are limited by a lack of
expertise and resources, which forces a reliance on general-purpose models.
Success with these models often requires domain-specific data for fine-tuning,
which can be costly and inefficient. Thus, using synthetic data for fine-tuning
is a popular, cost-effective alternative to gathering real-world data. This
work investigates the impact of synthetic data on the performance of object
detection models, compared to models trained on real-world data only,
specifically within the domain of warehouse logistics. To this end, we examined
the impact of synthetic data generated using the NVIDIA Omniverse Replicator
tool on the effectiveness of object detection models in real-world scenarios.
It comprises experiments focused on pallet detection in a warehouse setting,
utilizing both real and various synthetic dataset generation strategies. Our
findings provide valuable insights into the practical applications of synthetic
image data in computer vision, suggesting that a balanced integration of
synthetic and real data can lead to robust and efficient object detection
models.
</summary>
    <author>
      <name>Muammer Bay</name>
    </author>
    <author>
      <name>Timo von Marcard</name>
    </author>
    <author>
      <name>Dren Fazlija</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 12 figures, 2 tables. Code:
  https://github.com/MuammerBay/omniverse-replicator-sim2real-analysis ; Data:
  https://doi.org/10.5281/zenodo.17308406</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.12208v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.12208v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.12174v1</id>
    <updated>2025-10-14T06:07:57Z</updated>
    <published>2025-10-14T06:07:57Z</published>
    <title>UniGS: Unified Geometry-Aware Gaussian Splatting for Multimodal
  Rendering</title>
    <summary>  In this paper, we propose UniGS, a unified map representation and
differentiable framework for high-fidelity multimodal 3D reconstruction based
on 3D Gaussian Splatting. Our framework integrates a CUDA-accelerated
rasterization pipeline capable of rendering photo-realistic RGB images,
geometrically accurate depth maps, consistent surface normals, and semantic
logits simultaneously. We redesign the rasterization to render depth via
differentiable ray-ellipsoid intersection rather than using Gaussian centers,
enabling effective optimization of rotation and scale attribute through
analytic depth gradients. Furthermore, we derive the analytic gradient
formulation for surface normal rendering, ensuring geometric consistency among
reconstructed 3D scenes. To improve computational and storage efficiency, we
introduce a learnable attribute that enables differentiable pruning of
Gaussians with minimal contribution during training. Quantitative and
qualitative experiments demonstrate state-of-the-art reconstruction accuracy
across all modalities, validating the efficacy of our geometry-aware paradigm.
Source code and multimodal viewer will be available on GitHub.
</summary>
    <author>
      <name>Yusen Xie</name>
    </author>
    <author>
      <name>Zhenmin Huang</name>
    </author>
    <author>
      <name>Jianhao Jiao</name>
    </author>
    <author>
      <name>Dimitrios Kanoulas</name>
    </author>
    <author>
      <name>Jun Ma</name>
    </author>
    <link href="http://arxiv.org/abs/2510.12174v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.12174v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.12128v1</id>
    <updated>2025-10-14T04:06:02Z</updated>
    <published>2025-10-14T04:06:02Z</published>
    <title>nuGPR: GPU-Accelerated Gaussian Process Regression with Iterative
  Algorithms and Low-Rank Approximations</title>
    <summary>  Gaussian Process Regression (GPR) is an important type of supervised machine
learning model with inherent uncertainty measure in its predictions. We propose
a new framework, nuGPR, to address the well-known challenge of high computation
cost associated with GPR training. Our framework includes several ideas from
numerical linear algebra to reduce the amount of computation in key steps of
GPR, and we combine them to establish an end-to-end training algorithm.
Specifically, we leverage the preconditioned conjugate gradient method to
accelerate the convergence of the linear solves required in GPR. We exploit
clustering in the input data to identify block-diagonal structure of the
covariance matrix and subsequently construct low-rank approximations of the
off-diagonal blocks. These enhancements significantly reduce the time and space
complexity of our computations. In addition, unlike other frameworks that rely
on exact differentiation, we employ numerical gradients to optimize the
hyperparameters of our GPR model, further reducing the training cost by
eliminating the need for backpropagation. Lastly, we leverage the CUDA Toolkit
to efficiently parallelize the training procedure on NVIDIA GPUs. As a result,
nuGPR reduces total training time by up to 2x and peak memory consumption by up
to 12x on various synthetic and real-world datasets when compared to the best
existing GPU-based GPR implementation.
</summary>
    <author>
      <name>Ziqi Zhao</name>
    </author>
    <author>
      <name>Vivek Sarin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1137/24M1683615</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1137/24M1683615" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 6 figures, published in SIAM Journal on Scientific
  Computing, E-print available at:
  https://epubs.siam.org/eprint/5CF5CKX49Y4FUQXZFHCN/full</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SIAM Journal on Scientific Computing, 2025, Vol. 47, No. 5, pp.
  B1250-B1271</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2510.12128v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.12128v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65Y05, 60G15, 65F10, 65F55" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.12083v1</id>
    <updated>2025-10-14T02:47:52Z</updated>
    <published>2025-10-14T02:47:52Z</published>
    <title>An AI-Based Behavioral Health Safety Filter and Dataset for Identifying
  Mental Health Crises in Text-Based Conversations</title>
    <summary>  Large language models often mishandle psychiatric emergencies, offering
harmful or inappropriate advice and enabling destructive behaviors. This study
evaluated the Verily behavioral health safety filter (VBHSF) on two datasets:
the Verily Mental Health Crisis Dataset containing 1,800 simulated messages and
the NVIDIA Aegis AI Content Safety Dataset subsetted to 794 mental
health-related messages. The two datasets were clinician-labelled and we
evaluated performance using the clinician labels. Additionally, we carried out
comparative performance analyses against two open source, content moderation
guardrails: OpenAI Omni Moderation Latest and NVIDIA NeMo Guardrails. The VBHSF
demonstrated, well-balanced performance on the Verily Mental Health Crisis
Dataset v1.0, achieving high sensitivity (0.990) and specificity (0.992) in
detecting any mental health crises. It achieved an F1-score of 0.939,
sensitivity ranged from 0.917-0.992, and specificity was &gt;= 0.978 in
identifying specific crisis categories. When evaluated against the NVIDIA Aegis
AI Content Safety Dataset 2.0, VBHSF performance remained highly sensitive
(0.982) and accuracy (0.921) with reduced specificity (0.859). When compared
with the NVIDIA NeMo and OpenAI Omni Moderation Latest guardrails, the VBHSF
demonstrated superior performance metrics across both datasets, achieving
significantly higher sensitivity in all cases (all p &lt; 0.001) and higher
specificity relative to NVIDIA NeMo (p &lt; 0.001), but not to OpenAI Omni
Moderation Latest (p = 0.094). NVIDIA NeMo and OpenAI Omni Moderation Latest
exhibited inconsistent performance across specific crisis types, with
sensitivity for some categories falling below 0.10. Overall, the VBHSF
demonstrated robust, generalizable performance that prioritizes sensitivity to
minimize missed crises, a crucial feature for healthcare applications.
</summary>
    <author>
      <name>Benjamin W. Nelson</name>
    </author>
    <author>
      <name>Celeste Wong</name>
    </author>
    <author>
      <name>Matthew T. Silvestrini</name>
    </author>
    <author>
      <name>Sooyoon Shin</name>
    </author>
    <author>
      <name>Alanna Robinson</name>
    </author>
    <author>
      <name>Jessica Lee</name>
    </author>
    <author>
      <name>Eric Yang</name>
    </author>
    <author>
      <name>John Torous</name>
    </author>
    <author>
      <name>Andrew Trister</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Main Text: 2943; Abstract: 256; Tables and Figures: 5</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.12083v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.12083v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.12051v1</id>
    <updated>2025-10-14T01:26:36Z</updated>
    <published>2025-10-14T01:26:36Z</published>
    <title>APCE: Adaptive Progressive Context Expansion for Long Context Processing</title>
    <summary>  Deploying useful Long-Context Transformer Models (LCTMs) requires addressing
two key challenges: (1) A growing memory footprint due to quadratic
self-attention and linear KV-cache scaling in memory as sequence length
increases; (2) the ContextRot phenomena where empirical evidence suggests that
transformer architecture's performance degrades with increasing context length.
Given the shared dependency on the input, a natural question arises: Can we
surgically select the most important input chunks for processing to
synergistically (a) reduce the memory footprint, and (b) mitigate the
ContextRot effects? In this paper, we answer this question in the affirmative
for long-context summarization tasks. We propose APCE as a context-aware
solution to select the most important input chunks through low-dimensional
semantic similarity matching with the current query. By directly operating on
the input, APCE decouples from strict dependency on underlying hardware or CUDA
environments, promising a compatible solution scalable to different deployment
systems. Our empirical evaluations have demonstrated superior or on-par
summarization performance for APCE compared to the full dense baseline using a
fraction (50%-70%) of the input sequence resulting in KV-cache and
self-attention memory efficiency improvements. We hope our findings inspire
further research on context-aware efficiency solutions for LCTMs geared towards
other relevant long-context tasks.
</summary>
    <author>
      <name>Baisub Lee</name>
    </author>
    <author>
      <name>Sanghyun Byun</name>
    </author>
    <author>
      <name>Mohanad Odema</name>
    </author>
    <author>
      <name>Jung Guack</name>
    </author>
    <author>
      <name>Jacob Song</name>
    </author>
    <author>
      <name>Woo Seong Chung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NeurIPS 2025 Workshop: ML For Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.12051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.12051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.11566v1</id>
    <updated>2025-10-13T16:11:34Z</updated>
    <published>2025-10-13T16:11:34Z</published>
    <title>SCOOP'D: Learning Mixed-Liquid-Solid Scooping via Sim2Real Generative
  Policy</title>
    <summary>  Scooping items with tools such as spoons and ladles is common in daily life,
ranging from assistive feeding to retrieving items from environmental disaster
sites. However, developing a general and autonomous robotic scooping policy is
challenging since it requires reasoning about complex tool-object interactions.
Furthermore, scooping often involves manipulating deformable objects, such as
granular media or liquids, which is challenging due to their
infinite-dimensional configuration spaces and complex dynamics. We propose a
method, SCOOP'D, which uses simulation from OmniGibson (built on NVIDIA
Omniverse) to collect scooping demonstrations using algorithmic procedures that
rely on privileged state information. Then, we use generative policies via
diffusion to imitate demonstrations from observational input. We directly apply
the learned policy in diverse real-world scenarios, testing its performance on
various item quantities, item characteristics, and container types. In
zero-shot deployment, our method demonstrates promising results across 465
trials in diverse scenarios, including objects of different difficulty levels
that we categorize as "Level 1" and "Level 2." SCOOP'D outperforms all
baselines and ablations, suggesting that this is a promising approach to
acquiring robotic scooping skills. Project page is at
https://scoopdiff.github.io/.
</summary>
    <author>
      <name>Kuanning Wang</name>
    </author>
    <author>
      <name>Yongchong Gu</name>
    </author>
    <author>
      <name>Yuqian Fu</name>
    </author>
    <author>
      <name>Zeyu Shangguan</name>
    </author>
    <author>
      <name>Sicheng He</name>
    </author>
    <author>
      <name>Xiangyang Xue</name>
    </author>
    <author>
      <name>Yanwei Fu</name>
    </author>
    <author>
      <name>Daniel Seita</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page is at https://scoopdiff.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.11566v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.11566v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.11292v1</id>
    <updated>2025-10-13T11:28:30Z</updated>
    <published>2025-10-13T11:28:30Z</published>
    <title>LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences</title>
    <summary>  While Key-Value (KV) cache succeeds in reducing redundant computations in
auto-regressive models, it introduces significant memory overhead, limiting its
practical deployment in long-sequence scenarios. Existing KV retrieval methods
mitigate this by dynamically retaining only a subset of KV entries on the GPU.
However, they still suffer from notable efficiency and accuracy bottlenecks due
to per-token retrieval and coarse-grained page-level KV management, especially
in long-output reasoning scenarios. With the emergence of large reasoning
models, efficiently handling such scenarios has become increasingly important.
To address this issue, we present two key observations: (1) critical KVs
exhibit strong temporal locality during decoding, and (2) these KVs exhibit
distinct distribution patterns across the input prompt and generated output.
Building on these observations, we propose LouisKV, an efficient KV cache
retrieval framework designed for various long-sequence scenarios. Specifically,
LouisKV introduces a semantic-aware retrieval strategy leveraging temporal
locality to trigger retrieval only at semantic boundaries, drastically reducing
computation and data transfer overhead. LouisKV also designs a decoupled,
fine-grained management scheme that tailors differentiated strategies for input
and output sequences to create retrieval units that better match the model's
attention patterns, enabling precise identification of critical KVs.
Furthermore, to boost efficiency, LouisKV incorporates several kernel-level
optimizations, including custom Triton and CUDA kernels to accelerate the KV
clustering and retrieval. Evaluations show that LouisKV achieves up to
4.7$\times$ speedup over state-of-the-art KV retrieval methods while
maintaining near-lossless accuracy across diverse long-sequence tasks,
including long-input short-output, short-input long-output, and long-input
long-output scenarios.
</summary>
    <author>
      <name>Wenbo Wu</name>
    </author>
    <author>
      <name>Qingyi Si</name>
    </author>
    <author>
      <name>Xiurui Pan</name>
    </author>
    <author>
      <name>Ye Wang</name>
    </author>
    <author>
      <name>Jie Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2510.11292v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.11292v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.10718v1</id>
    <updated>2025-10-12T17:42:01Z</updated>
    <published>2025-10-12T17:42:01Z</published>
    <title>HYPERDOA: Robust and Efficient DoA Estimation using Hyperdimensional
  Computing</title>
    <summary>  Direction of Arrival (DoA) estimation techniques face a critical trade-off,
as classical methods often lack accuracy in challenging, low signal-to-noise
ratio (SNR) conditions, while modern deep learning approaches are too
energy-intensive and opaque for resource-constrained, safety-critical systems.
We introduce HYPERDOA, a novel estimator leveraging Hyperdimensional Computing
(HDC). The framework introduces two distinct feature extraction strategies --
Mean Spatial-Lag Autocorrelation and Spatial Smoothing -- for its HDC pipeline,
and then reframes DoA estimation as a pattern recognition problem. This
approach leverages HDC's inherent robustness to noise and its transparent
algebraic operations to bypass the expensive matrix decompositions and
``black-box'' nature of classical and deep learning methods, respectively. Our
evaluation demonstrates that HYPERDOA achieves ~35.39% higher accuracy than
state-of-the-art methods in low-SNR, coherent-source scenarios. Crucially, it
also consumes ~93% less energy than competing neural baselines on an embedded
NVIDIA Jetson Xavier NX platform. This dual advantage in accuracy and
efficiency establishes HYPERDOA as a robust and viable solution for
mission-critical applications on edge devices.
</summary>
    <author>
      <name>Rajat Bhattacharjya</name>
    </author>
    <author>
      <name>Woohyeok Park</name>
    </author>
    <author>
      <name>Arnab Sarkar</name>
    </author>
    <author>
      <name>Hyunwoo Oh</name>
    </author>
    <author>
      <name>Mohsen Imani</name>
    </author>
    <author>
      <name>Nikil Dutt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 figures, 5 pages. Authors' version posted for personal use and not
  for redistribution</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.10718v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.10718v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.09608v1</id>
    <updated>2025-10-10T17:59:58Z</updated>
    <published>2025-10-10T17:59:58Z</published>
    <title>StreamingVLM: Real-Time Understanding for Infinite Video Streams</title>
    <summary>  Vision-language models (VLMs) could power real-time assistants and autonomous
agents, but they face a critical challenge: understanding near-infinite video
streams without escalating latency and memory usage. Processing entire videos
with full attention leads to quadratic computational costs and poor performance
on long videos. Meanwhile, simple sliding window methods are also flawed, as
they either break coherence or suffer from high latency due to redundant
recomputation. In this paper, we introduce StreamingVLM, a model designed for
real-time, stable understanding of infinite visual input. Our approach is a
unified framework that aligns training with streaming inference. During
inference, we maintain a compact KV cache by reusing states of attention sinks,
a short window of recent vision tokens, and a long window of recent text
tokens. This streaming ability is instilled via a simple supervised fine-tuning
(SFT) strategy that applies full attention on short, overlapped video chunks,
which effectively mimics the inference-time attention pattern without training
on prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a
new benchmark with videos averaging over two hours that requires dense,
per-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM
achieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time
performance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy
also enhances general VQA abilities without any VQA-specific fine-tuning,
improving performance on LongVideoBench by +4.30 and OVOBench Realtime by
+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.
</summary>
    <author>
      <name>Ruyi Xu</name>
    </author>
    <author>
      <name>Guangxuan Xiao</name>
    </author>
    <author>
      <name>Yukang Chen</name>
    </author>
    <author>
      <name>Liuning He</name>
    </author>
    <author>
      <name>Kelly Peng</name>
    </author>
    <author>
      <name>Yao Lu</name>
    </author>
    <author>
      <name>Song Han</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The first two authors contributed equally to this work</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.09608v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.09608v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.09182v1</id>
    <updated>2025-10-10T09:24:53Z</updated>
    <published>2025-10-10T09:24:53Z</published>
    <title>Online Video Depth Anything: Temporally-Consistent Depth Prediction with
  Low Memory Consumption</title>
    <summary>  Depth estimation from monocular video has become a key component of many
real-world computer vision systems. Recently, Video Depth Anything (VDA) has
demonstrated strong performance on long video sequences. However, it relies on
batch-processing which prohibits its use in an online setting. In this work, we
overcome this limitation and introduce online VDA (oVDA). The key innovation is
to employ techniques from Large Language Models (LLMs), namely, caching latent
features during inference and masking frames at training. Our oVDA method
outperforms all competing online video depth estimation methods in both
accuracy and VRAM usage. Low VRAM usage is particularly important for
deployment on edge devices. We demonstrate that oVDA runs at 42 FPS on an
NVIDIA A100 and at 20 FPS on an NVIDIA Jetson edge device. We will release
both, code and compilation scripts, making oVDA easy to deploy on low-power
hardware.
</summary>
    <author>
      <name>Johann-Friedrich Feiden</name>
    </author>
    <author>
      <name>Tim Küchler</name>
    </author>
    <author>
      <name>Denis Zavadski</name>
    </author>
    <author>
      <name>Bogdan Savchynskyy</name>
    </author>
    <author>
      <name>Carsten Rother</name>
    </author>
    <link href="http://arxiv.org/abs/2510.09182v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.09182v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.08842v1</id>
    <updated>2025-10-09T21:56:53Z</updated>
    <published>2025-10-09T21:56:53Z</published>
    <title>Maple: A Multi-agent System for Portable Deep Learning across Clusters</title>
    <summary>  Training deep learning (DL) models across Graphics Processing Unit (GPU)
clusters is technically challenging. One aspect is that users have to compose
command lines to adapt to the heterogeneous launchers, schedulers, affinity
options, DL framework arguments, and environment variables. Composing correct
command lines is error-prone and can easily frustrate users, impeding research
or wasting resources. In this work, we present Maple, a multi-agent system that
generates correct DL command lines with users' natural language input. Maple
consists of four agents with the functionalities of information extraction,
template retrieval, command line verification, and error correction. We
evaluate Maple on nine GPU clusters across national computing centers in the
U.S., five representative deep learning model families, and four commonly used
parallel DL training paradigms. Our experiments also cover schedulers of SLURM
and PBS and heterogeneous architectures, such as NVIDIA A100/H200 GPUs and
Intel Max series GPUs. Maple achieves 92.0% accuracy in generating command
lines across the 567 test cases. Leverage multiple language models with an
aggregated size of 10B parameters, Maple delivers comparable performance to the
state-of-the-art models of GPT-5, Claude, and Gemini. Together, these results
highlight Maple's practical value in enabling portable and scalable distributed
DL across heterogeneous HPC environments.
</summary>
    <author>
      <name>Molang Wu</name>
    </author>
    <author>
      <name>Zhao Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2510.08842v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.08842v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.08726v1</id>
    <updated>2025-10-09T18:33:52Z</updated>
    <published>2025-10-09T18:33:52Z</published>
    <title>Neptune: Advanced ML Operator Fusion for Locality and Parallelism on
  GPUs</title>
    <summary>  Operator fusion has become a key optimization for deep learning, which
combines multiple deep learning operators to improve data reuse and reduce
global memory transfers. However, existing tensor compilers struggle to fuse
complex reduction computations involving loop-carried dependencies, such as
attention mechanisms.
  The paper introduces Neptune, a tensor compiler for advanced operator fusion
for sequences of reduction operators. Neptune presents a new approach for
advanced operator fusion, which intentionally breaks some existing dependencies
and compensates by constructing algebraic correction expressions that allow the
kernel to produce the correct result.
  On ten attention-based benchmarks, Neptune, starting from simple attention
code and a high-level scheduling template, outperforms existing compilers like
Triton, TVM, and FlexAttention, including Triton-based implementations of
FlashAttention. Across four different GPU architectures from NVIDIA and AMD,
Neptune-generated kernels have average speedup of $1.35\times$ over the next
best alternative, demonstrating its effectiveness for deep learning workloads.
</summary>
    <author>
      <name>Yifan Zhao</name>
    </author>
    <author>
      <name>Egan Johnson</name>
    </author>
    <author>
      <name>Prasanth Chatarasi</name>
    </author>
    <author>
      <name>Vikram Adve</name>
    </author>
    <author>
      <name>Sasa Misailovic</name>
    </author>
    <link href="http://arxiv.org/abs/2510.08726v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.08726v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.08230v1</id>
    <updated>2025-10-09T13:55:51Z</updated>
    <published>2025-10-09T13:55:51Z</published>
    <title>pyGinkgo: A Sparse Linear Algebra Operator Framework for Python</title>
    <summary>  Sparse linear algebra is a cornerstone of many scientific computing and
machine learning applications. Python has become a popular choice for these
applications due to its simplicity and ease of use. Yet high performance sparse
kernels in Python remain limited in functionality, especially on modern CPU and
GPU architectures. We present pyGinkgo, a lightweight and Pythonic interface to
the Ginkgo library, offering high-performance sparse linear algebra support
with platform portability across CUDA, HIP, and OpenMP backends. pyGinkgo
bridges the gap between high-performance C++ backends and Python usability by
exposing Ginkgo's capabilities via Pybind11 and a NumPy and PyTorch compatible
interface. We benchmark pyGinkgo's performance against state-of-the-art Python
libraries including SciPy, CuPy, PyTorch, and TensorFlow. Results across
hardware from different vendors demonstrate that pyGinkgo consistently
outperforms existing Python tools in both sparse matrix vector (SpMV) product
and iterative solver performance, while maintaining performance parity with
native Ginkgo C++ code. Our work positions pyGinkgo as a compelling backend for
sparse machine learning models and scientific workflows.
</summary>
    <author>
      <name>Keshvi Tuteja</name>
    </author>
    <author>
      <name>Gregor Olenik</name>
    </author>
    <author>
      <name>Roman Mishchuk</name>
    </author>
    <author>
      <name>Yu-Hsiang Tsai</name>
    </author>
    <author>
      <name>Markus Götz</name>
    </author>
    <author>
      <name>Achim Streit</name>
    </author>
    <author>
      <name>Hartwig Anzt</name>
    </author>
    <author>
      <name>Charlotte Debus</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3754598.3754648</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3754598.3754648" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at the 54th International Conference on
  Parallel Processing (ICPP'25)</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.08230v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.08230v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.07356v1</id>
    <updated>2025-10-08T15:41:15Z</updated>
    <published>2025-10-08T15:41:15Z</published>
    <title>ConCuR: Conciseness Makes State-of-the-Art Kernel Generation</title>
    <summary>  GPU kernel generation by LLMs has recently experienced rapid development,
leveraging test-time scaling and reinforcement learning techniques. However, a
key challenge for kernel generation is the scarcity of high-quality data, as
most high-quality kernels are proprietary and not open-source. This challenge
prevents us from leveraging supervised fine-tuning to align LLMs to the kernel
generation task. To address this challenge, we develop a pipeline that
generates and curates high-quality CUDA kernels with reasoning traces,
motivated by a critical observation that concise yet informative reasoning
traces result in robust generation of high-performance kernels. Using this
pipeline, we construct our dataset ConCuR and introduce our model KernelCoder,
which is the first model trained on a curated dataset consisting of PyTorch,
reasoning, and CUDA kernel pairs, to our knowledge. In the KernelBench setup,
our model achieves significant improvements over the existing top-performing
model, QwQ-32B, and outperforms all open-source models fine-tuned for kernel
generation, as well as frontier models such as DeepSeek-V3.1-Think and
Claude-4-sonnet. Finally, we show that the average reasoning length can serve
as a metric to assess the difficulty of kernel generation tasks. The
observations, metrics, and our data collection and curation pipeline can help
obtain better data in the kernel generation task in the future.
</summary>
    <author>
      <name>Lingcheng Kong</name>
    </author>
    <author>
      <name>Jiateng Wei</name>
    </author>
    <author>
      <name>Hanzhang Shen</name>
    </author>
    <author>
      <name>Huan Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2510.07356v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.07356v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.06902v1</id>
    <updated>2025-10-08T11:36:09Z</updated>
    <published>2025-10-08T11:36:09Z</published>
    <title>GROMACS Unplugged: How Power Capping and Frequency Shapes Performance on
  GPUs</title>
    <summary>  Molecular dynamics simulations are essential tools in computational
biophysics, but their performance depend heavily on hardware choices and
configuration. In this work, we presents a comprehensive performance analysis
of four NVIDIA GPU accelerators -- A40, A100, L4, and L40 -- using six
representative GROMACS biomolecular workloads alongside two synthetic
benchmarks: Pi Solver (compute bound) and STREAM Triad (memory bound). We
investigate how performance scales with GPU graphics clock frequency and how
workloads respond to power capping. The two synthetic benchmarks define the
extremes of frequency scaling: Pi Solver shows ideal compute scalability, while
STREAM Triad reveals memory bandwidth limits -- framing GROMACS's performance
in context. Our results reveal distinct frequency scaling behaviors: Smaller
GROMACS systems exhibit strong frequency sensitivity, while larger systems
saturate quickly, becoming increasingly memory bound. Under power capping,
performance remains stable until architecture- and workload-specific thresholds
are reached, with high-end GPUs like the A100 maintaining near-maximum
performance even under reduced power budgets. Our findings provide practical
guidance for selecting GPU hardware and optimizing GROMACS performance for
large-scale MD workflows under power constraints.
</summary>
    <author>
      <name>Ayesha Afzal</name>
    </author>
    <author>
      <name>Anna Kahler</name>
    </author>
    <author>
      <name>Georg Hager</name>
    </author>
    <author>
      <name>Gerhard Wellein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.06902v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.06902v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.06738v1</id>
    <updated>2025-10-08T07:51:11Z</updated>
    <published>2025-10-08T07:51:11Z</published>
    <title>AWM: Accurate Weight-Matrix Fingerprint for Large Language Models</title>
    <summary>  Protecting the intellectual property of large language models (LLMs) is
crucial, given the substantial resources required for their training.
Consequently, there is an urgent need for both model owners and third parties
to determine whether a suspect LLM is trained from scratch or derived from an
existing base model. However, the intensive post-training processes that models
typically undergo-such as supervised fine-tuning, extensive continued
pretraining, reinforcement learning, multi-modal extension, pruning, and
upcycling-pose significant challenges to reliable identification. In this work,
we propose a training-free fingerprinting method based on weight matrices. We
leverage the Linear Assignment Problem (LAP) and an unbiased Centered Kernel
Alignment (CKA) similarity to neutralize the effects of parameter
manipulations, yielding a highly robust and high-fidelity similarity metric. On
a comprehensive testbed of 60 positive and 90 negative model pairs, our method
demonstrates exceptional robustness against all six aforementioned
post-training categories while exhibiting a near-zero risk of false positives.
By achieving perfect scores on all classification metrics, our approach
establishes a strong basis for reliable model lineage verification. Moreover,
the entire computation completes within 30s on an NVIDIA 3090 GPU. The code is
available at https://github.com/LUMIA-Group/AWM.
</summary>
    <author>
      <name>Boyi Zeng</name>
    </author>
    <author>
      <name>Lin Chen</name>
    </author>
    <author>
      <name>Ziwei He</name>
    </author>
    <author>
      <name>Xinbing Wang</name>
    </author>
    <author>
      <name>Zhouhan Lin</name>
    </author>
    <link href="http://arxiv.org/abs/2510.06738v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.06738v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.06175v1</id>
    <updated>2025-10-07T17:35:28Z</updated>
    <published>2025-10-07T17:35:28Z</published>
    <title>VecInfer: Efficient LLM Inference with Low-Bit KV Cache via
  Outlier-Suppressed Vector Quantization</title>
    <summary>  The Key-Value (KV) cache introduces substantial memory overhead during large
language model (LLM) inference. Although existing vector quantization (VQ)
methods reduce KV cache usage and provide flexible representational capacity
across bit-widths, they suffer severe performance degradation at ultra-low
bit-widths due to key cache outliers that hinder effective codebook
utilization. To address this challenge, we propose VecInfer, a novel VQ method
for aggressive KV cache compression while enabling efficient inference. By
applying smooth and Hadamard transformations, VecInfer suppresses outliers in
the key cache, enabling the codebook to comprehensively cover the original data
distribution and thereby reducing quantization difficulty. To facilitate
efficient deployment, we design an optimized CUDA kernel that fuses computation
with dequantization to minimize memory access overhead. Extensive evaluations
demonstrate that VecInfer consistently outperforms existing quantization
baselines across both long-context understanding and mathematical reasoning
tasks. With only 2-bit quantization, VecInfer achieves performance comparable
to full precision, while delivering up to $\mathbf{2.7\times}$ speedup in
large-batch self-attention computation and $\mathbf{8.3\times}$ reduction in
single-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length.
</summary>
    <author>
      <name>Dingyu Yao</name>
    </author>
    <author>
      <name>Chenxu Yang</name>
    </author>
    <author>
      <name>Zhengyang Tong</name>
    </author>
    <author>
      <name>Zheng Lin</name>
    </author>
    <author>
      <name>Wei Liu</name>
    </author>
    <author>
      <name>Jian Luan</name>
    </author>
    <author>
      <name>Weiping Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2510.06175v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.06175v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.05485v1</id>
    <updated>2025-10-07T01:02:46Z</updated>
    <published>2025-10-07T01:02:46Z</published>
    <title>TensorBLEU: Vectorized GPU-based BLEU Score Implementation for
  Per-Sentence In-Training Evaluation</title>
    <summary>  Modern natural language processing models have achieved unprecedented scale,
yet the tools for their evaluation often remain a computational bottleneck,
limiting the pace of research. This is particularly acute for in-training
evaluation metrics, such as per-sentence reward signals in Reinforcement
Learning, which must operate efficiently on batches of token IDs directly on
the GPU. In this paper, we introduce TensorBLEU, a novel implementation of the
BLEU metric designed from the ground up for this specific use case. Our
approach is fully vectorized for GPU-accelerated, per-sentence computation
within PyTorch and introduces a memory-efficient counting mechanism. By
creating a compact, batch-specific dictionary of n-grams using
\texttt{torch.unique}, our method avoids the prohibitive memory costs of
traditional hashing-based vectorization, making it practical for
large-vocabulary models. We benchmark TensorBLEU against NLTK, the standard
library for token-ID-based BLEU calculation on the CPU. Experiments show that
TensorBLEU provides speedups of over 13x on consumer-grade GPUs (NVIDIA T4) and
exceeding 40x on data-center-class hardware (NVIDIA A100). This performance
transforms a significant bottleneck into a negligible part of the training
loop. By clearly defining its role as a "Token-ID BLEU" for development
purposes and open-sourcing our implementation, we provide a powerful tool for
accelerating research in areas like RL-based model fine-tuning.
</summary>
    <author>
      <name>Adam Filipek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.05485v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.05485v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.04692v1</id>
    <updated>2025-10-06T11:05:46Z</updated>
    <published>2025-10-06T11:05:46Z</published>
    <title>Bio-Inspired Robotic Houbara: From Development to Field Deployment for
  Behavioral Studies</title>
    <summary>  Biomimetic intelligence and robotics are transforming field ecology by
enabling lifelike robotic surrogates that interact naturally with animals under
real world conditions. Studying avian behavior in the wild remains challenging
due to the need for highly realistic morphology, durable outdoor operation, and
intelligent perception that can adapt to uncontrolled environments. We present
a next generation bio inspired robotic platform that replicates the morphology
and visual appearance of the female Houbara bustard to support controlled
ethological studies and conservation oriented field research. The system
introduces a fully digitally replicable fabrication workflow that combines high
resolution structured light 3D scanning, parametric CAD modelling, articulated
3D printing, and photorealistic UV textured vinyl finishing to achieve
anatomically accurate and durable robotic surrogates. A six wheeled rocker
bogie chassis ensures stable mobility on sand and irregular terrain, while an
embedded NVIDIA Jetson module enables real time RGB and thermal perception,
lightweight YOLO based detection, and an autonomous visual servoing loop that
aligns the robot's head toward detected targets without human intervention. A
lightweight thermal visible fusion module enhances perception in low light
conditions. Field trials in desert aviaries demonstrated reliable real time
operation at 15 to 22 FPS with latency under 100 ms and confirmed that the
platform elicits natural recognition and interactive responses from live
Houbara bustards under harsh outdoor conditions. This integrated framework
advances biomimetic field robotics by uniting reproducible digital fabrication,
embodied visual intelligence, and ecological validation, providing a
transferable blueprint for animal robot interaction research, conservation
robotics, and public engagement.
</summary>
    <author>
      <name>Lyes Saad Saoud</name>
    </author>
    <author>
      <name>Irfan Hussain</name>
    </author>
    <link href="http://arxiv.org/abs/2510.04692v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.04692v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.04553v1</id>
    <updated>2025-10-06T07:34:21Z</updated>
    <published>2025-10-06T07:34:21Z</published>
    <title>Fast Witness Persistence for MRI Volumes via Hybrid Landmarking</title>
    <summary>  We introduce a scalable witness-based persistent homology pipeline for
full-brain MRI volumes that couples density-aware landmark selection with a
GPU-ready witness filtration. Candidates are scored by a hybrid metric that
balances geometric coverage against inverse kernel density, yielding landmark
sets that shrink mean pairwise distances by 30-60% over random or density-only
baselines while preserving topological features. Benchmarks on BrainWeb, IXI,
and synthetic manifolds execute in under ten seconds on a single NVIDIA RTX
4090 GPU, avoiding the combinatorial blow-up of Cech, Vietoris-Rips, and alpha
filtrations. The package is distributed on PyPI as whale-tda (installable via
pip); source and issues are hosted at https://github.com/jorgeLRW/whale. The
release also exposes a fast preset (mri_deep_dive_fast) for exploratory sweeps,
and ships with reproducibility-focused scripts and artifacts for drop-in use in
medical imaging workflows.
</summary>
    <author>
      <name>Jorge Leonardo Ruiz Williams</name>
    </author>
    <link href="http://arxiv.org/abs/2510.04553v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.04553v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.06263v1</id>
    <updated>2025-10-05T19:30:56Z</updated>
    <published>2025-10-05T19:30:56Z</published>
    <title>Dual-stage and Lightweight Patient Chart Summarization for Emergency
  Physicians</title>
    <summary>  Electronic health records (EHRs) contain extensive unstructured clinical data
that can overwhelm emergency physicians trying to identify critical
information. We present a two-stage summarization system that runs entirely on
embedded devices, enabling offline clinical summarization while preserving
patient privacy. In our approach, a dual-device architecture first retrieves
relevant patient record sections using the Jetson Nano-R (Retrieve), then
generates a structured summary on another Jetson Nano-S (Summarize),
communicating via a lightweight socket link. The summarization output is
two-fold: (1) a fixed-format list of critical findings, and (2) a
context-specific narrative focused on the clinician's query. The retrieval
stage uses locally stored EHRs, splits long notes into semantically coherent
sections, and searches for the most relevant sections per query. The generation
stage uses a locally hosted small language model (SLM) to produce the summary
from the retrieved text, operating within the constraints of two NVIDIA Jetson
devices. We first benchmarked six open-source SLMs under 7B parameters to
identify viable models. We incorporated an LLM-as-Judge evaluation mechanism to
assess summary quality in terms of factual accuracy, completeness, and clarity.
Preliminary results on MIMIC-IV and de-identified real EHRs demonstrate that
our fully offline system can effectively produce useful summaries in under 30
seconds.
</summary>
    <author>
      <name>Jiajun Wu</name>
    </author>
    <author>
      <name>Swaleh Zaidi</name>
    </author>
    <author>
      <name>Braden Teitge</name>
    </author>
    <author>
      <name>Henry Leung</name>
    </author>
    <author>
      <name>Jiayu Zhou</name>
    </author>
    <author>
      <name>Jessalyn Holodinsky</name>
    </author>
    <author>
      <name>Steve Drew</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the IEEE Annual Congress on Artificial Intelligence of
  Things (IEEE AIoT) 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.06263v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.06263v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.04008v1</id>
    <updated>2025-10-05T02:57:40Z</updated>
    <published>2025-10-05T02:57:40Z</published>
    <title>Replacing Softmax Similarity with a Sharpened Angular Similarity: Theory
  and Practice of Scaling To Billion-Context Attention</title>
    <summary>  Softmax Attention has a quadratic time complexity, which becomes prohibitive
to run at long contexts, even with highly optimized GPU kernels. For example,
FlashAttention (an exact, GPU-optimized implementation of Softmax Attention)
cannot complete a single forward-backward pass of a multi-head attention layer
once the context exceeds ~4 million tokens on an NVIDIA GH200 (96 GB). We
introduce RACE Attention, a kernel-inspired alternative to Softmax Attention
that is linear in sequence length and embedding dimension. RACE Attention
replaces the exponential kernel with a sharpened angular (cosine) similarity,
and approximates attention outputs via randomized projections and soft
Locality-Sensitive Hashing (LSH). Across language modeling, masked language
modeling, and text classification, RACE Attention matches the accuracy of
strong baselines while reducing runtime and memory. In a controlled scale test,
it processes up to 12 million tokens during a single forward-backward pass on
an NVIDIA GH200 GPU and 75 million tokens on an Intel Xeon Gold 5220R CPU, well
beyond the practical limits of the current state-of-the-art attention
implementations. RACE Attention thus offers a practical, theoretically grounded
mechanism for outrageously long context windows on today's hardware. We hope
that it gets adopted in practice.
</summary>
    <author>
      <name>Sahil Joshi</name>
    </author>
    <author>
      <name>Agniva Chowdhury</name>
    </author>
    <author>
      <name>Amar Kanakamedala</name>
    </author>
    <author>
      <name>Ekam Singh</name>
    </author>
    <author>
      <name>Evan Tu</name>
    </author>
    <author>
      <name>Anshumali Shrivastava</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.04008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.04008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.03872v1</id>
    <updated>2025-10-04T16:49:19Z</updated>
    <published>2025-10-04T16:49:19Z</published>
    <title>Datacenter Energy Optimized Power Profiles</title>
    <summary>  This paper presents datacenter power profiles, a new NVIDIA software feature
released with Blackwell B200, aimed at improving energy efficiency and/or
performance. The initial feature provides coarse-grain user control for HPC and
AI workloads leveraging hardware and software innovations for intelligent power
management and domain knowledge of HPC and AI workloads. The resulting
workload-aware optimization recipes maximize computational throughput while
operating within strict facility power constraints. The phase-1 Blackwell
implementation achieves up to 15% energy savings while maintaining performance
levels above 97% for critical applications, enabling an overall throughput
increase of up to 13% in a power-constrained facility.
  KEYWORDS GPU power management, energy efficiency, power profile, HPC
optimization, Max-Q, Blackwell architecture
</summary>
    <author>
      <name>Sreedhar Narayanaswamy</name>
    </author>
    <author>
      <name>Pratikkumar Dilipkumar Patel</name>
    </author>
    <author>
      <name>Ian Karlin</name>
    </author>
    <author>
      <name>Apoorv Gupta</name>
    </author>
    <author>
      <name>Sudhir Saripalli</name>
    </author>
    <author>
      <name>Janey Guo</name>
    </author>
    <link href="http://arxiv.org/abs/2510.03872v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.03872v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.03847v1</id>
    <updated>2025-10-04T15:48:04Z</updated>
    <published>2025-10-04T15:48:04Z</published>
    <title>Small Language Models for Agentic Systems: A Survey of Architectures,
  Capabilities, and Deployment Trade offs</title>
    <summary>  Small language models (SLMs; 1-12B params, sometimes up to 20B) are
sufficient and often superior for agentic workloads where the objective is
schema- and API-constrained accuracy rather than open-ended generation. We
synthesize recent evidence across open and proprietary SLMs (Phi-4-Mini,
Qwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B,
DeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4,
StableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with
guided decoding libraries (XGrammar, Outlines). We formalize SLM-default,
LLM-fallback systems with uncertainty-aware routing and verifier cascades, and
propose engineering metrics that reflect real production goals: cost per
successful task (CPS), schema validity rate, executable call rate, p50/p95
latency, and energy per request. Guided decoding, strict JSON Schema outputs,
and validator-first tool execution close much of the capability gap with larger
models and often let SLMs match or surpass LLMs on tool use, function calling,
and RAG at 10x-100x lower token cost with materially better latency and energy.
We provide design patterns for agent stacks that prioritize SLMs: schema-first
prompting, type-safe function registries, confidence scoring with verifier
rollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits
where fallback remains valuable (open-domain reasoning and some long-horizon
planning). The result is a practical blueprint for building fast, inexpensive,
and reliable agents that default to SLMs while preserving headroom with
targeted LLM assistance.
  Keywords: small language models, agents, function calling, structured
outputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency,
edge inference
</summary>
    <author>
      <name>Raghav Sharma</name>
    </author>
    <author>
      <name>Manan Mehta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 Pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.03847v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.03847v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.03760v1</id>
    <updated>2025-10-04T10:00:25Z</updated>
    <published>2025-10-04T10:00:25Z</published>
    <title>EvoEngineer: Mastering Automated CUDA Kernel Code Evolution with Large
  Language Models</title>
    <summary>  CUDA kernel optimization has become a critical bottleneck for AI performance,
as deep learning training and inference efficiency directly depends on highly
optimized GPU kernels.
  Despite the promise of Large Language Models (LLMs) for automating kernel
optimization, this field suffers from a fragmented ecosystem of isolated and
incomparable approaches with unclear problem formulations.
  Furthermore, general-purpose LLM code evolution methods cannot meet strict
correctness requirements of CUDA kernel optimization.
  We address these fundamental challenges by first formalizing CUDA kernel
optimization as a code optimization task with a clear objective, constraints,
and evaluation metrics.
  We then establish the first systematic LLM-based code evolution framework,
EvoEngineer, that provides guidance for designing and adapting optimization
strategies to achieve a balance between performance and correctness.
  Finally, we implement a kernel optimization system based on this framework
and conduct extensive experiments on 91 real-world CUDA kernels.
  Our results demonstrate that EvoEngineer achieves a principled balance
between performance and correctness, with the highest averaged median speedup
of \textbf{2.72}$\times$ over baseline CUDA kernels and a code validity rate of
\textbf{69.8}\%, outperforming existing methods on both dimensions.
  Our method achieves a maximum speedup of \textbf{36.75}$\times$ among all
operations over PyTorch kernels and delivers the highest speedup on \textbf{28}
(\textbf{56.0\%}) of 50 operations that achieve over \textbf{2$\times$}
acceleration.
</summary>
    <author>
      <name>Ping Guo</name>
    </author>
    <author>
      <name>Chenyu Zhu</name>
    </author>
    <author>
      <name>Siyuan Chen</name>
    </author>
    <author>
      <name>Fei Liu</name>
    </author>
    <author>
      <name>Xi Lin</name>
    </author>
    <author>
      <name>Zhichao Lu</name>
    </author>
    <author>
      <name>Qingfu Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under Review of ICLR 2026</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.03760v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.03760v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.02894v1</id>
    <updated>2025-10-03T11:00:31Z</updated>
    <published>2025-10-03T11:00:31Z</published>
    <title>PyRadiomics-cuda: a GPU-accelerated 3D features extraction from medical
  images within PyRadiomics</title>
    <summary>  PyRadiomics-cuda is a GPU-accelerated extension of the PyRadiomics library,
designed to address the computational challenges of extracting
three-dimensional shape features from medical images. By offloading key
geometric computations to GPU hardware it dramatically reduces processing times
for large volumetric datasets. The system maintains full compatibility with the
original PyRadiomics API, enabling seamless integration into existing AI
workflows without code modifications. This transparent acceleration facilitates
efficient, scalable radiomics analysis, supporting rapid feature extraction
essential for high-throughput AI pipeline. Tests performed on a typical
computational cluster, budget and home devices prove usefulness in all
scenarios. PyRadiomics-cuda is implemented in Python and C/CUDA and is freely
available under the BSD license at https://github.com/mis-wut/pyradiomics-CUDA
Additionally PyRadiomics-cuda test suite is available at
https://github.com/mis-wut/pyradiomics-cuda-data-gen. It provides detailed
handbook and sample scripts suited for different kinds of workflows plus
detailed installation instructions. The dataset used for testing is available
at Kaggle
https://www.kaggle.com/datasets/sabahesaraki/kidney-tumor-segmentation-challengekits-19
</summary>
    <author>
      <name>Jakub Lisowski</name>
    </author>
    <author>
      <name>Piotr Tyrakowski</name>
    </author>
    <author>
      <name>Szymon Zyguła</name>
    </author>
    <author>
      <name>Krzysztof Kaczmarski</name>
    </author>
    <link href="http://arxiv.org/abs/2510.02894v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.02894v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.02878v1</id>
    <updated>2025-10-03T10:35:14Z</updated>
    <published>2025-10-03T10:35:14Z</published>
    <title>On the energy efficiency of sparse matrix computations on multi-GPU
  clusters</title>
    <summary>  We investigate the energy efficiency of a library designed for parallel
computations with sparse matrices. The library leverages high-performance,
energy-efficient Graphics Processing Unit (GPU) accelerators to enable
large-scale scientific applications. Our primary development objective was to
maximize parallel performance and scalability in solving sparse linear systems
whose dimensions far exceed the memory capacity of a single node. To this end,
we devised methods that expose a high degree of parallelism while optimizing
algorithmic implementations for efficient multi-GPU usage. Previous work has
already demonstrated the library's performance efficiency on large-scale
systems comprising thousands of NVIDIA GPUs, achieving improvements over
state-of-the-art solutions. In this paper, we extend those results by providing
energy profiles that address the growing sustainability requirements of modern
HPC platforms. We present our methodology and tools for accurate runtime energy
measurements of the library's core components and discuss the findings. Our
results confirm that optimizing GPU computations and minimizing data movement
across memory and computing nodes reduces both time-to-solution and energy
consumption. Moreover, we show that the library delivers substantial advantages
over comparable software frameworks on standard benchmarks.
</summary>
    <author>
      <name>Massimo Bernaschi</name>
    </author>
    <author>
      <name>Alessandro Celestini</name>
    </author>
    <author>
      <name>Pasqua D'Ambra</name>
    </author>
    <author>
      <name>Giorgio Richelli</name>
    </author>
    <link href="http://arxiv.org/abs/2510.02878v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.02878v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.02264v1</id>
    <updated>2025-10-02T17:44:31Z</updated>
    <published>2025-10-02T17:44:31Z</published>
    <title>Paving the Way Towards Kinematic Assessment Using Monocular Video: A
  Preclinical Benchmark of State-of-the-Art Deep-Learning-Based 3D Human Pose
  Estimators Against Inertial Sensors in Daily Living Activities</title>
    <summary>  Advances in machine learning and wearable sensors offer new opportunities for
capturing and analyzing human movement outside specialized laboratories.
Accurate assessment of human movement under real-world conditions is essential
for telemedicine, sports science, and rehabilitation. This preclinical
benchmark compares monocular video-based 3D human pose estimation models with
inertial measurement units (IMUs), leveraging the VIDIMU dataset containing a
total of 13 clinically relevant daily activities which were captured using both
commodity video cameras and five IMUs. During this initial study only healthy
subjects were recorded, so results cannot be generalized to pathological
cohorts. Joint angles derived from state-of-the-art deep learning frameworks
(MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, and NVIDIA
BodyTrack) were evaluated against joint angles computed from IMU data using
OpenSim inverse kinematics following the Human3.6M dataset format with 17
keypoints. Among them, MotionAGFormer demonstrated superior performance,
achieving the lowest overall RMSE ($9.27\deg \pm 4.80\deg$) and MAE ($7.86\deg
\pm 4.18\deg$), as well as the highest Pearson correlation ($0.86 \pm 0.15$)
and the highest coefficient of determination $R^{2}$ ($0.67 \pm 0.28$). The
results reveal that both technologies are viable for out-of-the-lab kinematic
assessment. However, they also highlight key trade-offs between video- and
sensor-based approaches including costs, accessibility, and precision. This
study clarifies where off-the-shelf video models already provide clinically
promising kinematics in healthy adults and where they lag behind IMU-based
estimates while establishing valuable guidelines for researchers and clinicians
seeking to develop robust, cost-effective, and user-friendly solutions for
telehealth and remote patient monitoring.
</summary>
    <author>
      <name>Mario Medrano-Paredes</name>
    </author>
    <author>
      <name>Carmen Fernández-González</name>
    </author>
    <author>
      <name>Francisco-Javier Díaz-Pernas</name>
    </author>
    <author>
      <name>Hichem Saoudi</name>
    </author>
    <author>
      <name>Javier González-Alonso</name>
    </author>
    <author>
      <name>Mario Martínez-Zarzuela</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">All tables, graphs and figures generated can be obtained in the
  Zenodo repository complementary to this work:
  https://doi.org/10.5281/zenodo.15088423</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.02264v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.02264v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.01730v1</id>
    <updated>2025-10-02T07:14:54Z</updated>
    <published>2025-10-02T07:14:54Z</published>
    <title>Edge GPU Aware Multiple AI Model Pipeline for Accelerated MRI
  Reconstruction and Analysis</title>
    <summary>  Advancements in AI have greatly enhanced the medical imaging process, making
it quicker to diagnose patients. However, very few have investigated the
optimization of a multi-model system with hardware acceleration. As specialized
edge devices emerge, the efficient use of their accelerators is becoming
increasingly crucial. This paper proposes a hardware-accelerated method for
simultaneous reconstruction and diagnosis of \ac{MRI} from \ac{CT} images.
Real-time performance of achieving a throughput of nearly 150 frames per second
was achieved by leveraging hardware engines available in modern NVIDIA edge
GPU, along with scheduling techniques. This includes the GPU and the \ac{DLA}
available in both Jetson AGX Xavier and Jetson AGX Orin, which were considered
in this paper. The hardware allocation of different layers of the multiple AI
models was done in such a way that the ideal time between the hardware engines
is reduced. In addition, the AI models corresponding to the \ac{GAN} model were
fine-tuned in such a way that no fallback execution into the GPU engine is
required without compromising accuracy. Indeed, the accuracy corresponding to
the fine-tuned edge GPU-aware AI models exhibited an accuracy enhancement of
5\%. A further hardware allocation of two fine-tuned GPU-aware GAN models
proves they can double the performance over the original model, leveraging
adequate partitioning on the NVIDIA Jetson AGX Xavier and Orin devices. The
results prove the effectiveness of employing hardware-aware models in parallel
for medical image analysis and diagnosis.
</summary>
    <author>
      <name>Ashiyana Abdul Majeed</name>
    </author>
    <author>
      <name>Mahmoud Meribout</name>
    </author>
    <author>
      <name>Safa Mohammed Sali</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages. 14 figures. This work has been submitted to IEEE for
  possible publication</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.01730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.01730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.01533v1</id>
    <updated>2025-10-02T00:10:20Z</updated>
    <published>2025-10-02T00:10:20Z</published>
    <title>NVIDIA AI Aerial: AI-Native Wireless Communications</title>
    <summary>  6G brings a paradigm shift towards AI-native wireless systems, necessitating
the seamless integration of digital signal processing (DSP) and machine
learning (ML) within the software stacks of cellular networks. This
transformation brings the life cycle of modern networks closer to AI systems,
where models and algorithms are iteratively trained, simulated, and deployed
across adjacent environments. In this work, we propose a robust framework that
compiles Python-based algorithms into GPU-runnable blobs. The result is a
unified approach that ensures efficiency, flexibility, and the highest possible
performance on NVIDIA GPUs. As an example of the capabilities of the framework,
we demonstrate the efficacy of performing the channel estimation function in
the PUSCH receiver through a convolutional neural network (CNN) trained in
Python. This is done in a digital twin first, and subsequently in a real-time
testbed. Our proposed methodology, realized in the NVIDIA AI Aerial platform,
lays the foundation for scalable integration of AI/ML models into
next-generation cellular systems, and is essential for realizing the vision of
natively intelligent 6G networks.
</summary>
    <author>
      <name>Kobi Cohen-Arazi</name>
    </author>
    <author>
      <name>Michael Roe</name>
    </author>
    <author>
      <name>Zhen Hu</name>
    </author>
    <author>
      <name>Rohan Chavan</name>
    </author>
    <author>
      <name>Anna Ptasznik</name>
    </author>
    <author>
      <name>Joanna Lin</name>
    </author>
    <author>
      <name>Joao Morais</name>
    </author>
    <author>
      <name>Joseph Boccuzzi</name>
    </author>
    <author>
      <name>Tommaso Balercia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.01533v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.01533v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.01290v1</id>
    <updated>2025-10-01T04:09:02Z</updated>
    <published>2025-10-01T04:09:02Z</published>
    <title>ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning
  Models</title>
    <summary>  The long-output context generation of large reasoning models enables extended
chain of thought (CoT) but also drives rapid growth of the key-value (KV)
cache, quickly overwhelming GPU memory. To address this challenge, we propose
ThinKV, a thought-adaptive KV cache compression framework. ThinKV is based on
the observation that attention sparsity reveals distinct thought types with
varying importance within the CoT. It applies a hybrid quantization-eviction
strategy, assigning token precision by thought importance and progressively
evicting tokens from less critical thoughts as reasoning trajectories evolve.
Furthermore, to implement ThinKV, we design a kernel that extends
PagedAttention to enable efficient reuse of evicted tokens' memory slots,
eliminating compaction overheads. Extensive experiments on DeepSeek-R1-Distill,
GPT-OSS, and NVIDIA AceReason across mathematics and coding benchmarks show
that ThinKV achieves near-lossless accuracy with less than 5% of the original
KV cache, while improving performance with up to 5.8x higher inference
throughput over state-of-the-art baselines.
</summary>
    <author>
      <name>Akshat Ramachandran</name>
    </author>
    <author>
      <name>Marina Neseem</name>
    </author>
    <author>
      <name>Charbel Sakr</name>
    </author>
    <author>
      <name>Rangharajan Venkatesan</name>
    </author>
    <author>
      <name>Brucek Khailany</name>
    </author>
    <author>
      <name>Tushar Krishna</name>
    </author>
    <link href="http://arxiv.org/abs/2510.01290v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.01290v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.00392v1</id>
    <updated>2025-10-01T01:07:35Z</updated>
    <published>2025-10-01T01:07:35Z</published>
    <title>A Deep Learning Pipeline for Epilepsy Genomic Analysis Using GPT-2 XL
  and NVIDIA H100</title>
    <summary>  Epilepsy is a chronic neurological condition characterized by recurrent
seizures, with global prevalence estimated at 50 million people worldwide.
While progress in high-throughput sequencing has allowed for broad-based
transcriptomic profiling of brain tissues, the deciphering of these highly
complex datasets remains one of the challenges. To address this issue, in this
paper we propose a new analysis pipeline that integrates the power of deep
learning strategies with GPU-acceleration computation for investigating Gene
expression patterns in epilepsy. Specifically, our proposed approach employs
GPT-2 XL, a transformer-based Large Language Model (LLM) with 1.5 billion
parameters for genomic sequence analysis over the latest NVIDIA H100 Tensor
Core GPUs based on Hopper architecture. Our proposed method enables efficient
preprocessing of RNA sequence data, gene sequence encoding, and subsequent
pattern identification. We conducted experiments on two epilepsy datasets
including GEO accession GSE264537 and GSE275235. The obtained results reveal
several significant transcriptomic modifications, including reduced hippocampal
astrogliosis after ketogenic diet treatment as well as restored
excitatory-inhibitory signaling equilibrium in zebrafish epilepsy model.
Moreover, our results highlight the effectiveness of leveraging LLMs in
combination with advanced hardware acceleration for transcriptomic
characterization in neurological diseases.
</summary>
    <author>
      <name>Muhammad Omer Latif</name>
    </author>
    <author>
      <name>Hayat Ullah</name>
    </author>
    <author>
      <name>Muhammad Ali Shafique</name>
    </author>
    <author>
      <name>Zhihua Dong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.00392v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.00392v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.03312v1</id>
    <updated>2025-09-30T22:03:22Z</updated>
    <published>2025-09-30T22:03:22Z</published>
    <title>Universal Beta Splatting</title>
    <summary>  We introduce Universal Beta Splatting (UBS), a unified framework that
generalizes 3D Gaussian Splatting to N-dimensional anisotropic Beta kernels for
explicit radiance field rendering. Unlike fixed Gaussian primitives, Beta
kernels enable controllable dependency modeling across spatial, angular, and
temporal dimensions within a single representation. Our unified approach
captures complex light transport effects, handles anisotropic view-dependent
appearance, and models scene dynamics without requiring auxiliary networks or
specific color encodings. UBS maintains backward compatibility by approximating
to Gaussian Splatting as a special case, guaranteeing plug-in usability and
lower performance bounds. The learned Beta parameters naturally decompose scene
properties into interpretable without explicit supervision: spatial (surface
vs. texture), angular (diffuse vs. specular), and temporal (static vs.
dynamic). Our CUDA-accelerated implementation achieves real-time rendering
while consistently outperforming existing methods across static,
view-dependent, and dynamic benchmarks, establishing Beta kernels as a scalable
universal primitive for radiance field rendering. Our project website is
available at https://rongliu-leo.github.io/universal-beta-splatting/.
</summary>
    <author>
      <name>Rong Liu</name>
    </author>
    <author>
      <name>Zhongpai Gao</name>
    </author>
    <author>
      <name>Benjamin Planche</name>
    </author>
    <author>
      <name>Meida Chen</name>
    </author>
    <author>
      <name>Van Nguyen Nguyen</name>
    </author>
    <author>
      <name>Meng Zheng</name>
    </author>
    <author>
      <name>Anwesa Choudhuri</name>
    </author>
    <author>
      <name>Terrence Chen</name>
    </author>
    <author>
      <name>Yue Wang</name>
    </author>
    <author>
      <name>Andrew Feng</name>
    </author>
    <author>
      <name>Ziyan Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2510.03312v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.03312v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.26541v2</id>
    <updated>2025-10-09T13:03:29Z</updated>
    <published>2025-09-30T17:15:27Z</published>
    <title>TASP: Topology-aware Sequence Parallelism</title>
    <summary>  Long-context large language models (LLMs) face constraints due to the
quadratic complexity of the self-attention mechanism. The mainstream sequence
parallelism (SP) method, Ring Attention, attempts to solve this by distributing
the query into multiple query chunks across accelerators and enable each Q
tensor to access all KV tensors from other accelerators via the Ring AllGather
communication primitive. However, it exhibits low communication efficiency,
restricting its practical applicability. This inefficiency stems from the
mismatch between the Ring AllGather communication primitive it adopts and the
AlltoAll topology of modern accelerators. A Ring AllGather primitive is
composed of iterations of ring-styled data transfer, which can only utilize a
very limited fraction of an AlltoAll topology.
  Inspired by the Hamiltonian decomposition of complete directed graphs, we
identify that modern accelerator topology can be decomposed into multiple
orthogonal ring datapaths which can concurrently transfer data without
interference. Based on this, we further observe that the Ring AllGather
primitive can also be decomposed into the same number of concurrent ring-styled
data transfer at every iteration. Based on these insights, we propose TASP, a
topology-aware SP method for long-context LLMs that fully utilizes the
communication capacity of modern accelerators via topology decomposition and
primitive decomposition. Experimental results on both single-node and
multi-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate
that TASP achieves higher communication efficiency than Ring Attention on these
modern accelerator topologies and achieves up to 3.58 speedup than Ring
Attention and its variant Zigzag-Ring Attention. The code is available at
https://github.com/infinigence/HamiltonAttention.
</summary>
    <author>
      <name>Yida Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Capital Normal University</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Infinigence-AI</arxiv:affiliation>
    </author>
    <author>
      <name>Ke Hong</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tsinghua University</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Infinigence-AI</arxiv:affiliation>
    </author>
    <author>
      <name>Xiuhong Li</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Infinigence-AI</arxiv:affiliation>
    </author>
    <author>
      <name>Yuanchao Xu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Capital Normal University</arxiv:affiliation>
    </author>
    <author>
      <name>Wenxun Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tsinghua University</arxiv:affiliation>
    </author>
    <author>
      <name>Guohao Dai</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Infinigence-AI</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shanghai Jiao Tong University</arxiv:affiliation>
    </author>
    <author>
      <name>Yu Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tsinghua University</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2509.26541v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.26541v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.26219v1</id>
    <updated>2025-09-30T13:19:05Z</updated>
    <published>2025-09-30T13:19:05Z</published>
    <title>Beyond Pixels: Efficient Dataset Distillation via Sparse Gaussian
  Representation</title>
    <summary>  Dataset distillation has emerged as a promising paradigm that synthesizes
compact, informative datasets capable of retaining the knowledge of large-scale
counterparts, thereby addressing the substantial computational and storage
burdens of modern model training. Conventional approaches typically rely on
dense pixel-level representations, which introduce redundancy and are difficult
to scale up. In this work, we propose GSDD, a novel and efficient sparse
representation for dataset distillation based on 2D Gaussians. Instead of
representing all pixels equally, GSDD encodes critical discriminative
information in a distilled image using only a small number of Gaussian
primitives. This sparse representation could improve dataset diversity under
the same storage budget, enhancing coverage of difficult samples and boosting
distillation performance. To ensure both efficiency and scalability, we adapt
CUDA-based splatting operators for parallel inference and training, enabling
high-quality rendering with minimal computational and memory overhead. Our
method is simple yet effective, broadly applicable to different distillation
pipelines, and highly scalable. Experiments show that GSDD achieves
state-of-the-art performance on CIFAR-10, CIFAR-100, and ImageNet subsets,
while remaining highly efficient encoding and decoding cost. Our code is
available at https://github.com/j-cyoung/GSDatasetDistillation.
</summary>
    <author>
      <name>Chenyang Jiang</name>
    </author>
    <author>
      <name>Zhengcen Li</name>
    </author>
    <author>
      <name>Hang Zhao</name>
    </author>
    <author>
      <name>Qiben Shan</name>
    </author>
    <author>
      <name>Shaocong Wu</name>
    </author>
    <author>
      <name>Jingyong Su</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages; Code is available on
  https://github.com/j-cyoung/GSDatasetDistillation</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.26219v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.26219v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0; I.4.2; I.4.10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.26217v1</id>
    <updated>2025-09-30T13:19:00Z</updated>
    <published>2025-09-30T13:19:00Z</published>
    <title>Benchmarking Deep Learning Convolutions on Energy-constrained CPUs</title>
    <summary>  This work evaluates state-of-the-art convolution algorithms for CPU-based
deep learning inference. While most prior studies focus on GPUs or NPUs, CPU
implementations remain relatively underoptimized. We benchmark direct,
GEMM-based, and Winograd convolutions across modern CPUs from ARM __ , Intel __
, AMD __ , Apple __ , and Nvidia __ , considering both latency and energy
efficiency. Our results highlight the key architectural factors that govern CPU
efficiency for convolution operations, providing practical guidance for
energy-aware embedded deployment. As a main results of this work, the Nvidia __
AGX Orin combined with the GEMM algorithm achieves the best trade-off between
inference latency and energy consumption.
</summary>
    <author>
      <name>Enrique Galvez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ALSOC</arxiv:affiliation>
    </author>
    <author>
      <name>Adrien Cassagne</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ALSOC</arxiv:affiliation>
    </author>
    <author>
      <name>Alix Munier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ALSOC</arxiv:affiliation>
    </author>
    <author>
      <name>Manuel Bouyer</name>
    </author>
    <link href="http://arxiv.org/abs/2509.26217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.26217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.25853v1</id>
    <updated>2025-09-30T06:43:59Z</updated>
    <published>2025-09-30T06:43:59Z</published>
    <title>SAIL: SRAM-Accelerated LLM Inference System with Lookup-Table-based GEMV</title>
    <summary>  Large Language Model (LLM) inference requires substantial computational
resources, yet CPU-based inference remains essential for democratizing AI due
to the widespread availability of CPUs compared to specialized accelerators.
However, efficient LLM inference on CPUs faces two fundamental challenges: (1)
existing CPU architectures struggle with low-precision arithmetic required by
quantized models, where optimal bit precision varies across models and layers;
and (2) the memory-bound nature of the token generation phase creates severe
performance bottlenecks. To address these challenges, we propose SAIL
(SRAM-Accelerated Inference of LLMs), a CPU-based inference solution that
efficiently supports arbitrary bit precisions with minimal overhead. SAIL
integrates three key innovations: First, we introduce Batched LUT-based General
Matrix-Vector Multiplication (LUT-GEMV) with SRAM-based processing-in-memory,
enabling high data reuse through lookup tables and reducing memory movement.
Second, our Pattern-Aware LUT optimization identifies and exploits redundancy
in input activation patterns, reducing computation cycles by 13.8\%. Third, we
develop an in-memory type conversion algorithm that leverages PIM's parallelism
for efficient de-/quantization operations, alleviating pressure on CPU's vector
units. Our architecture requires only 2\% hardware overhead and a single new
instruction, while maintaining dual functionality as both compute and storage
units. Experimental evaluations using a modified gem5 simulator demonstrate
that SAIL achieves up to 10.7x speedup and 19.9x higher tokens per dollar
compared to ARM Neoverse-N1 CPU baselines, and up to 7.04x better cost
efficiency than NVIDIA V100 GPUs, establishing a practical path for efficient
CPU-based LLM inference.
</summary>
    <author>
      <name>Jingyao Zhang</name>
    </author>
    <author>
      <name>Jaewoo Park</name>
    </author>
    <author>
      <name>Jongeun Lee</name>
    </author>
    <author>
      <name>Elaheh Sadredini</name>
    </author>
    <link href="http://arxiv.org/abs/2509.25853v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.25853v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.03297v1</id>
    <updated>2025-09-29T21:41:22Z</updated>
    <published>2025-09-29T21:41:22Z</published>
    <title>Convolutional Neural Nets vs Vision Transformers: A SpaceNet Case Study
  with Balanced vs Imbalanced Regimes</title>
    <summary>  We present a controlled comparison of a convolutional neural network
(EfficientNet-B0) and a Vision Transformer (ViT-Base) on SpaceNet under two
label-distribution regimes: a naturally imbalanced five-class split and a
balanced-resampled split with 700 images per class (70:20:10 train/val/test).
With matched preprocessing (224x224, ImageNet normalization), lightweight
augmentations, and a 40-epoch budget on a single NVIDIA P100, we report
accuracy, macro-F1, balanced accuracy, per-class recall, and deployment metrics
(model size and latency). On the imbalanced split, EfficientNet-B0 reaches 93%
test accuracy with strong macro-F1 and lower latency; ViT-Base is competitive
at 93% with a larger parameter count and runtime. On the balanced split, both
models are strong; EfficientNet-B0 reaches 99% while ViT-Base remains
competitive, indicating that balancing narrows architecture gaps while CNNs
retain an efficiency edge. We release manifests, logs, and per-image
predictions to support reproducibility.
</summary>
    <author>
      <name>Akshar Gothi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure, 9 tables. Code and artifacts:
  https://github.com/akshar27/spacenet-cnn-vs-vit (release v1.0.1)</arxiv:comment>
    <link href="http://arxiv.org/abs/2510.03297v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2510.03297v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.25182v1</id>
    <updated>2025-09-29T17:59:31Z</updated>
    <published>2025-09-29T17:59:31Z</published>
    <title>DC-VideoGen: Efficient Video Generation with Deep Compression Video
  Autoencoder</title>
    <summary>  We introduce DC-VideoGen, a post-training acceleration framework for
efficient video generation. DC-VideoGen can be applied to any pre-trained video
diffusion model, improving efficiency by adapting it to a deep compression
latent space with lightweight fine-tuning. The framework builds on two key
innovations: (i) a Deep Compression Video Autoencoder with a novel chunk-causal
temporal design that achieves 32x/64x spatial and 4x temporal compression while
preserving reconstruction quality and generalization to longer videos; and (ii)
AE-Adapt-V, a robust adaptation strategy that enables rapid and stable transfer
of pre-trained models into the new latent space. Adapting the pre-trained
Wan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100
GPU. The accelerated models achieve up to 14.8x lower inference latency than
their base counterparts without compromising quality, and further enable
2160x3840 video generation on a single GPU. Code:
https://github.com/dc-ai-projects/DC-VideoGen.
</summary>
    <author>
      <name>Junyu Chen</name>
    </author>
    <author>
      <name>Wenkun He</name>
    </author>
    <author>
      <name>Yuchao Gu</name>
    </author>
    <author>
      <name>Yuyang Zhao</name>
    </author>
    <author>
      <name>Jincheng Yu</name>
    </author>
    <author>
      <name>Junsong Chen</name>
    </author>
    <author>
      <name>Dongyun Zou</name>
    </author>
    <author>
      <name>Yujun Lin</name>
    </author>
    <author>
      <name>Zhekai Zhang</name>
    </author>
    <author>
      <name>Muyang Li</name>
    </author>
    <author>
      <name>Haocheng Xi</name>
    </author>
    <author>
      <name>Ligeng Zhu</name>
    </author>
    <author>
      <name>Enze Xie</name>
    </author>
    <author>
      <name>Song Han</name>
    </author>
    <author>
      <name>Han Cai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Tech Report. The first three authors contributed equally to this work</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.25182v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.25182v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.25180v2</id>
    <updated>2025-10-01T02:18:37Z</updated>
    <published>2025-09-29T17:59:25Z</published>
    <title>DC-Gen: Post-Training Diffusion Acceleration with Deeply Compressed
  Latent Space</title>
    <summary>  Existing text-to-image diffusion models excel at generating high-quality
images, but face significant efficiency challenges when scaled to high
resolutions, like 4K image generation. While previous research accelerates
diffusion models in various aspects, it seldom handles the inherent redundancy
within the latent space. To bridge this gap, this paper introduces DC-Gen, a
general framework that accelerates text-to-image diffusion models by leveraging
a deeply compressed latent space. Rather than a costly training-from-scratch
approach, DC-Gen uses an efficient post-training pipeline to preserve the
quality of the base model. A key challenge in this paradigm is the
representation gap between the base model's latent space and a deeply
compressed latent space, which can lead to instability during direct
fine-tuning. To overcome this, DC-Gen first bridges the representation gap with
a lightweight embedding alignment training. Once the latent embeddings are
aligned, only a small amount of LoRA fine-tuning is needed to unlock the base
model's inherent generation quality. We verify DC-Gen's effectiveness on SANA
and FLUX.1-Krea. The resulting DC-Gen-SANA and DC-Gen-FLUX models achieve
quality comparable to their base models but with a significant speedup.
Specifically, DC-Gen-FLUX reduces the latency of 4K image generation by 53x on
the NVIDIA H100 GPU. When combined with NVFP4 SVDQuant, DC-Gen-FLUX generates a
4K image in just 3.5 seconds on a single NVIDIA 5090 GPU, achieving a total
latency reduction of 138x compared to the base FLUX.1-Krea model. Code:
https://github.com/dc-ai-projects/DC-Gen.
</summary>
    <author>
      <name>Wenkun He</name>
    </author>
    <author>
      <name>Yuchao Gu</name>
    </author>
    <author>
      <name>Junyu Chen</name>
    </author>
    <author>
      <name>Dongyun Zou</name>
    </author>
    <author>
      <name>Yujun Lin</name>
    </author>
    <author>
      <name>Zhekai Zhang</name>
    </author>
    <author>
      <name>Haocheng Xi</name>
    </author>
    <author>
      <name>Muyang Li</name>
    </author>
    <author>
      <name>Ligeng Zhu</name>
    </author>
    <author>
      <name>Jincheng Yu</name>
    </author>
    <author>
      <name>Junsong Chen</name>
    </author>
    <author>
      <name>Enze Xie</name>
    </author>
    <author>
      <name>Song Han</name>
    </author>
    <author>
      <name>Han Cai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Tech Report. The first three authors contributed equally to this work</arxiv:comment>
    <link href="http://arxiv.org/abs/2509.25180v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.25180v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.25164v2</id>
    <updated>2025-09-30T03:10:22Z</updated>
    <published>2025-09-29T17:58:04Z</published>
    <title>YOLO26: Key Architectural Enhancements and Performance Benchmarking for
  Real-Time Object Detection</title>
    <summary>  This study presents a comprehensive analysis of Ultralytics YOLO26,
highlighting its key architectural enhancements and performance benchmarking
for real-time object detection. YOLO26, released in September 2025, stands as
the newest and most advanced member of the YOLO family, purpose-built to
deliver efficiency, accuracy, and deployment readiness on edge and low-power
devices. The paper sequentially details architectural innovations of YOLO26,
including the removal of Distribution Focal Loss (DFL), adoption of end-to-end
NMS-free inference, integration of ProgLoss and Small-Target-Aware Label
Assignment (STAL), and the introduction of the MuSGD optimizer for stable
convergence. Beyond architecture, the study positions YOLO26 as a multi-task
framework, supporting object detection, instance segmentation, pose/keypoints
estimation, oriented detection, and classification. We present performance
benchmarks of YOLO26 on edge devices such as NVIDIA Jetson Nano and Orin,
comparing its results with YOLOv8, YOLOv11, YOLOv12, YOLOv13, and
transformer-based detectors(RF-DETR and RT-DETR). This paper further explores
real-time deployment pathways, flexible export options (ONNX, TensorRT, CoreML,
TFLite), and quantization for INT8/FP16. Practical use cases of YOLO26 across
robotics, manufacturing, and IoT are highlighted to demonstrate cross-industry
adaptability. Finally, insights on deployment efficiency and broader
implications are discussed, with future directions for YOLO26 and the YOLO
lineage outlined.
</summary>
    <author>
      <name>Ranjan Sapkota</name>
    </author>
    <author>
      <name>Rahul Harsha Cheppally</name>
    </author>
    <author>
      <name>Ajay Sharda</name>
    </author>
    <author>
      <name>Manoj Karkee</name>
    </author>
    <link href="http://arxiv.org/abs/2509.25164v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.25164v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.25149v1</id>
    <updated>2025-09-29T17:53:17Z</updated>
    <published>2025-09-29T17:53:17Z</published>
    <title>Pretraining Large Language Models with NVFP4</title>
    <summary>  Large Language Models (LLMs) today are powerful problem solvers across many
domains, and they continue to get stronger as they scale in model size,
training set size, and training set quality, as shown by extensive research and
experimentation across the industry. Training a frontier model today requires
on the order of tens to hundreds of yottaflops, which is a massive investment
of time, compute, and energy. Improving pretraining efficiency is therefore
essential to enable the next generation of even more capable LLMs. While 8-bit
floating point (FP8) training is now widely adopted, transitioning to even
narrower precision, such as 4-bit floating point (FP4), could unlock additional
improvements in computational speed and resource utilization. However,
quantization at this level poses challenges to training stability, convergence,
and implementation, notably for large-scale models trained on long token
horizons.
  In this study, we introduce a novel approach for stable and accurate training
of large language models (LLMs) using the NVFP4 format. Our method integrates
Random Hadamard transforms (RHT) to bound block-level outliers, employs a
two-dimensional quantization scheme for consistent representations across both
the forward and backward passes, utilizes stochastic rounding for unbiased
gradient estimation, and incorporates selective high-precision layers. We
validate our approach by training a 12-billion-parameter model on 10 trillion
tokens -- the longest publicly documented training run in 4-bit precision to
date. Our results show that the model trained with our NVFP4-based pretraining
technique achieves training loss and downstream task accuracies comparable to
an FP8 baseline. These findings highlight that NVFP4, when combined with our
training approach, represents a major step forward in narrow-precision LLM
training algorithms.
</summary>
    <author>
      <name> NVIDIA</name>
    </author>
    <author>
      <name>Felix Abecassis</name>
    </author>
    <author>
      <name>Anjulie Agrusa</name>
    </author>
    <author>
      <name>Dong Ahn</name>
    </author>
    <author>
      <name>Jonah Alben</name>
    </author>
    <author>
      <name>Stefania Alborghetti</name>
    </author>
    <author>
      <name>Michael Andersch</name>
    </author>
    <author>
      <name>Sivakumar Arayandi</name>
    </author>
    <author>
      <name>Alexis Bjorlin</name>
    </author>
    <author>
      <name>Aaron Blakeman</name>
    </author>
    <author>
      <name>Evan Briones</name>
    </author>
    <author>
      <name>Ian Buck</name>
    </author>
    <author>
      <name>Bryan Catanzaro</name>
    </author>
    <author>
      <name>Jinhang Choi</name>
    </author>
    <author>
      <name>Mike Chrzanowski</name>
    </author>
    <author>
      <name>Eric Chung</name>
    </author>
    <author>
      <name>Victor Cui</name>
    </author>
    <author>
      <name>Steve Dai</name>
    </author>
    <author>
      <name>Bita Darvish Rouhani</name>
    </author>
    <author>
      <name>Carlo del Mundo</name>
    </author>
    <author>
      <name>Deena Donia</name>
    </author>
    <author>
      <name>Burc Eryilmaz</name>
    </author>
    <author>
      <name>Henry Estela</name>
    </author>
    <author>
      <name>Abhinav Goel</name>
    </author>
    <author>
      <name>Oleg Goncharov</name>
    </author>
    <author>
      <name>Yugi Guvvala</name>
    </author>
    <author>
      <name>Robert Hesse</name>
    </author>
    <author>
      <name>Russell Hewett</name>
    </author>
    <author>
      <name>Herbert Hum</name>
    </author>
    <author>
      <name>Ujval Kapasi</name>
    </author>
    <author>
      <name>Brucek Khailany</name>
    </author>
    <author>
      <name>Mikail Khona</name>
    </author>
    <author>
      <name>Nick Knight</name>
    </author>
    <author>
      <name>Alex Kondratenko</name>
    </author>
    <author>
      <name>Ronny Krashinsky</name>
    </author>
    <author>
      <name>Ben Lanir</name>
    </author>
    <author>
      <name>Simon Layton</name>
    </author>
    <author>
      <name>Michael Lightstone</name>
    </author>
    <author>
      <name>Daniel Lo</name>
    </author>
    <author>
      <name>Paulius Micikevicius</name>
    </author>
    <author>
      <name>Asit Mishra</name>
    </author>
    <author>
      <name>Tim Moon</name>
    </author>
    <author>
      <name>Deepak Narayanan</name>
    </author>
    <author>
      <name>Chao Ni</name>
    </author>
    <author>
      <name>Abhijit Paithankar</name>
    </author>
    <author>
      <name>Satish Pasumarthi</name>
    </author>
    <author>
      <name>Ankit Patel</name>
    </author>
    <author>
      <name>Mostofa Patwary</name>
    </author>
    <author>
      <name>Ashwin Poojary</name>
    </author>
    <author>
      <name>Gargi Prasad</name>
    </author>
    <author>
      <name>Sweta Priyadarshi</name>
    </author>
    <author>
      <name>Yigong Qin</name>
    </author>
    <author>
      <name>Xiaowei Ren</name>
    </author>
    <author>
      <name>Oleg Rybakov</name>
    </author>
    <author>
      <name>Charbel Sakr</name>
    </author>
    <author>
      <name>Sanjeev Satheesh</name>
    </author>
    <author>
      <name>Stas Sergienko</name>
    </author>
    <author>
      <name>Pasha Shamis</name>
    </author>
    <author>
      <name>Kirthi Shankar</name>
    </author>
    <author>
      <name>Nishant Sharma</name>
    </author>
    <author>
      <name>Mohammad Shoeybi</name>
    </author>
    <author>
      <name>Michael Siu</name>
    </author>
    <author>
      <name>Misha Smelyanskiy</name>
    </author>
    <author>
      <name>Darko Stosic</name>
    </author>
    <author>
      <name>Dusan Stosic</name>
    </author>
    <author>
      <name>Bor-Yiing Su</name>
    </author>
    <author>
      <name>Frank Sun</name>
    </author>
    <author>
      <name>Nima Tajbakhsh</name>
    </author>
    <author>
      <name>Shelby Thomas</name>
    </author>
    <author>
      <name>Przemek Tredak</name>
    </author>
    <author>
      <name>Evgeny Tsykunov</name>
    </author>
    <author>
      <name>Gandhi Vaithilingam</name>
    </author>
    <author>
      <name>Aditya Vavre</name>
    </author>
    <author>
      <name>Rangharajan Venkatesan</name>
    </author>
    <author>
      <name>Roger Waleffe</name>
    </author>
    <author>
      <name>Qiyu Wan</name>
    </author>
    <author>
      <name>Hexin Wang</name>
    </author>
    <author>
      <name>Mengdi Wang</name>
    </author>
    <author>
      <name>Lizzie Wei</name>
    </author>
    <author>
      <name>Hao Wu</name>
    </author>
    <author>
      <name>Evan Wu</name>
    </author>
    <author>
      <name>Keith Wyss</name>
    </author>
    <author>
      <name>Ning Xu</name>
    </author>
    <author>
      <name>Jinze Xue</name>
    </author>
    <author>
      <name>Charlene Yang</name>
    </author>
    <author>
      <name>Yujia Zhai</name>
    </author>
    <author>
      <name>Ruoxi Zhang</name>
    </author>
    <author>
      <name>Jingyang Zhu</name>
    </author>
    <author>
      <name>Zhongbo Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/2509.25149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2509.25149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
